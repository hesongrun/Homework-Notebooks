{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week - 8 - Deep Neural Nets and Text\n",
    "\n",
    "In this week we introduce the use of Deep Neural Networks to work with text. We have already seen some uses of neural networks for text in our classification HW, where we used a simple neural network--the one-layer perceptron--to classify text. It performed quite well, but comes up short in more sophisticated classification tasks, such as in predicting intent. We have also seen slightly deeper, 2-level neural nets in the form of word embeddings such as Word2Vec. While they work well, they have some drawbacks, such as representing words with multiple meanings in a singular space. \n",
    "\n",
    "BERT, which is a language model built using bidirectional encoders, allows us to take advantage of a powerful pre-trained model which we can then use to perform our own tasks based on data we analyze. \n",
    "\n",
    "In this notebook we use ```huggingface/transformers```, a python package that allows for easy interface to use pre-trained BERT models. It is built using Tensorflow and PyTorch, two computational graph packages which are built specifically for creating powerful neural networks. We will also be introducing Keras, which allows us to easily build Neural Networks in an abstracted way. Keras is a popular way to understand how we can stack layers to create such Neural Networks, but to reach state-of-the-art results we will stick with using BERT and similar models that can be tuned to extremely high performance on specific language understanding and generation tasks.\n",
    "\n",
    "To demonstrate this, we begin by using the [Corpus of Linguistic Acceptability](https://nyu-mll.github.io/CoLA/). We will also use BERT by learning how to extract embeddings from such a model and use it to semantically probe sentences. There are a number of new packages and methods we will be using so be sure to update lucem_illud_2020.\n",
    "\n",
    "## NOTE\n",
    "\n",
    "This notebook **requires** GPUs for training models in section 1 and section 3. To train models, please use this [Google Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) to create the models. Note that I have only given you view access: please create your own colab file to train your models, using the code and instructions I have given in the Colab file. So while you have to do the homework on this notebook, the models which you will train should be done on Google Colab, which has GPU access. If you happen to have GPU access on your personal machines or some other way to train the models, you are welcome to do that too.\n",
    "\n",
    "Note that if you run the computationally intensive models on your local computer they will take a long time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # pip install torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig # pip install tranformers==2.4.1\n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lucem_illud #pip install -U git+git://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoLA Dataset and pre-processing\n",
    "\n",
    "We start with loading our dataset and pre-processing it. The pre-processing follows similar steps as we have done in the past, but we will be using pre-written modules offered by the transformers package. These are some of the things we have to take care of when using this particular BERT model.\n",
    "\n",
    "    -special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP])\n",
    "    -tokens that conforms with the fixed vocabulary used in BERT\n",
    "    -token IDs from BERTâ€™s tokenizer\n",
    "    -mask IDs to indicate which elements in the sequence are tokens and which are padding elements\n",
    "    -segment IDs used to distinguish different sentences\n",
    "    -positional embeddings used to show token position within the sequence\n",
    "\n",
    "\n",
    "We will be using parts of the code from [this notebook](https://colab.research.google.com/drive/1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO#scrollTo=BJR6t_gCQe_x) which walks us through the process of using a pre-trained BERT model. The interface to use these models comes from the package [huggingface/transformers](https://github.com/huggingface/transformers). \n",
    "\n",
    "We start by setting up our GPU if we can - this may not work on your machine, so it has been commented out.\n",
    "\n",
    "An aside: check out this tutorial too - https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if gpu:\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cec2667e954c4199046060fa461bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenize the first sentence:\n",
      "['[CLS]', 'our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Deep Neural Nets\n",
    "\n",
    "A popular, simplified package for introducing deep neural networks is [Keras](https://keras.io). It is a high level package in that we don't bother with every detail or hyper-parameter associated with the neural network (e.g., regularizers), and can stack on layers directly. For a rapid tutorial on neural networks for text such as the LSTM or the Recurrent Neural Network, Colah's blog is a great start. [LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) is an article on LSTMs, and if you'd like to  learn about RNN, Andrej Karpathy does a great job in [this blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), in addition to our reading from the newest online verion of Jurafsky & Martin's review of deep learning methods in their book on speech and language processing, chapters [6,7,9,10](https://web.stanford.edu/~jurafsky/slp3/), and the [*Deep Learning*](https://www.deeplearningbook.org/) book by Goodfellow, Bengio & Courville.\n",
    "\n",
    "In the following cells we build a basic deep net that has an embedding layer and an LSTM to perform classification. This is to illustrate the process of using Keras, which is a very popular library for such work. It may not yield state of the art performance because it constrains the hyperparameters you can tune, but is nonetheless an useful tool and works well on some datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_in_size = tokenizer.vocab_size\n",
    "embedding_dim = 32\n",
    "unit = 100\n",
    "no_labels = len(np.unique(train_labels))\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 128, 32)           976704    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,030,106\n",
      "Trainable params: 1,030,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm.add(LSTM(unit))\n",
    "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "241/241 [==============================] - 15s 56ms/step - loss: 0.6132 - accuracy: 0.6979\n",
      "Epoch 2/10\n",
      "241/241 [==============================] - 14s 57ms/step - loss: 0.6084 - accuracy: 0.7059\n",
      "Epoch 3/10\n",
      "241/241 [==============================] - 13s 55ms/step - loss: 0.6023 - accuracy: 0.7129\n",
      "Epoch 4/10\n",
      "241/241 [==============================] - 13s 56ms/step - loss: 0.6125 - accuracy: 0.7001\n",
      "Epoch 5/10\n",
      "241/241 [==============================] - 13s 56ms/step - loss: 0.6103 - accuracy: 0.7020\n",
      "Epoch 6/10\n",
      "241/241 [==============================] - 14s 57ms/step - loss: 0.6059 - accuracy: 0.7065\n",
      "Epoch 7/10\n",
      "241/241 [==============================] - 15s 60ms/step - loss: 0.6049 - accuracy: 0.7079\n",
      "Epoch 8/10\n",
      "241/241 [==============================] - 15s 62ms/step - loss: 0.6067 - accuracy: 0.7067\n",
      "Epoch 9/10\n",
      "241/241 [==============================] - 17s 72ms/step - loss: 0.6084 - accuracy: 0.7043\n",
      "Epoch 10/10\n",
      "241/241 [==============================] - 14s 56ms/step - loss: 0.6120 - accuracy: 0.6994\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
    "                              epochs=10,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy of this model isn't terrible, but it still hovers around 70%. Below there is code for a slightly modified neural network - how does this one perform? Note that in this model, I have added another LSTM layer. You are encouraged to explore the [Keras documentaion](https://keras.io/layers/about-keras-layers/) to explore what kind of layers you can add and how they change performances for different tasks. Different kinds of losses, optimizers, activations and layers can change the flavour of your net dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lstm2 = Sequential()\n",
    "# model_lstm2.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "# model_lstm2.add(LSTM(units))\n",
    "# model_lstm2.add(LSTM(units))\n",
    "# model_lstm2.add(Dense(1, activation='sigmoid'))\n",
    "# model_lstm2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# history_lstm2 = model_lstm2.fit(input_data_train, labels, epochs=10, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On with BERT!\n",
    "\n",
    "So while Neural Networks can do a good job with some kind of classification tasks, they don't perform too well on intent classification. Let us see how a bidirectional transformer embedding like BERT might do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our Models\n",
    "\n",
    "### Train Model\n",
    "Now that our input data is properly formatted, it's time to fine tune the BERT model.\n",
    "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n",
    "We'll load BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
    "\n",
    "### Structure of Fine-Tuning Model\n",
    "\n",
    "As we've showed beforehand, the first token of every sequence is the special classification token ([CLS]). Unlike the hidden state vector corresponding to a normal word token, the hidden state corresponding to this special token is designated by the authors of BERT as an aggregate representation of the whole sentence used for classification tasks. As such, when we feed in an input sentence to our model during training, the output is the length 768 hidden state vector corresponding to this token. The additional layer that we've added on top consists of untrained linear neurons of size [hidden_state, number_of_labels], so [768,2], meaning that the output of BERT plus our classification layer is a vector of two numbers representing the \"score\" for \"grammatical/non-grammatical\" that are then fed into cross-entropy loss.\n",
    "\n",
    "### The Fine-Tuning Process\n",
    "\n",
    "Because the pre-trained BERT layers already encode a lot of information about the language, training the classifier is relatively inexpensive. Rather than training every layer in a large model from scratch, it's as if we have already trained the bottom layers 95% of where they need to be, and only really need to train the top layer, with a bit of tweaking going on in the lower levels to accomodate our task.\n",
    "Sometimes practicioners will opt to \"freeze\" certain layers when fine-tuning, or to apply different learning rates, apply diminishing learning rates, etc. all in an effort to preserve the good quality weights in the network and speed up training (often considerably). In fact, recent research on BERT specifically has demonstrated that freezing the majority of the weights results in only minimal accuracy declines, but there are exceptions and broader rules of transfer learning that should also be considered. For example, if your task and fine-tuning dataset is very different from the dataset used to train the transfer learning model, freezing the weights may not be a good idea. OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\").\n",
    "\n",
    "Credit to Michel Kana's [tutorial](https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03) and the [tutorial](https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP#scrollTo=GuE5BqICAne2) by Chris McCormick and Nick Ryan who describe the workings of BERT and the way it is used by the ```transformers``` package. \n",
    "\n",
    "## WARNING: SHIFT TO A GPU ENABLED MACHINE (e.g., Google Colab)\n",
    "\n",
    "Note that you only want to run the following code if you have a GPU. Otherwise, rerun the **same** cells we just ran on the Colab file to train your model, download it to your local, and load it by running\n",
    "```model = BertForSequenceClassification.from_pretrained(\"my_model_directory\", num_labels=2)```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d488989c1b904a1d8e3a1a1c5e46c990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=433.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7613954fd84781aa09314032f90a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=440473133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following cell can take upto 12 hours or longer if run without a GPU. The [Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) demonstrates how to fine-tune models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # This training code is based on the `run_glue.py` script here:\n",
    "# # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# # Set the seed value all over the place to make this reproducible.\n",
    "# seed_val = 42\n",
    "\n",
    "# random.seed(seed_val)\n",
    "# np.random.seed(seed_val)\n",
    "# torch.manual_seed(seed_val)\n",
    "# torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# # Store the average loss after each epoch so we can plot them.\n",
    "# loss_values = []\n",
    "\n",
    "# # For each epoch...\n",
    "# for epoch_i in range(0, epochs):\n",
    "    \n",
    "#     # ========================================\n",
    "#     #               Training\n",
    "#     # ========================================\n",
    "    \n",
    "#     # Perform one full pass over the training set.\n",
    "\n",
    "#     print(\"\")\n",
    "#     print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "#     print('Training...')\n",
    "\n",
    "#     # Measure how long the training epoch takes.\n",
    "#     t0 = time.time()\n",
    "\n",
    "#     # Reset the total loss for this epoch.\n",
    "#     total_loss = 0\n",
    "\n",
    "#     # Put the model into training mode. Don't be mislead--the call to \n",
    "#     # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "#     # `dropout` and `batchnorm` layers behave differently during training\n",
    "#     # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "#     model.train()\n",
    "\n",
    "#     # For each batch of training data...\n",
    "#     for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "#         # Progress update every 40 batches.\n",
    "#         if step % 40 == 0 and not step == 0:\n",
    "#             # Calculate elapsed time in minutes.\n",
    "#             elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "#             # Report progress.\n",
    "#             print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "#         # Unpack this training batch from our dataloader. \n",
    "#         #\n",
    "#         # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "#         # `to` method.\n",
    "#         #\n",
    "#         # `batch` contains three pytorch tensors:\n",
    "#         #   [0]: input ids \n",
    "#         #   [1]: attention masks\n",
    "#         #   [2]: labels \n",
    "#         b_input_ids = batch[0].to(device)\n",
    "#         b_input_mask = batch[1].to(device)\n",
    "#         b_labels = batch[2].to(device)\n",
    "\n",
    "#         # Always clear any previously calculated gradients before performing a\n",
    "#         # backward pass. PyTorch doesn't do this automatically because \n",
    "#         # accumulating the gradients is \"convenient while training RNNs\". \n",
    "#         # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "#         model.zero_grad()        \n",
    "\n",
    "#         # Perform a forward pass (evaluate the model on this training batch).\n",
    "#         # This will return the loss (rather than the model output) because we\n",
    "#         # have provided the `labels`.\n",
    "#         # The documentation for this `model` function is here: \n",
    "#         # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "#         outputs = model(b_input_ids, \n",
    "#                     token_type_ids=None, \n",
    "#                     attention_mask=b_input_mask, \n",
    "#                     labels=b_labels)\n",
    "        \n",
    "#         # The call to `model` always returns a tuple, so we need to pull the \n",
    "#         # loss value out of the tuple.\n",
    "#         loss = outputs[0]\n",
    "\n",
    "#         # Accumulate the training loss over all of the batches so that we can\n",
    "#         # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "#         # single value; the `.item()` function just returns the Python value \n",
    "#         # from the tensor.\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Perform a backward pass to calculate the gradients.\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Clip the norm of the gradients to 1.0.\n",
    "#         # This is to help prevent the \"exploding gradients\" problem.\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "#         # Update parameters and take a step using the computed gradient.\n",
    "#         # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "#         # modified based on their gradients, the learning rate, etc.\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Update the learning rate.\n",
    "#         scheduler.step()\n",
    "\n",
    "#     # Calculate the average loss over the training data.\n",
    "#     avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "#     # Store the loss value for plotting the learning curve.\n",
    "#     loss_values.append(avg_train_loss)\n",
    "\n",
    "#     print(\"\")\n",
    "#     print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "#     print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "#     # ========================================\n",
    "#     #               Validation\n",
    "#     # ========================================\n",
    "#     # After the completion of each training epoch, measure our performance on\n",
    "#     # our validation set.\n",
    "\n",
    "#     print(\"\")\n",
    "#     print(\"Running Validation...\")\n",
    "\n",
    "#     t0 = time.time()\n",
    "\n",
    "#     # Put the model in evaluation mode--the dropout layers behave differently\n",
    "#     # during evaluation.\n",
    "#     model.eval()\n",
    "\n",
    "#     # Tracking variables \n",
    "#     eval_loss, eval_accuracy = 0, 0\n",
    "#     nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "#     # Evaluate data for one epoch\n",
    "#     for batch in validation_dataloader:\n",
    "        \n",
    "#         # Add batch to GPU\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "#         # Unpack the inputs from our dataloader\n",
    "#         b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "#         # Telling the model not to compute or store gradients, saving memory and\n",
    "#         # speeding up validation\n",
    "#         with torch.no_grad():        \n",
    "\n",
    "#             # Forward pass, calculate logit predictions.\n",
    "#             # This will return the logits rather than the loss because we have\n",
    "#             # not provided labels.\n",
    "#             # token_type_ids is the same as the \"segment ids\", which \n",
    "#             # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "#             # The documentation for this `model` function is here: \n",
    "#             # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "#             outputs = model(b_input_ids, \n",
    "#                             token_type_ids=None, \n",
    "#                             attention_mask=b_input_mask)\n",
    "        \n",
    "#         # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "#         # values prior to applying an activation function like the softmax.\n",
    "#         logits = outputs[0]\n",
    "\n",
    "#         # Move logits and labels to CPU\n",
    "#         logits = logits.detach().cpu().numpy()\n",
    "#         label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "#         # Calculate the accuracy for this batch of test sentences.\n",
    "#         tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "#         # Accumulate the total accuracy.\n",
    "#         eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "#         # Track the number of batches\n",
    "#         nb_eval_steps += 1\n",
    "\n",
    "#     # Report the final accuracy for this validation run.\n",
    "#     print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "#     print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "# print(\"\")\n",
    "# print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Use plot styling from seaborn.\n",
    "# sns.set(style='darkgrid')\n",
    "\n",
    "# # Increase the plot size and font size.\n",
    "# sns.set(font_scale=1.5)\n",
    "# plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# # Plot the learning curve.\n",
    "# plt.plot(loss_values, 'b-o')\n",
    "\n",
    "# # Label the plot.\n",
    "# plt.title(\"Training loss\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COME BACK TO THIS NOTEBOOK to load and work with your trained model\n",
    "\n",
    "Once you tune your model on Colab (or on your own machine if you decided to do that instead), you load it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"model_save\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"../data/cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask) \n",
    "\n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 516 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 354 of 516 (68.60%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthews Corr. Coef. for each batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hesongrun/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "\n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "\n",
    "    # Calculate and store the coef for this batch.  \n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "    matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.049286405809014416,\n",
       " -0.050964719143762556,\n",
       " 0.5405187056040147,\n",
       " 0.17338428937260214,\n",
       " 0.4133804997216296,\n",
       " 0.7410010097502685,\n",
       " 0.6831300510639732,\n",
       " 0.0,\n",
       " 0.8320502943378436,\n",
       " 0.8246211251235321,\n",
       " 0.7679476477883045,\n",
       " 0.6666666666666666,\n",
       " 0.7562449037944323,\n",
       " 0.7141684885491869,\n",
       " 0.30261376633440124,\n",
       " 0.6476427756840265,\n",
       " 0.0]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.555\n"
     ]
    }
   ],
   "source": [
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty good performance. Note that we used [Matthews Correlation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) to meausure this. It ranges from -1 to 1, with +1 being the best. The Google BERT model has a similar score too, so this model performed quite well. It took a long time though, approximately a day with no GPU. It would be significantly faster if a CUDA enabled machine ran this. Hence, we recommend that you run this on the Collab notebook.\n",
    "\n",
    "The following lines save the model to disk, if you would like to: note that we ran this in the colab file to save it to disk there as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "# output_dir = './model_save/'\n",
    "\n",
    "# # Create output directory if needed\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# # They can then be reloaded using `from_pretrained()`\n",
    "# model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "# model_to_save.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# # Good practice: save your training arguments together with the trained model\n",
    "# # torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that estimate a deep classification model with Keras (and LSTM) and also BERT in order to predict pre-established data labels relevant to your final project (as for week 3's homework). Which works better? Are the errors the same or different?\n",
    "\n",
    "<span style=\"color:red\">***Stretch***</span>: <span style=\"color:red\">Now alter the neural network by stacking network layers, adjusting the embedding dimension, compare its performance with your model above, and interpret why it might be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we train deep neural networks to distinguish between journal articles in financial economics and labor economics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>JEL</th>\n",
       "      <th>source</th>\n",
       "      <th>JEL_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEQUENTIAL BANKING</td>\n",
       "      <td>We study environments in which agents may borr...</td>\n",
       "      <td>[G21]</td>\n",
       "      <td>JOURNAL OF POLITICAL ECONOMY</td>\n",
       "      <td>[BANKS; DEPOSITORY INSTITUTIONS; MICRO FINANCE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TESTING FINANCIAL MARKET EQUILIBRIUM UNDER ASY...</td>\n",
       "      <td>We devise tests that distinguish between compe...</td>\n",
       "      <td>[G14]</td>\n",
       "      <td>JOURNAL OF POLITICAL ECONOMY</td>\n",
       "      <td>[INFORMATION AND MARKET EFFICIENCY; EVENT STUD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SOCIAL NORMS, SAVINGS BEHAVIOR, AND GROWTH</td>\n",
       "      <td>We argue that many goods and decisions are not...</td>\n",
       "      <td>[O41, A13]</td>\n",
       "      <td>JOURNAL OF POLITICAL ECONOMY</td>\n",
       "      <td>[ONE, TWO, AND MULTISECTOR GROWTH MODELS, RELA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COMPETITION IN THE BRITISH ELECTRICITY SPOT MA...</td>\n",
       "      <td>Most of the British electricity supply industr...</td>\n",
       "      <td>[L94, L33, L51]</td>\n",
       "      <td>JOURNAL OF POLITICAL ECONOMY</td>\n",
       "      <td>[ELECTRIC UTILITIES, COMPARISON OF PUBLIC AND ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEASURABLE DYNAMIC GAINS FROM TRADE</td>\n",
       "      <td>Productive factors, such as human and physical...</td>\n",
       "      <td>[F11]</td>\n",
       "      <td>JOURNAL OF POLITICAL ECONOMY</td>\n",
       "      <td>[NEOCLASSICAL MODELS OF TRADE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3110</th>\n",
       "      <td>INFERENCE ON DIRECTIONALLY DIFFERENTIABLE FUNC...</td>\n",
       "      <td>This article studies an asymptotic framework f...</td>\n",
       "      <td>[C15, J31]</td>\n",
       "      <td>REVIEW OF ECONOMIC STUDIES</td>\n",
       "      <td>[STATISTICAL SIMULATION METHODS: GENERAL, WAGE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3111</th>\n",
       "      <td>DISCRETE ACTIONS IN INFORMATION-CONSTRAINED DE...</td>\n",
       "      <td>Individuals are constantly processing external...</td>\n",
       "      <td>[D83, D87, G11]</td>\n",
       "      <td>REVIEW OF ECONOMIC STUDIES</td>\n",
       "      <td>[SEARCH; LEARNING; INFORMATION AND KNOWLEDGE; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112</th>\n",
       "      <td>ROBUSTLY OPTIMAL AUCTIONS WITH UNKNOWN RESALE ...</td>\n",
       "      <td>The standard revenue-maximizing auction discri...</td>\n",
       "      <td>[D44, D82]</td>\n",
       "      <td>REVIEW OF ECONOMIC STUDIES</td>\n",
       "      <td>[AUCTIONS, ASYMMETRIC AND PRIVATE INFORMATION;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3113</th>\n",
       "      <td>AN EMPIRICAL MODEL OF WAGE DISPERSION WITH SOR...</td>\n",
       "      <td>We estimate an equilibrium on-the-job search m...</td>\n",
       "      <td>[D24, J24, J31]</td>\n",
       "      <td>REVIEW OF ECONOMIC STUDIES</td>\n",
       "      <td>[PRODUCTION; COST; CAPITAL; CAPITAL, TOTAL FAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3114</th>\n",
       "      <td>INFERENCE ON CAUSAL AND STRUCTURAL PARAMETERS ...</td>\n",
       "      <td>This article considers the problem of testing ...</td>\n",
       "      <td>[C15, D63, L22]</td>\n",
       "      <td>REVIEW OF ECONOMIC STUDIES</td>\n",
       "      <td>[STATISTICAL SIMULATION METHODS: GENERAL, EQUI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3115 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                                    SEQUENTIAL BANKING   \n",
       "1     TESTING FINANCIAL MARKET EQUILIBRIUM UNDER ASY...   \n",
       "2            SOCIAL NORMS, SAVINGS BEHAVIOR, AND GROWTH   \n",
       "3     COMPETITION IN THE BRITISH ELECTRICITY SPOT MA...   \n",
       "4                   MEASURABLE DYNAMIC GAINS FROM TRADE   \n",
       "...                                                 ...   \n",
       "3110  INFERENCE ON DIRECTIONALLY DIFFERENTIABLE FUNC...   \n",
       "3111  DISCRETE ACTIONS IN INFORMATION-CONSTRAINED DE...   \n",
       "3112  ROBUSTLY OPTIMAL AUCTIONS WITH UNKNOWN RESALE ...   \n",
       "3113  AN EMPIRICAL MODEL OF WAGE DISPERSION WITH SOR...   \n",
       "3114  INFERENCE ON CAUSAL AND STRUCTURAL PARAMETERS ...   \n",
       "\n",
       "                                               abstract              JEL  \\\n",
       "0     We study environments in which agents may borr...            [G21]   \n",
       "1     We devise tests that distinguish between compe...            [G14]   \n",
       "2     We argue that many goods and decisions are not...       [O41, A13]   \n",
       "3     Most of the British electricity supply industr...  [L94, L33, L51]   \n",
       "4     Productive factors, such as human and physical...            [F11]   \n",
       "...                                                 ...              ...   \n",
       "3110  This article studies an asymptotic framework f...       [C15, J31]   \n",
       "3111  Individuals are constantly processing external...  [D83, D87, G11]   \n",
       "3112  The standard revenue-maximizing auction discri...       [D44, D82]   \n",
       "3113  We estimate an equilibrium on-the-job search m...  [D24, J24, J31]   \n",
       "3114  This article considers the problem of testing ...  [C15, D63, L22]   \n",
       "\n",
       "                            source  \\\n",
       "0     JOURNAL OF POLITICAL ECONOMY   \n",
       "1     JOURNAL OF POLITICAL ECONOMY   \n",
       "2     JOURNAL OF POLITICAL ECONOMY   \n",
       "3     JOURNAL OF POLITICAL ECONOMY   \n",
       "4     JOURNAL OF POLITICAL ECONOMY   \n",
       "...                            ...   \n",
       "3110    REVIEW OF ECONOMIC STUDIES   \n",
       "3111    REVIEW OF ECONOMIC STUDIES   \n",
       "3112    REVIEW OF ECONOMIC STUDIES   \n",
       "3113    REVIEW OF ECONOMIC STUDIES   \n",
       "3114    REVIEW OF ECONOMIC STUDIES   \n",
       "\n",
       "                                              JEL_label  \n",
       "0     [BANKS; DEPOSITORY INSTITUTIONS; MICRO FINANCE...  \n",
       "1     [INFORMATION AND MARKET EFFICIENCY; EVENT STUD...  \n",
       "2     [ONE, TWO, AND MULTISECTOR GROWTH MODELS, RELA...  \n",
       "3     [ELECTRIC UTILITIES, COMPARISON OF PUBLIC AND ...  \n",
       "4                        [NEOCLASSICAL MODELS OF TRADE]  \n",
       "...                                                 ...  \n",
       "3110  [STATISTICAL SIMULATION METHODS: GENERAL, WAGE...  \n",
       "3111  [SEARCH; LEARNING; INFORMATION AND KNOWLEDGE; ...  \n",
       "3112  [AUCTIONS, ASYMMETRIC AND PRIVATE INFORMATION;...  \n",
       "3113  [PRODUCTION; COST; CAPITAL; CAPITAL, TOTAL FAC...  \n",
       "3114  [STATISTICAL SIMULATION METHODS: GENERAL, EQUI...  \n",
       "\n",
       "[3115 rows x 5 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JEL = pd.read_pickle('/Users/hesongrun/Dropbox/TTIC31220/JEL/Top5_JEL.pkl')\n",
    "\n",
    "JEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'G': 209,\n",
       "         'O': 108,\n",
       "         'L': 190,\n",
       "         'F': 223,\n",
       "         'D': 1277,\n",
       "         'J': 328,\n",
       "         'E': 198,\n",
       "         'R': 22,\n",
       "         'Q': 9,\n",
       "         'M': 18,\n",
       "         'C': 373,\n",
       "         'K': 30,\n",
       "         'A': 9,\n",
       "         'H': 35,\n",
       "         'P': 5,\n",
       "         'I': 80,\n",
       "         'B': 1})"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "def one_JEL(x):\n",
    "    lst = []\n",
    "    for i in x:\n",
    "        lst.append(i[0])\n",
    "    return max(lst,key=lst.count)\n",
    "\n",
    "JEL['one_JEL'] = JEL['JEL'].apply(one_JEL)\n",
    "\n",
    "collections.Counter(JEL['one_JEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>General Economics and Teaching</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>History of Economic Thought, Methodology, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>Mathematical and Quantitative Methods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D</td>\n",
       "      <td>Microeconomics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E</td>\n",
       "      <td>Macroeconomics and Monetary Economics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F</td>\n",
       "      <td>International Economics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>G</td>\n",
       "      <td>Financial Economics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>H</td>\n",
       "      <td>Public Economics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I</td>\n",
       "      <td>Health, Education, and Welfare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>J</td>\n",
       "      <td>Labor and Demographic Economics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>K</td>\n",
       "      <td>Law and Economics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>L</td>\n",
       "      <td>Industrial Organization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M</td>\n",
       "      <td>Business Administration and Business Economics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>N</td>\n",
       "      <td>Economic History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>O</td>\n",
       "      <td>Economic Development, Innovation, Technologica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>P</td>\n",
       "      <td>Economic Systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Q</td>\n",
       "      <td>Agricultural and Natural Resource Economics; E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>R</td>\n",
       "      <td>Urban, Rural, Regional, Real Estate, and Trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Y</td>\n",
       "      <td>Miscellaneous Categories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Z</td>\n",
       "      <td>Other Special Topics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Code                                        Description\n",
       "0     A                     General Economics and Teaching\n",
       "1     B  History of Economic Thought, Methodology, and ...\n",
       "2     C              Mathematical and Quantitative Methods\n",
       "3     D                                     Microeconomics\n",
       "4     E              Macroeconomics and Monetary Economics\n",
       "5     F                            International Economics\n",
       "6     G                                Financial Economics\n",
       "7     H                                   Public Economics\n",
       "8     I                     Health, Education, and Welfare\n",
       "9     J                    Labor and Demographic Economics\n",
       "10    K                                  Law and Economics\n",
       "11    L                            Industrial Organization\n",
       "12    M  Business Administration and Business Economics...\n",
       "13    N                                   Economic History\n",
       "14    O  Economic Development, Innovation, Technologica...\n",
       "15    P                                   Economic Systems\n",
       "16    Q  Agricultural and Natural Resource Economics; E...\n",
       "17    R  Urban, Rural, Regional, Real Estate, and Trans...\n",
       "18    Y                           Miscellaneous Categories\n",
       "19    Z                               Other Special Topics"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JEL_lv1 = pd.read_pickle('/Users/hesongrun/Dropbox/TTIC31220/JEL/JEL_lv1.pkl')\n",
    "\n",
    "JEL_lv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-151-81d92d99a669>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_df['label'] = np.where(sample_df['one_JEL']==\"G\", 1, 0)\n",
      "<ipython-input-151-81d92d99a669>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_df['sentence_list'] = sample_df.apply(lambda row: sentence_split(row['abstract']), axis = 1)\n"
     ]
    }
   ],
   "source": [
    "JEL_GJ = JEL[(JEL['one_JEL']=='G')|(JEL['one_JEL']=='J')]\n",
    "\n",
    "def sentence_split(text):\n",
    "    sentence_list = text.split('.')\n",
    "    return(sentence_list)\n",
    "\n",
    "# AER = JEL_GJ[JEL_GJ['source']=='AMERICAN ECONOMIC REVIEW']\n",
    "\n",
    "sample_df = JEL_GJ\n",
    "\n",
    "sample_df['label'] = np.where(sample_df['one_JEL']==\"G\", 1, 0)\n",
    "\n",
    "sample_df['sentence_list'] = sample_df.apply(lambda row: sentence_split(row['abstract']), axis = 1)\n",
    "\n",
    "sample_df = sample_df[['title', 'abstract', 'label', 'sentence_list', 'source']]\n",
    "\n",
    "sentence_df = sample_df[sample_df['source']!= 'AMERICAN ECONOMIC REVIEW'].explode('sentence_list')\n",
    "\n",
    "sentence_df = sentence_df[sentence_df['sentence_list']!='']\n",
    "\n",
    "sentence_df.columns = ['title', 'abstract', 'label', 'sentence', 'source']\n",
    "\n",
    "sentence_df.to_csv('hw8_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Text for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(sentence_df.sentence)\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "\n",
    "labels = sentence_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'we', 'study', 'environments', 'in', 'which', 'agents', 'may', 'borrow', 'sequential', '##ly', 'from', 'more', 'than', 'one', 'lend', '##er', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 128\n",
    "\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_in_size = tokenizer.vocab_size\n",
    "embedding_dim = 32\n",
    "unit = 100\n",
    "no_labels = len(np.unique(train_labels))\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 128, 32)           976704    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,030,106\n",
      "Trainable params: 1,030,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm.add(LSTM(unit))\n",
    "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "67/67 [==============================] - 5s 49ms/step - loss: 0.6724 - accuracy: 0.6074\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 3s 48ms/step - loss: 0.6599 - accuracy: 0.6315\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 3s 52ms/step - loss: 0.6562 - accuracy: 0.6363\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 4s 54ms/step - loss: 0.6590 - accuracy: 0.6387\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 4s 54ms/step - loss: 0.6615 - accuracy: 0.6305\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 4s 56ms/step - loss: 0.6627 - accuracy: 0.6273\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 4s 59ms/step - loss: 0.6665 - accuracy: 0.6170\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 4s 54ms/step - loss: 0.6580 - accuracy: 0.6370\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 4s 53ms/step - loss: 0.6645 - accuracy: 0.6207\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 4s 54ms/step - loss: 0.6638 - accuracy: 0.6239\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
    "                              epochs=10,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the abvoe cell, we can see the training accuracy hovers around 0.6. This suggests that LSTM cannot distinguish between journal articles on financial economics and labor economics very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_df = sample_df[sample_df['source'] == 'AMERICAN ECONOMIC REVIEW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_sen_df = holdout_df.explode('sentence_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_sen_df.columns = ['title', 'abstract', 'label', 'sentence', 'source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_sen_df = holdout_sen_df[holdout_sen_df['sentence']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_sen_df.to_csv('holdout_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"model_JEL\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"holdout_df.csv\")\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask) \n",
    "\n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 699 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 304 of 699 (43.49%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthews Corr. Coef. for each batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hesongrun/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "\n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "\n",
    "    # Calculate and store the coef for this batch.  \n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "    matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.770\n"
     ]
    }
   ],
   "source": [
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this example, BERT outperforms traditional LSTM. It has much higher MCC, which means higher classification accuracy for labeling different sentences.\n",
    "\n",
    "From another perspective, the distinction between journal articles on financial economics and labor economics may be apparent. This is the fundamental reason behind the relatively high MCC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings, Context Words\n",
    "\n",
    "We saw how a bootstrapped BERT model performed so much better than a model trained from scatch. Because BERT's method of capturing context is bidirectional, meaning that words can now have different word embedding values based on their location within a sentence. Let us use the same BERT model to capture sentence and word embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through the sentence format for the BERT model, as well as how our vocabulary looks like. Note that you have to use the BERT tokenizer to use the BERT model because of the similar vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERTS model uses a WordPiece technique to do its tokenizing, as described in the paper. That's why the word embedding is split up the way it is.\n",
    "A quick peek at what the voabulary looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['peninsula',\n",
       " 'adults',\n",
       " 'novels',\n",
       " 'emerged',\n",
       " 'vienna',\n",
       " 'metro',\n",
       " 'debuted',\n",
       " 'shoes',\n",
       " 'tamil',\n",
       " 'songwriter',\n",
       " 'meets',\n",
       " 'prove',\n",
       " 'beating',\n",
       " 'instance',\n",
       " 'heaven',\n",
       " 'scared',\n",
       " 'sending',\n",
       " 'marks',\n",
       " 'artistic',\n",
       " 'passage',\n",
       " 'superior',\n",
       " '03',\n",
       " 'significantly',\n",
       " 'shopping',\n",
       " '##tive',\n",
       " 'retained',\n",
       " '##izing',\n",
       " 'malaysia',\n",
       " 'technique',\n",
       " 'cheeks']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[6000:6030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "after         2,044\n",
      "stealing     11,065\n",
      "money         2,769\n",
      "from          2,013\n",
      "the           1,996\n",
      "bank          2,924\n",
      "vault        11,632\n",
      ",             1,010\n",
      "the           1,996\n",
      "bank          2,924\n",
      "robber       27,307\n",
      "was           2,001\n",
      "seen          2,464\n",
      "fishing       5,645\n",
      "on            2,006\n",
      "the           1,996\n",
      "mississippi   5,900\n",
      "river         2,314\n",
      "bank          2,924\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indices.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment ID\n",
    "\n",
    "BERT is trained on and expects sentence pairs, using 1s and 0s to distinguish between the two sentences. That is, for each token in â€œtokenized_text,â€ we must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). For our purposes, single-sentence inputs only require a series of 1s, so we will create a vector of 1s for each token in our input sentence.\n",
    "\n",
    "If you want to process two sentences, assign each word in the first sentence plus the â€˜[SEP]â€™ token a 0, and all tokens of the second sentence a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we did for classification, we now convert these segments to tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer is the hidden state layer, and this is what we pick up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model_embedding = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model_embedding.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_embedding(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0][0][0]), len(output[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Output\n",
    "\n",
    "This kind of forward pass returns us the last layer of the net, which we will use to make our vectors. The first object returned contains the batch number, followed by each of the tokens and their vector values. The second object contains a vector value, which I suspect is the sentence vector of the tokens. \n",
    "\n",
    "The first index is the batch size, and our batch size is 1, so we just choose the 0th index and work with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = output[0]\n",
    "sentence_embedding = output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4964, -0.1831, -0.5231,  ..., -0.1902,  0.3738,  0.3964],\n",
       "        [-0.1323, -0.2762, -0.3495,  ..., -0.4567,  0.3786, -0.1096],\n",
       "        [-0.3626, -0.4002,  0.0676,  ..., -0.3207, -0.2709, -0.3004],\n",
       "        ...,\n",
       "        [ 0.2961, -0.2856, -0.0382,  ..., -0.6056, -0.5163,  0.2005],\n",
       "        [ 0.4878, -0.0909, -0.2358,  ..., -0.0017, -0.5945, -0.2431],\n",
       "        [-0.2517, -0.3519, -0.4688,  ...,  0.2500,  0.0336, -0.2627]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s take a quick look at the range of values for a given layer and token.\n",
    "\n",
    "Youâ€™ll find that the range is fairly similar for all layers and tokens, with the majority of values falling between [-2, 2], and a small smattering of values around -10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vec = word_embeddings[0][0]\n",
    "vec = vec.detach().numpy()\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are grouped by layer - we can use the permute function to make it grouped by each individual token instead. Let us look at what the later looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors\n",
    "\n",
    "So each of those tokens have embedding values - let us try and compare them with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vecs = []\n",
    "# For each token in the sentence...\n",
    "for embedding in word_embeddings[0]:\n",
    "    cat_vec = embedding.detach().numpy()\n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs.append(cat_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method to create the vectors is to sum the last four layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Vector\n",
    "\n",
    "To get a single vector for our entire sentence we have multiple application-dependent strategies - we could just average all the tokens in our sentence. We can also use this oppurtunity to see if the second vector returned is a sentence vector too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding_0 = sentence_embedding.detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding_1 = np.mean(token_vecs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_embedding_0), len(sentence_embedding_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the power of these vectors is how they are context dependant - our sentence had multiple uses of the word bank. Let us see the index and the word of the sentence and check the context accordingly. We'll then print the simlarity values for the similar and different meanings and see how it turns out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 after\n",
      "2 stealing\n",
      "3 money\n",
      "4 from\n",
      "5 the\n",
      "6 bank\n",
      "7 vault\n",
      "8 ,\n",
      "9 the\n",
      "10 bank\n",
      "11 robber\n",
      "12 was\n",
      "13 seen\n",
      "14 fishing\n",
      "15 on\n",
      "16 the\n",
      "17 mississippi\n",
      "18 river\n",
      "19 bank\n",
      "20 .\n",
      "21 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "    print(i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 vector values for each instance of \"bank\".\n",
      "\n",
      "bank vault    [ 0.90010506 -0.5380408  -0.16690889  0.22416201  0.689658  ]\n",
      "bank robber   [ 0.7977125  -0.5217273  -0.19836998  0.18898547  0.5940932 ]\n",
      "river bank    [ 0.29608968 -0.2856338  -0.038183    0.16736193  0.7712634 ]\n"
     ]
    }
   ],
   "source": [
    "print('First 5 vector values for each instance of \"bank\".')\n",
    "print('')\n",
    "print(\"bank vault   \", str(token_vecs[6][:5]))\n",
    "print(\"bank robber  \", str(token_vecs[10][:5]))\n",
    "print(\"river bank   \", str(token_vecs[19][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.95\n",
      "Vector similarity for *different* meanings:  0.70\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(token_vecs[10], token_vecs[19])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = 1 - cosine(token_vecs[10], token_vecs[6])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense! Let us see if the mean value of all the tokens and what we think is the sentence vector is the same thing, by checking their cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008313219994306564"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cosine(sentence_embedding_0, sentence_embedding_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is good - it seems it is indeed the sentence vector, so we can now write two functions which calculate the word and sentence vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(text, word_id, model, tokenizer):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    word_embeddings = model(tokens_tensor)[0]\n",
    "    sentence_embeddings = model(tokens_tensor)[1]\n",
    "    vector = word_embeddings[0][word_id].detach().numpy()\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_10 = word_vector(text, 10, model_embedding, tokenizer)\n",
    "word_6 = word_vector(text, 6, model_embedding, tokenizer)\n",
    "word_19 = word_vector(text, 19, model_embedding, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vector(text, model, tokenizer, method=\"average\"):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    word_embeddings = model(tokens_tensor)[0]\n",
    "    sentence_embeddings = model(tokens_tensor)[1]\n",
    "    token_vecs = []\n",
    "    \n",
    "    for embedding in word_embeddings[0]:\n",
    "        cat_vec = embedding.detach().numpy()\n",
    "        token_vecs.append(cat_vec)\n",
    "        \n",
    "    if method == \"average\":\n",
    "        sentence_embedding = np.mean(token_vecs, axis=0)\n",
    "    if method == \"model\":\n",
    "        sentence_embedding = sentence_embeddings\n",
    "    # do something\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_vec_0 = sentence_vector(text, model_embedding, tokenizer)\n",
    "sen_vec_1 = sentence_vector(text, model_embedding, tokenizer, method=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity metrics\n",
    "It is worth noting that word-level similarity comparisons are not appropriate with BERT embeddings because these embeddings are contextually dependent, meaning that the word vector changes depending on the sentence it appears in. This enables direct sensitivity to polysemy so that, e.g., your representation encodes river â€œbankâ€ and not a financial institution â€œbankâ€. Nevertheless, it makes direct word-to-word similarity comparisons less valuable. For sentence embeddings, however, similarity comparison is still valid such that one can query, for example, a single sentence against a dataset of other sentences in order to find the most similar. Depending on the similarity metric used, the resulting similarity values will be less informative than the relative ranking of similarity outputs as some similarity metrics make assumptions about the vector space (equally-weighted dimensions, for example) that do not hold for our 768-dimensional vector space.\n",
    "\n",
    "### Using the Vectors\n",
    "\n",
    "Without fine-tuning, BERT features may be less useful than plain GloVe or word2vec.\n",
    "They start to be interesting when you fine-tune a classifier on top of BERT. \n",
    "\n",
    "### Using Transformers Pipelines\n",
    "\n",
    "The context vectors make the other pipeline functions which transformers has built in a lot more powerful. \n",
    "\n",
    "### NOTE\n",
    "The pipeline functionality in transformers is currently being worked on and might be broken, so it is an optional part of the exercise. Do try to uncomment the lines of code and try to see if it works, though! If you have managed to get transformers v2.5.1 installed, it will work - I managed to get it to work sometimes, it can be annoying to get it to work but when it works it works well.\n",
    "\n",
    "Consider the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe5cae68e2d4bed8e13467ff162222e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=629.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9844535bbef4419a2736edb693b15f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=267844284.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739005944ffd43a99e2bed009f86d3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc30effb7b64016842cefe12985a218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Allocate a pipeline for sentiment-analysis\n",
    "nlp_sentiment = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997735023498535}]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"This BERT model is so good at classifiying sentiment, I love it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a strong positive sentiment, which we'd expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.999719500541687}]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"I'm so sad that I have to spend this weekend just doing HW and readings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative label, bingo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3712689505554dd4b3cb4abbcb7c0fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2088910dd6481daf06126a172a8f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=260793700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0613327d96ac4f2dbd6f329d4c7b012c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=213450.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5883d4d2bff14094909cc16e3847e9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=435797.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Allocate a pipeline for question-answering\n",
    "nlp_question = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9860534071922302,\n",
       " 'start': 34,\n",
       " 'end': 64,\n",
       " 'answer': 'analysing complex textual data'}"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "    'question': 'What is my favorite thing to do on weekends ?',\n",
    "    'context': 'There is nothing I like more than analysing complex textual data all weekend '\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also great at question-answering tasks!\n",
    "We can also extract features, as we manually did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aded06d808114e0baa627473c8c6f38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=411.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3edbede9d49147a1a056b12a9c4f5e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=263273408.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd04509cdc6240ab84451ed481475610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=213450.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e58d92e392b4a71810a70ecb17708b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=435797.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nlp_feature = pipeline('feature-extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = nlp_feature(\"Just sitting here exploring data all day long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vec[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.28079739212989807,\n",
       "   0.0664445012807846,\n",
       "   -0.16763967275619507,\n",
       "   -0.16896049678325653,\n",
       "   -0.22660808265209198,\n",
       "   -0.14127124845981598,\n",
       "   0.42102956771850586,\n",
       "   -0.09199235588312149,\n",
       "   -0.031907953321933746,\n",
       "   -0.8537280559539795,\n",
       "   -0.26838088035583496,\n",
       "   0.08569946885108948,\n",
       "   -0.1340431272983551,\n",
       "   -0.03496021032333374,\n",
       "   -0.5356060266494751,\n",
       "   -0.029006630182266235,\n",
       "   0.20110872387886047,\n",
       "   0.08040327578783035,\n",
       "   -0.08755531907081604,\n",
       "   -0.08443009108304977,\n",
       "   0.05363844335079193,\n",
       "   -0.18426796793937683,\n",
       "   0.5852552652359009,\n",
       "   -0.17160028219223022,\n",
       "   0.05120442807674408,\n",
       "   0.12319859117269516,\n",
       "   0.37301188707351685,\n",
       "   0.16151967644691467,\n",
       "   -0.1783244013786316,\n",
       "   0.5117567777633667,\n",
       "   -0.02019638940691948,\n",
       "   0.19047735631465912,\n",
       "   0.06237814575433731,\n",
       "   0.05721733719110489,\n",
       "   -0.3619837462902069,\n",
       "   0.22840623557567596,\n",
       "   -0.1847357153892517,\n",
       "   -0.21770460903644562,\n",
       "   -0.07018117606639862,\n",
       "   -0.09863680601119995,\n",
       "   -0.5033515691757202,\n",
       "   0.11645970493555069,\n",
       "   0.5387319326400757,\n",
       "   -0.18301953375339508,\n",
       "   0.045671895146369934,\n",
       "   -0.4805182218551636,\n",
       "   0.07694830745458603,\n",
       "   0.03903837502002716,\n",
       "   -0.08155328035354614,\n",
       "   0.1363123506307602,\n",
       "   -0.09813513606786728,\n",
       "   0.09356572479009628,\n",
       "   -0.19313709437847137,\n",
       "   -0.0409696027636528,\n",
       "   0.1512814611196518,\n",
       "   0.08282738924026489,\n",
       "   -0.0629231259226799,\n",
       "   0.13535535335540771,\n",
       "   -0.5575191974639893,\n",
       "   0.26149630546569824,\n",
       "   -0.07125763595104218,\n",
       "   0.033854998648166656,\n",
       "   0.3390797972679138,\n",
       "   0.06560899317264557,\n",
       "   -0.22296366095542908,\n",
       "   0.07824650406837463,\n",
       "   0.04569867253303528,\n",
       "   0.21588373184204102,\n",
       "   -0.1998806893825531,\n",
       "   -0.2116689234972,\n",
       "   0.09120095521211624,\n",
       "   0.22718782722949982,\n",
       "   0.303043395280838,\n",
       "   0.890407919883728,\n",
       "   0.22981899976730347,\n",
       "   -0.21156862378120422,\n",
       "   0.3337339162826538,\n",
       "   -0.04453226923942566,\n",
       "   -0.02711961418390274,\n",
       "   -0.09593846648931503,\n",
       "   0.057736121118068695,\n",
       "   0.19910471141338348,\n",
       "   -0.11624057590961456,\n",
       "   -0.2266002595424652,\n",
       "   -0.028790079057216644,\n",
       "   -0.13013119995594025,\n",
       "   0.10612832009792328,\n",
       "   -0.11570262908935547,\n",
       "   -0.08082104474306107,\n",
       "   0.15435482561588287,\n",
       "   0.1601402908563614,\n",
       "   -0.15008316934108734,\n",
       "   -0.24606426060199738,\n",
       "   0.046916231513023376,\n",
       "   -0.20780424773693085,\n",
       "   0.10484108328819275,\n",
       "   -0.04270895570516586,\n",
       "   0.07523752748966217,\n",
       "   6.085381031036377,\n",
       "   -0.061667803674936295,\n",
       "   -0.23195809125900269,\n",
       "   0.06998215615749359,\n",
       "   0.0938836857676506,\n",
       "   -0.11683934181928635,\n",
       "   0.2404944896697998,\n",
       "   -0.328864723443985,\n",
       "   -0.034948475658893585,\n",
       "   -0.46981388330459595,\n",
       "   0.0344100147485733,\n",
       "   0.49799221754074097,\n",
       "   0.39733439683914185,\n",
       "   0.08457028865814209,\n",
       "   0.1202300563454628,\n",
       "   -0.10584193468093872,\n",
       "   -0.09611060470342636,\n",
       "   -0.3282928168773651,\n",
       "   0.09671969711780548,\n",
       "   0.049704041332006454,\n",
       "   0.07413184642791748,\n",
       "   -0.21588188409805298,\n",
       "   0.15652702748775482,\n",
       "   0.005386378616094589,\n",
       "   0.9506378769874573,\n",
       "   0.1033695787191391,\n",
       "   -0.1121620237827301,\n",
       "   -0.0011059343814849854,\n",
       "   -0.001170705072581768,\n",
       "   -0.2742651700973511,\n",
       "   0.10748089104890823,\n",
       "   -0.1626310646533966,\n",
       "   -0.4893864393234253,\n",
       "   -0.2280603051185608,\n",
       "   -0.0715828686952591,\n",
       "   -0.1492326259613037,\n",
       "   0.10891944915056229,\n",
       "   0.008096151053905487,\n",
       "   -0.04215297847986221,\n",
       "   -0.036261364817619324,\n",
       "   -0.8590203523635864,\n",
       "   0.1194952130317688,\n",
       "   0.09703504294157028,\n",
       "   -0.09249447286128998,\n",
       "   0.09470720589160919,\n",
       "   -0.16255386173725128,\n",
       "   -0.2843359708786011,\n",
       "   2.624413251876831,\n",
       "   -0.12164181470870972,\n",
       "   0.030225761234760284,\n",
       "   -0.12604984641075134,\n",
       "   0.05357097461819649,\n",
       "   -0.08468538522720337,\n",
       "   -0.18748122453689575,\n",
       "   -0.2489502876996994,\n",
       "   0.2305166870355606,\n",
       "   -0.2518126964569092,\n",
       "   -0.2152705192565918,\n",
       "   0.1833399087190628,\n",
       "   0.21675308048725128,\n",
       "   0.05431963503360748,\n",
       "   0.09860704094171524,\n",
       "   -1.0202133655548096,\n",
       "   -0.007301501929759979,\n",
       "   -0.22835960984230042,\n",
       "   0.23675398528575897,\n",
       "   0.13464245200157166,\n",
       "   -0.2398894876241684,\n",
       "   -0.0015054978430271149,\n",
       "   -0.6380895972251892,\n",
       "   0.02323605678975582,\n",
       "   0.3602570593357086,\n",
       "   0.04916900768876076,\n",
       "   0.031636446714401245,\n",
       "   -2.1947340965270996,\n",
       "   0.3170282542705536,\n",
       "   0.1387958526611328,\n",
       "   0.13678273558616638,\n",
       "   0.003398451954126358,\n",
       "   0.1387530267238617,\n",
       "   0.005030553787946701,\n",
       "   -0.2804130017757416,\n",
       "   -0.17104016244411469,\n",
       "   0.3840469717979431,\n",
       "   0.13680823147296906,\n",
       "   0.0855025127530098,\n",
       "   -0.4032360017299652,\n",
       "   0.015949930995702744,\n",
       "   0.1298864632844925,\n",
       "   -0.19909200072288513,\n",
       "   -0.039330631494522095,\n",
       "   0.007921304553747177,\n",
       "   -0.13287632167339325,\n",
       "   -0.3336023986339569,\n",
       "   -0.12062513828277588,\n",
       "   0.19588854908943176,\n",
       "   0.19319652020931244,\n",
       "   0.13310939073562622,\n",
       "   0.058580707758665085,\n",
       "   -0.09095705300569534,\n",
       "   -0.061363935470581055,\n",
       "   0.17067165672779083,\n",
       "   -0.18688486516475677,\n",
       "   -0.12096935510635376,\n",
       "   -0.14287224411964417,\n",
       "   0.1442725658416748,\n",
       "   0.36059847474098206,\n",
       "   0.19136765599250793,\n",
       "   -0.03786735609173775,\n",
       "   0.11734558641910553,\n",
       "   0.17427366971969604,\n",
       "   -0.06699507683515549,\n",
       "   -0.19577880203723907,\n",
       "   0.02570239081978798,\n",
       "   0.08491039276123047,\n",
       "   0.42135992646217346,\n",
       "   0.5814640522003174,\n",
       "   -0.39639273285865784,\n",
       "   -0.03314348682761192,\n",
       "   -0.08839772641658783,\n",
       "   0.09495879709720612,\n",
       "   0.046528030186891556,\n",
       "   -0.10590064525604248,\n",
       "   0.12356507033109665,\n",
       "   -0.17063239216804504,\n",
       "   -0.1169813722372055,\n",
       "   -0.23107431828975677,\n",
       "   0.05537477508187294,\n",
       "   0.29407235980033875,\n",
       "   0.20254653692245483,\n",
       "   0.011806134134531021,\n",
       "   -0.3329518139362335,\n",
       "   0.2028825581073761,\n",
       "   0.184933602809906,\n",
       "   0.09737400710582733,\n",
       "   -0.2933533191680908,\n",
       "   -0.014712901785969734,\n",
       "   -0.037921760231256485,\n",
       "   -0.017111873254179955,\n",
       "   0.08761604130268097,\n",
       "   -0.030812203884124756,\n",
       "   -0.08504315465688705,\n",
       "   0.07453297823667526,\n",
       "   -0.0010540969669818878,\n",
       "   0.3211292624473572,\n",
       "   0.02436837926506996,\n",
       "   0.007990257814526558,\n",
       "   0.06423584371805191,\n",
       "   0.34073513746261597,\n",
       "   0.29837706685066223,\n",
       "   0.07283467054367065,\n",
       "   -0.02352580428123474,\n",
       "   0.6100217700004578,\n",
       "   0.10051994025707245,\n",
       "   -0.18003562092781067,\n",
       "   -0.2914906442165375,\n",
       "   -0.534946620464325,\n",
       "   -0.1487496942281723,\n",
       "   -0.04322604462504387,\n",
       "   -1.198394536972046,\n",
       "   -0.11274401098489761,\n",
       "   -0.33366066217422485,\n",
       "   0.2787924110889435,\n",
       "   -2.934847831726074,\n",
       "   0.024573834612965584,\n",
       "   0.24009501934051514,\n",
       "   -0.001540452241897583,\n",
       "   -0.10958167910575867,\n",
       "   0.29140976071357727,\n",
       "   0.09774919599294662,\n",
       "   -0.4551195502281189,\n",
       "   0.23169517517089844,\n",
       "   -0.46620801091194153,\n",
       "   0.02112693339586258,\n",
       "   -0.6199561357498169,\n",
       "   0.06250537186861038,\n",
       "   -0.12774643301963806,\n",
       "   0.25704067945480347,\n",
       "   -0.06207556277513504,\n",
       "   -0.05455467104911804,\n",
       "   -0.23015938699245453,\n",
       "   -0.21192142367362976,\n",
       "   -0.14584361016750336,\n",
       "   0.059484343975782394,\n",
       "   -0.3123905658721924,\n",
       "   0.4359772205352783,\n",
       "   -0.2780084013938904,\n",
       "   0.3652206063270569,\n",
       "   0.3439832329750061,\n",
       "   0.19921991229057312,\n",
       "   0.10600948333740234,\n",
       "   3.6819167137145996,\n",
       "   0.2765500843524933,\n",
       "   -0.15455570816993713,\n",
       "   0.09237843751907349,\n",
       "   -0.3022404909133911,\n",
       "   0.013119937852025032,\n",
       "   0.04309057071805,\n",
       "   -0.45908603072166443,\n",
       "   0.0307452529668808,\n",
       "   -0.07204145193099976,\n",
       "   -0.08520267903804779,\n",
       "   -0.2387385219335556,\n",
       "   0.05929221212863922,\n",
       "   -0.4444526135921478,\n",
       "   -0.05480575934052467,\n",
       "   -0.4397672414779663,\n",
       "   0.045697059482336044,\n",
       "   0.038081660866737366,\n",
       "   0.10586252808570862,\n",
       "   -0.6242749094963074,\n",
       "   -0.11423887312412262,\n",
       "   0.0795186460018158,\n",
       "   0.08646105229854584,\n",
       "   -0.04021652042865753,\n",
       "   0.06479869782924652,\n",
       "   -0.005205214023590088,\n",
       "   -0.2825011610984802,\n",
       "   -0.38159191608428955,\n",
       "   0.27384036779403687,\n",
       "   0.016155175864696503,\n",
       "   -0.7521315813064575,\n",
       "   0.2528271973133087,\n",
       "   0.17298553884029388,\n",
       "   -0.3031437397003174,\n",
       "   -0.0546368807554245,\n",
       "   0.06651043891906738,\n",
       "   -0.06294231861829758,\n",
       "   -0.048167884349823,\n",
       "   -0.1764613687992096,\n",
       "   -0.23466061055660248,\n",
       "   -0.013834799639880657,\n",
       "   -0.16199511289596558,\n",
       "   0.09453613311052322,\n",
       "   -0.30075669288635254,\n",
       "   -0.17989669740200043,\n",
       "   -0.19597971439361572,\n",
       "   -0.14192038774490356,\n",
       "   -0.10136280953884125,\n",
       "   0.2656053900718689,\n",
       "   -0.2115485519170761,\n",
       "   0.03069951757788658,\n",
       "   -0.19061213731765747,\n",
       "   0.04466821998357773,\n",
       "   -0.16349676251411438,\n",
       "   -0.008528614416718483,\n",
       "   0.26784637570381165,\n",
       "   -0.07709279656410217,\n",
       "   0.32529938220977783,\n",
       "   -0.053822606801986694,\n",
       "   0.07142585515975952,\n",
       "   0.182859867811203,\n",
       "   -0.028945982456207275,\n",
       "   0.0966944545507431,\n",
       "   0.11392461508512497,\n",
       "   0.10665483772754669,\n",
       "   -0.04228641837835312,\n",
       "   0.0512424036860466,\n",
       "   -0.08662618696689606,\n",
       "   -0.2465372532606125,\n",
       "   -0.01491432636976242,\n",
       "   0.14906880259513855,\n",
       "   -0.06340408325195312,\n",
       "   -1.8984962701797485,\n",
       "   0.17997224628925323,\n",
       "   0.05148749053478241,\n",
       "   -0.2742824852466583,\n",
       "   -0.022634774446487427,\n",
       "   -0.1239970251917839,\n",
       "   0.01495140790939331,\n",
       "   -0.03199753910303116,\n",
       "   0.3755049407482147,\n",
       "   0.5077056288719177,\n",
       "   0.05168411135673523,\n",
       "   0.3939163088798523,\n",
       "   -0.0172903910279274,\n",
       "   -0.3436944782733917,\n",
       "   0.12561120092868805,\n",
       "   0.10647272318601608,\n",
       "   0.5563494563102722,\n",
       "   0.16544227302074432,\n",
       "   -0.4380694031715393,\n",
       "   -0.2936632037162781,\n",
       "   0.1345704197883606,\n",
       "   0.41246673464775085,\n",
       "   -0.10533399879932404,\n",
       "   0.1439582109451294,\n",
       "   0.03666131943464279,\n",
       "   0.08479955792427063,\n",
       "   -0.36918801069259644,\n",
       "   0.012730197049677372,\n",
       "   -0.16313442587852478,\n",
       "   0.431342750787735,\n",
       "   -0.15211862325668335,\n",
       "   0.02971786819398403,\n",
       "   -0.1978994756937027,\n",
       "   -0.11578571796417236,\n",
       "   -0.10289070755243301,\n",
       "   -0.11375576257705688,\n",
       "   0.16022799909114838,\n",
       "   0.3630654811859131,\n",
       "   -0.3239903748035431,\n",
       "   -0.06578422337770462,\n",
       "   0.007671738043427467,\n",
       "   0.001818343997001648,\n",
       "   0.13394418358802795,\n",
       "   -0.17648135125637054,\n",
       "   -0.2676340937614441,\n",
       "   0.1519172638654709,\n",
       "   -0.10827609896659851,\n",
       "   -1.3623853921890259,\n",
       "   -0.07092957198619843,\n",
       "   -0.1888505518436432,\n",
       "   -0.16689437627792358,\n",
       "   -0.14233052730560303,\n",
       "   0.1437319964170456,\n",
       "   0.20127657055854797,\n",
       "   -0.0558173730969429,\n",
       "   0.034043051302433014,\n",
       "   -0.12135712802410126,\n",
       "   0.31329816579818726,\n",
       "   0.3236243724822998,\n",
       "   0.301614373922348,\n",
       "   0.07194534689188004,\n",
       "   -0.14758719503879547,\n",
       "   -0.3529120087623596,\n",
       "   -0.1290404349565506,\n",
       "   0.22641095519065857,\n",
       "   0.12182259559631348,\n",
       "   -0.2276514172554016,\n",
       "   -0.04301583394408226,\n",
       "   0.14558732509613037,\n",
       "   -0.58607417345047,\n",
       "   -0.17569565773010254,\n",
       "   -0.010849399492144585,\n",
       "   -0.28004273772239685,\n",
       "   -0.32879331707954407,\n",
       "   -0.1969221979379654,\n",
       "   0.04764225706458092,\n",
       "   -0.11026996374130249,\n",
       "   0.08103182911872864,\n",
       "   4.825360298156738,\n",
       "   -0.3243458569049835,\n",
       "   0.1010005921125412,\n",
       "   -0.05974077060818672,\n",
       "   -0.29431307315826416,\n",
       "   -0.12848220765590668,\n",
       "   0.17817427217960358,\n",
       "   0.037998273968696594,\n",
       "   0.7144266963005066,\n",
       "   0.06037371605634689,\n",
       "   -0.244562566280365,\n",
       "   -0.24537254869937897,\n",
       "   0.040366820991039276,\n",
       "   0.05328486114740372,\n",
       "   -0.4414788484573364,\n",
       "   0.482692152261734,\n",
       "   0.028515439480543137,\n",
       "   -0.11976704001426697,\n",
       "   0.03353565186262131,\n",
       "   -0.1216430515050888,\n",
       "   -0.049542367458343506,\n",
       "   -0.019580304622650146,\n",
       "   0.20781518518924713,\n",
       "   -0.18230155110359192,\n",
       "   0.1222962737083435,\n",
       "   0.02566393092274666,\n",
       "   0.08802035450935364,\n",
       "   0.09810687601566315,\n",
       "   -0.1734490990638733,\n",
       "   -0.250870019197464,\n",
       "   -0.30231142044067383,\n",
       "   0.15177962183952332,\n",
       "   0.2289220094680786,\n",
       "   0.24345572292804718,\n",
       "   0.10894379764795303,\n",
       "   -0.11363254487514496,\n",
       "   0.005332518368959427,\n",
       "   0.016617074608802795,\n",
       "   0.26598063111305237,\n",
       "   0.09045692533254623,\n",
       "   0.3567602336406708,\n",
       "   0.13098542392253876,\n",
       "   0.9233294725418091,\n",
       "   -0.06542506814002991,\n",
       "   0.2404169738292694,\n",
       "   0.30530330538749695,\n",
       "   -0.1372372806072235,\n",
       "   -0.030320487916469574,\n",
       "   0.3635057508945465,\n",
       "   0.14912579953670502,\n",
       "   -0.22279641032218933,\n",
       "   0.223955899477005,\n",
       "   0.13512273132801056,\n",
       "   -0.1511809080839157,\n",
       "   0.05975569039583206,\n",
       "   0.06742765009403229,\n",
       "   0.020081188529729843,\n",
       "   -0.14436708390712738,\n",
       "   -0.13655751943588257,\n",
       "   0.05709066614508629,\n",
       "   -0.02961466647684574,\n",
       "   -0.07548588514328003,\n",
       "   -0.1239301934838295,\n",
       "   -0.17754468321800232,\n",
       "   0.18264055252075195,\n",
       "   -0.2513381242752075,\n",
       "   0.28174254298210144,\n",
       "   -0.1688459813594818,\n",
       "   -0.27026379108428955,\n",
       "   -0.17168927192687988,\n",
       "   -0.08322717249393463,\n",
       "   0.010901546105742455,\n",
       "   -0.550295352935791,\n",
       "   0.26077204942703247,\n",
       "   0.2789183557033539,\n",
       "   -0.01403234526515007,\n",
       "   -0.38232144713401794,\n",
       "   0.16681990027427673,\n",
       "   -0.061552781611680984,\n",
       "   -0.22987574338912964,\n",
       "   -0.11973485350608826,\n",
       "   -0.156490758061409,\n",
       "   -0.020268060266971588,\n",
       "   -0.30564582347869873,\n",
       "   0.05130109190940857,\n",
       "   0.19486738741397858,\n",
       "   -0.288236528635025,\n",
       "   -0.31249040365219116,\n",
       "   -0.03030044585466385,\n",
       "   0.2983812093734741,\n",
       "   0.10992014408111572,\n",
       "   -0.2398691177368164,\n",
       "   -0.047199659049510956,\n",
       "   -0.021207286044955254,\n",
       "   0.020983606576919556,\n",
       "   0.349675714969635,\n",
       "   -0.0722489282488823,\n",
       "   -0.11469598114490509,\n",
       "   -0.001362629234790802,\n",
       "   -0.07586884498596191,\n",
       "   0.3944663107395172,\n",
       "   0.18480412662029266,\n",
       "   0.015379898250102997,\n",
       "   -0.0046892836689949036,\n",
       "   0.14852628111839294,\n",
       "   -0.19586937129497528,\n",
       "   -0.11002076417207718,\n",
       "   0.21834541857242584,\n",
       "   -0.03235179930925369,\n",
       "   0.11129449307918549,\n",
       "   0.2836686968803406,\n",
       "   0.10111206769943237,\n",
       "   0.204070046544075,\n",
       "   -0.19913941621780396,\n",
       "   -0.045815251767635345,\n",
       "   0.12864425778388977,\n",
       "   -0.009162221103906631,\n",
       "   -0.32102689146995544,\n",
       "   -7.354916095733643,\n",
       "   0.2814881503582001,\n",
       "   0.007649119943380356,\n",
       "   0.14334067702293396,\n",
       "   -0.27960604429244995,\n",
       "   -0.09527631103992462,\n",
       "   0.29021406173706055,\n",
       "   0.09965111315250397,\n",
       "   0.17775674164295197,\n",
       "   0.06827154010534286,\n",
       "   -0.29400715231895447,\n",
       "   -0.13373520970344543,\n",
       "   -1.7726619243621826,\n",
       "   0.08600625395774841,\n",
       "   -0.07312236726284027,\n",
       "   0.13071633875370026,\n",
       "   -0.12630948424339294,\n",
       "   -0.8496419787406921,\n",
       "   -0.012155579403042793,\n",
       "   0.34730562567710876,\n",
       "   -0.24470102787017822,\n",
       "   0.1664908081293106,\n",
       "   0.17103543877601624,\n",
       "   0.25135737657546997,\n",
       "   0.23906783759593964,\n",
       "   0.05568687245249748,\n",
       "   -0.02849046140909195,\n",
       "   -0.04394454509019852,\n",
       "   0.07371429353952408,\n",
       "   -0.08893977105617523,\n",
       "   0.024265404790639877,\n",
       "   -0.04721665382385254,\n",
       "   0.39583060145378113,\n",
       "   -0.06935445219278336,\n",
       "   0.0019108301494270563,\n",
       "   -0.2702770531177521,\n",
       "   -0.010834266431629658,\n",
       "   -0.14161303639411926,\n",
       "   0.20045197010040283,\n",
       "   -0.33274388313293457,\n",
       "   -0.09886786341667175,\n",
       "   -0.0310361310839653,\n",
       "   0.16586533188819885,\n",
       "   0.2994541823863983,\n",
       "   -0.36826080083847046,\n",
       "   -0.18471330404281616,\n",
       "   -0.46401023864746094,\n",
       "   -2.1590569019317627,\n",
       "   0.12460511922836304,\n",
       "   0.11814296990633011,\n",
       "   0.3193495571613312,\n",
       "   0.13232392072677612,\n",
       "   0.01676204800605774,\n",
       "   -0.06380067020654678,\n",
       "   -0.1537935882806778,\n",
       "   0.18390701711177826,\n",
       "   -0.009505892172455788,\n",
       "   0.09738504886627197,\n",
       "   0.2518867552280426,\n",
       "   -0.2057875543832779,\n",
       "   -0.214822918176651,\n",
       "   -0.016515634953975677,\n",
       "   0.5897840261459351,\n",
       "   0.052439458668231964,\n",
       "   -0.22500236332416534,\n",
       "   0.3014533221721649,\n",
       "   -0.30591684579849243,\n",
       "   -0.05621124804019928,\n",
       "   -0.19069881737232208,\n",
       "   -0.3809804618358612,\n",
       "   -0.12730631232261658,\n",
       "   -0.013144291937351227,\n",
       "   -0.11826538294553757,\n",
       "   0.21745507419109344,\n",
       "   0.24585257470607758,\n",
       "   0.20594321191310883,\n",
       "   -0.13156172633171082,\n",
       "   0.0384615883231163,\n",
       "   -0.21717911958694458,\n",
       "   -0.019957564771175385,\n",
       "   -0.3735862374305725,\n",
       "   -0.1325725018978119,\n",
       "   0.5215298533439636,\n",
       "   0.23171012103557587,\n",
       "   0.21484822034835815,\n",
       "   -0.20708857476711273,\n",
       "   -0.03268027305603027,\n",
       "   0.2988465428352356,\n",
       "   -0.06309190392494202,\n",
       "   -0.044207312166690826,\n",
       "   -0.11364059895277023,\n",
       "   0.005607292056083679,\n",
       "   0.07338649779558182,\n",
       "   -0.07835130393505096,\n",
       "   -0.1860552430152893,\n",
       "   0.19590282440185547,\n",
       "   -0.23281027376651764,\n",
       "   0.07054181396961212,\n",
       "   -0.24805200099945068,\n",
       "   -0.08754753321409225,\n",
       "   -0.18889039754867554,\n",
       "   -7.182732224464417e-05,\n",
       "   0.04395377263426781,\n",
       "   -0.37124672532081604,\n",
       "   0.05245083570480347,\n",
       "   -0.18718105554580688,\n",
       "   2.4119420051574707,\n",
       "   0.041348934173583984,\n",
       "   -0.0077982209622859955,\n",
       "   -0.05522533878684044,\n",
       "   -0.03203282505273819,\n",
       "   0.2589864730834961,\n",
       "   -0.026213135570287704,\n",
       "   0.2268059402704239,\n",
       "   1.4288944005966187,\n",
       "   0.0999172255396843,\n",
       "   -0.21881452202796936,\n",
       "   -0.03430657833814621,\n",
       "   -0.011796943843364716,\n",
       "   -0.06754474341869354,\n",
       "   -0.027797149494290352,\n",
       "   0.3310866951942444,\n",
       "   -0.09177816659212112,\n",
       "   0.13208672404289246,\n",
       "   0.06792205572128296,\n",
       "   -0.030220214277505875,\n",
       "   0.2578144073486328,\n",
       "   -0.11922529339790344,\n",
       "   0.20231766998767853,\n",
       "   -0.04075711965560913,\n",
       "   -0.04223082959651947,\n",
       "   -0.0688612088561058,\n",
       "   -0.2922442555427551,\n",
       "   -0.23770849406719208,\n",
       "   0.29817646741867065,\n",
       "   -0.3711102306842804,\n",
       "   0.1391029804944992,\n",
       "   -0.0005590058863162994,\n",
       "   -0.2322985976934433,\n",
       "   0.9053874015808105,\n",
       "   -0.13803395628929138,\n",
       "   0.1583106517791748,\n",
       "   0.2712419927120209,\n",
       "   -0.13938620686531067,\n",
       "   0.08495017141103745,\n",
       "   -0.4206404685974121,\n",
       "   -0.06801236420869827,\n",
       "   -0.030804408714175224,\n",
       "   -0.440983384847641,\n",
       "   0.4754047989845276,\n",
       "   -0.07190141081809998,\n",
       "   0.03937526047229767,\n",
       "   -0.03677181154489517,\n",
       "   -0.20400367677211761,\n",
       "   0.15076154470443726,\n",
       "   -0.08466316759586334,\n",
       "   -0.003745436668395996,\n",
       "   -0.237865149974823,\n",
       "   -0.2641668915748596,\n",
       "   0.2623319625854492,\n",
       "   -0.2391541600227356,\n",
       "   0.0754583477973938,\n",
       "   -0.5911702513694763,\n",
       "   -0.08598636090755463,\n",
       "   0.14061510562896729,\n",
       "   -0.08961517363786697,\n",
       "   -0.049170948565006256,\n",
       "   0.011406878009438515,\n",
       "   -0.011288292706012726,\n",
       "   0.051780298352241516,\n",
       "   -0.04352888464927673,\n",
       "   0.1086176410317421,\n",
       "   0.14760953187942505,\n",
       "   -0.14580804109573364,\n",
       "   -0.1142735406756401,\n",
       "   -0.26506307721138,\n",
       "   -0.0852656289935112,\n",
       "   0.16805750131607056,\n",
       "   -2.1287763118743896,\n",
       "   -0.12182048708200455,\n",
       "   0.1302778571844101,\n",
       "   -0.2915593385696411,\n",
       "   -0.2633645236492157,\n",
       "   -0.2972119152545929,\n",
       "   0.28921279311180115,\n",
       "   0.17075982689857483,\n",
       "   0.17035463452339172,\n",
       "   0.3574211597442627,\n",
       "   -0.08114854246377945,\n",
       "   1.8210911750793457,\n",
       "   -0.10423401743173599,\n",
       "   -0.03336162120103836,\n",
       "   -0.08374713361263275,\n",
       "   -0.017731498926877975,\n",
       "   0.18137243390083313,\n",
       "   0.16613143682479858,\n",
       "   -0.1265735775232315,\n",
       "   -0.446445107460022,\n",
       "   -0.09487175196409225,\n",
       "   1.5146443843841553,\n",
       "   0.14150163531303406,\n",
       "   0.34425902366638184,\n",
       "   0.27152761816978455,\n",
       "   -0.20204037427902222,\n",
       "   -0.059525273740291595,\n",
       "   0.10047189146280289,\n",
       "   0.18689686059951782,\n",
       "   -0.009166623465716839,\n",
       "   -0.18844284117221832,\n",
       "   0.2977171540260315,\n",
       "   0.015029499307274818],\n",
       "  [-0.0001528523862361908,\n",
       "   0.08012700080871582,\n",
       "   -0.3884722590446472,\n",
       "   0.2897636294364929,\n",
       "   0.10035362839698792,\n",
       "   -0.11797268688678741,\n",
       "   0.26589858531951904,\n",
       "   -0.3086322844028473,\n",
       "   0.11265406757593155,\n",
       "   0.44062718749046326,\n",
       "   0.03702692687511444,\n",
       "   0.4588068723678589,\n",
       "   0.26594144105911255,\n",
       "   0.0931842252612114,\n",
       "   -0.3193678855895996,\n",
       "   -0.011046665720641613,\n",
       "   -0.11461169272661209,\n",
       "   0.06018771976232529,\n",
       "   0.030406184494495392,\n",
       "   0.03136977180838585,\n",
       "   -0.10687681287527084,\n",
       "   -0.07048787921667099,\n",
       "   0.017662368714809418,\n",
       "   0.1577022671699524,\n",
       "   -0.36370429396629333,\n",
       "   -0.06969467550516129,\n",
       "   0.41403457522392273,\n",
       "   0.016984233632683754,\n",
       "   -0.020112089812755585,\n",
       "   0.25843340158462524,\n",
       "   -0.0542411170899868,\n",
       "   0.11880528926849365,\n",
       "   0.34902408719062805,\n",
       "   0.2729915678501129,\n",
       "   -0.1218070462346077,\n",
       "   0.24036936461925507,\n",
       "   0.02343384549021721,\n",
       "   0.6490574479103088,\n",
       "   -0.12018179893493652,\n",
       "   0.050108011811971664,\n",
       "   0.07274463772773743,\n",
       "   0.09980960935354233,\n",
       "   0.026752494275569916,\n",
       "   -0.260776162147522,\n",
       "   0.26716911792755127,\n",
       "   0.10708962380886078,\n",
       "   -0.15078583359718323,\n",
       "   0.15027450025081635,\n",
       "   0.16827845573425293,\n",
       "   -0.29695871472358704,\n",
       "   0.1583957076072693,\n",
       "   -0.18971040844917297,\n",
       "   -0.11853621155023575,\n",
       "   -0.11579326540231705,\n",
       "   -0.17175506055355072,\n",
       "   -0.1540989726781845,\n",
       "   0.15222039818763733,\n",
       "   0.026889007538557053,\n",
       "   -0.3622489869594574,\n",
       "   0.62801194190979,\n",
       "   -0.5350469946861267,\n",
       "   0.08478527516126633,\n",
       "   0.18905973434448242,\n",
       "   -0.029970891773700714,\n",
       "   -0.18735693395137787,\n",
       "   0.22901196777820587,\n",
       "   -0.07461278885602951,\n",
       "   -0.011661604046821594,\n",
       "   0.124981589615345,\n",
       "   -0.20561760663986206,\n",
       "   0.4230867326259613,\n",
       "   -0.4493594765663147,\n",
       "   -0.23516720533370972,\n",
       "   0.014312516897916794,\n",
       "   -0.2371276319026947,\n",
       "   -0.3079020380973816,\n",
       "   0.06486791372299194,\n",
       "   -0.12396024912595749,\n",
       "   0.12174259126186371,\n",
       "   -0.2739606201648712,\n",
       "   -0.08850382268428802,\n",
       "   0.3755233585834503,\n",
       "   0.014344163239002228,\n",
       "   -0.2604106068611145,\n",
       "   0.02897544950246811,\n",
       "   0.16294503211975098,\n",
       "   0.23932930827140808,\n",
       "   0.10503634810447693,\n",
       "   -0.27573883533477783,\n",
       "   0.2669031023979187,\n",
       "   -0.12328869104385376,\n",
       "   -0.22090287506580353,\n",
       "   -0.25630834698677063,\n",
       "   -0.16973038017749786,\n",
       "   -0.20106612145900726,\n",
       "   0.10432033240795135,\n",
       "   -0.18833062052726746,\n",
       "   0.1075998991727829,\n",
       "   0.21901671588420868,\n",
       "   0.2219955176115036,\n",
       "   0.01820056326687336,\n",
       "   0.13149133324623108,\n",
       "   -0.5911524891853333,\n",
       "   -0.03853912651538849,\n",
       "   -0.1977066844701767,\n",
       "   0.1644415259361267,\n",
       "   0.20242071151733398,\n",
       "   -0.16990050673484802,\n",
       "   -0.0567864365875721,\n",
       "   0.6623990535736084,\n",
       "   0.2702387273311615,\n",
       "   -0.12408778071403503,\n",
       "   0.1522270143032074,\n",
       "   -0.2540279030799866,\n",
       "   -0.31624966859817505,\n",
       "   0.4545537233352661,\n",
       "   0.1786951869726181,\n",
       "   -0.00036853551864624023,\n",
       "   0.15513572096824646,\n",
       "   0.15764005482196808,\n",
       "   0.13469654321670532,\n",
       "   -0.06942030787467957,\n",
       "   0.1624557375907898,\n",
       "   -0.14433375000953674,\n",
       "   0.15981881320476532,\n",
       "   -0.076387919485569,\n",
       "   0.13210731744766235,\n",
       "   -0.17590853571891785,\n",
       "   0.3516254425048828,\n",
       "   0.2392224818468094,\n",
       "   -0.3323215842247009,\n",
       "   0.2902434468269348,\n",
       "   0.22360800206661224,\n",
       "   -0.3945879340171814,\n",
       "   0.06739280372858047,\n",
       "   0.16413748264312744,\n",
       "   -0.10280176997184753,\n",
       "   0.40574386715888977,\n",
       "   -0.4385488033294678,\n",
       "   0.09518909454345703,\n",
       "   0.020760655403137207,\n",
       "   0.07459377497434616,\n",
       "   0.33550629019737244,\n",
       "   0.10091514885425568,\n",
       "   -0.1044139489531517,\n",
       "   0.1840468943119049,\n",
       "   -0.6618697643280029,\n",
       "   -0.31592702865600586,\n",
       "   -0.17223957180976868,\n",
       "   0.2372971773147583,\n",
       "   -0.25026214122772217,\n",
       "   -0.01762564107775688,\n",
       "   -0.0839921161532402,\n",
       "   0.10148411244153976,\n",
       "   -0.3418377935886383,\n",
       "   -0.10066720843315125,\n",
       "   -0.01253143697977066,\n",
       "   0.2818213105201721,\n",
       "   0.36611539125442505,\n",
       "   0.09728732705116272,\n",
       "   -0.27087998390197754,\n",
       "   -0.043160516768693924,\n",
       "   -0.5030598640441895,\n",
       "   0.07583247870206833,\n",
       "   -0.19294431805610657,\n",
       "   0.07756470143795013,\n",
       "   -0.37598949670791626,\n",
       "   0.19966359436511993,\n",
       "   -0.31780320405960083,\n",
       "   0.2505808174610138,\n",
       "   0.03841850534081459,\n",
       "   -0.04967194050550461,\n",
       "   -0.1039886549115181,\n",
       "   0.4728975296020508,\n",
       "   -0.39852696657180786,\n",
       "   -0.32598188519477844,\n",
       "   -0.1102256253361702,\n",
       "   -0.11366556584835052,\n",
       "   -0.33428341150283813,\n",
       "   -0.3177690804004669,\n",
       "   0.1420769840478897,\n",
       "   -0.11092092841863632,\n",
       "   0.13650622963905334,\n",
       "   -0.0608917698264122,\n",
       "   0.0047651417553424835,\n",
       "   -0.178646981716156,\n",
       "   0.17380577325820923,\n",
       "   -0.33048373460769653,\n",
       "   -0.1618754267692566,\n",
       "   0.13877403736114502,\n",
       "   0.26607605814933777,\n",
       "   -0.23538300395011902,\n",
       "   -0.5548015832901001,\n",
       "   0.3508424162864685,\n",
       "   -0.012734171003103256,\n",
       "   -0.001833692193031311,\n",
       "   -0.007569892331957817,\n",
       "   0.20163077116012573,\n",
       "   0.08954044431447983,\n",
       "   0.1518249213695526,\n",
       "   -0.46814092993736267,\n",
       "   0.15189315378665924,\n",
       "   -0.10995199531316757,\n",
       "   0.08503085374832153,\n",
       "   -0.03314637020230293,\n",
       "   0.038498155772686005,\n",
       "   0.42788729071617126,\n",
       "   -0.09630054980516434,\n",
       "   -0.27916309237480164,\n",
       "   -0.12436121702194214,\n",
       "   -0.29365473985671997,\n",
       "   -0.34371790289878845,\n",
       "   -0.18815182149410248,\n",
       "   0.24043625593185425,\n",
       "   -0.045665137469768524,\n",
       "   -0.20900604128837585,\n",
       "   -0.18979723751544952,\n",
       "   -0.21167275309562683,\n",
       "   -0.22652889788150787,\n",
       "   0.28182780742645264,\n",
       "   0.49571558833122253,\n",
       "   -0.14773212373256683,\n",
       "   0.12212135642766953,\n",
       "   0.0011845920234918594,\n",
       "   -0.005463220179080963,\n",
       "   -0.1704462170600891,\n",
       "   -0.05154668167233467,\n",
       "   0.2391967475414276,\n",
       "   0.10938853025436401,\n",
       "   -0.09988327324390411,\n",
       "   -0.16368810832500458,\n",
       "   0.3197430968284607,\n",
       "   0.35496023297309875,\n",
       "   -0.13813532888889313,\n",
       "   0.12288510799407959,\n",
       "   0.042640186846256256,\n",
       "   0.13763141632080078,\n",
       "   -0.14395979046821594,\n",
       "   -0.014802627265453339,\n",
       "   -0.21556240320205688,\n",
       "   -0.10527382791042328,\n",
       "   0.05362609773874283,\n",
       "   0.2700231671333313,\n",
       "   0.35781311988830566,\n",
       "   -0.16150984168052673,\n",
       "   -0.018870655447244644,\n",
       "   -0.08956129103899002,\n",
       "   0.21524262428283691,\n",
       "   0.28005826473236084,\n",
       "   -0.025290098041296005,\n",
       "   0.12048906087875366,\n",
       "   -0.10716897994279861,\n",
       "   -0.060037653893232346,\n",
       "   0.025192752480506897,\n",
       "   0.2860791087150574,\n",
       "   0.2313190996646881,\n",
       "   0.2370048612356186,\n",
       "   0.1806296408176422,\n",
       "   -0.08796036243438721,\n",
       "   -0.60421222448349,\n",
       "   0.03309307247400284,\n",
       "   0.019192293286323547,\n",
       "   -0.09953451156616211,\n",
       "   0.13113057613372803,\n",
       "   -0.1855754256248474,\n",
       "   0.006712840870022774,\n",
       "   -0.11718925833702087,\n",
       "   -0.054636672139167786,\n",
       "   -0.4797154366970062,\n",
       "   -0.006999921053647995,\n",
       "   0.057807207107543945,\n",
       "   -0.15878590941429138,\n",
       "   -0.09185124933719635,\n",
       "   0.45645850896835327,\n",
       "   -0.3218238055706024,\n",
       "   0.24412411451339722,\n",
       "   -0.012574464082717896,\n",
       "   -0.0922943502664566,\n",
       "   0.10339878499507904,\n",
       "   0.05599648132920265,\n",
       "   0.06368481367826462,\n",
       "   0.14838489890098572,\n",
       "   0.06993455439805984,\n",
       "   0.5977004766464233,\n",
       "   0.04449925571680069,\n",
       "   0.25672250986099243,\n",
       "   0.20496836304664612,\n",
       "   0.017163261771202087,\n",
       "   -0.06512807309627533,\n",
       "   0.029148997738957405,\n",
       "   -0.03319743648171425,\n",
       "   -0.20121797919273376,\n",
       "   -0.05652504041790962,\n",
       "   0.09818068891763687,\n",
       "   -0.2953506410121918,\n",
       "   -0.11224687099456787,\n",
       "   -0.4462205171585083,\n",
       "   0.4261648952960968,\n",
       "   -0.4157569110393524,\n",
       "   -0.27478885650634766,\n",
       "   0.03416993468999863,\n",
       "   0.0534609854221344,\n",
       "   0.003058403730392456,\n",
       "   -0.08163159340620041,\n",
       "   -0.2761996388435364,\n",
       "   0.040117308497428894,\n",
       "   0.07725998759269714,\n",
       "   0.34324291348457336,\n",
       "   0.2671559453010559,\n",
       "   0.2691324055194855,\n",
       "   0.3970053791999817,\n",
       "   0.08045603334903717,\n",
       "   0.12817546725273132,\n",
       "   -0.273129940032959,\n",
       "   -0.07787744700908661,\n",
       "   -0.49394863843917847,\n",
       "   -0.26179540157318115,\n",
       "   -0.4598730206489563,\n",
       "   -0.37602829933166504,\n",
       "   -0.2862061858177185,\n",
       "   0.3783940374851227,\n",
       "   0.1063387468457222,\n",
       "   -0.15312176942825317,\n",
       "   0.17783966660499573,\n",
       "   -0.050953008234500885,\n",
       "   -0.044772159308195114,\n",
       "   -0.1803150624036789,\n",
       "   0.3877364993095398,\n",
       "   -0.049768149852752686,\n",
       "   0.029177842661738396,\n",
       "   0.055231474339962006,\n",
       "   -0.07215339690446854,\n",
       "   -0.25619375705718994,\n",
       "   0.051755063235759735,\n",
       "   -0.6389032602310181,\n",
       "   -0.12020543217658997,\n",
       "   -0.42154616117477417,\n",
       "   0.28700506687164307,\n",
       "   -0.016775943338871002,\n",
       "   0.4312608540058136,\n",
       "   0.12270768731832504,\n",
       "   -0.05267127603292465,\n",
       "   0.29488635063171387,\n",
       "   -0.13097353279590607,\n",
       "   0.0009658988565206528,\n",
       "   0.1228676438331604,\n",
       "   0.14946047961711884,\n",
       "   0.16690917313098907,\n",
       "   -0.05604257062077522,\n",
       "   0.3292650580406189,\n",
       "   0.016556020826101303,\n",
       "   0.20510385930538177,\n",
       "   -0.15346142649650574,\n",
       "   -0.13668029010295868,\n",
       "   -0.38155966997146606,\n",
       "   0.09475290775299072,\n",
       "   0.09593715518712997,\n",
       "   -0.09257331490516663,\n",
       "   -0.2515583038330078,\n",
       "   0.5767688155174255,\n",
       "   0.1887921243906021,\n",
       "   0.22871629893779755,\n",
       "   0.09541834145784378,\n",
       "   0.26925644278526306,\n",
       "   0.09543052315711975,\n",
       "   0.26321589946746826,\n",
       "   -0.13490749895572662,\n",
       "   0.4118645191192627,\n",
       "   0.16759982705116272,\n",
       "   -0.09127175807952881,\n",
       "   0.6363311409950256,\n",
       "   0.01829797774553299,\n",
       "   0.2899838387966156,\n",
       "   -0.4428272247314453,\n",
       "   -0.18466950953006744,\n",
       "   -0.17261789739131927,\n",
       "   -0.004524877294898033,\n",
       "   0.07285408675670624,\n",
       "   0.17670057713985443,\n",
       "   -0.37890803813934326,\n",
       "   -0.08752289414405823,\n",
       "   0.0701400488615036,\n",
       "   0.6047356724739075,\n",
       "   -0.08484721183776855,\n",
       "   -0.11216221749782562,\n",
       "   -0.426354318857193,\n",
       "   0.24071523547172546,\n",
       "   -0.21940714120864868,\n",
       "   0.137845978140831,\n",
       "   0.10833153128623962,\n",
       "   -0.33782604336738586,\n",
       "   -0.04937800392508507,\n",
       "   0.2600056827068329,\n",
       "   -0.009205730631947517,\n",
       "   -0.09147585928440094,\n",
       "   -0.563521683216095,\n",
       "   0.1000271663069725,\n",
       "   0.39462053775787354,\n",
       "   -0.041023798286914825,\n",
       "   -0.4200049936771393,\n",
       "   -0.12763741612434387,\n",
       "   0.27446818351745605,\n",
       "   0.07823363691568375,\n",
       "   0.28290364146232605,\n",
       "   -0.2729302644729614,\n",
       "   -0.5816155672073364,\n",
       "   0.6044450402259827,\n",
       "   0.09110778570175171,\n",
       "   0.41094276309013367,\n",
       "   -0.34195607900619507,\n",
       "   0.10724375396966934,\n",
       "   -0.354634165763855,\n",
       "   -0.09115085005760193,\n",
       "   -0.020411711186170578,\n",
       "   -0.05969971418380737,\n",
       "   -0.27264779806137085,\n",
       "   0.19546370208263397,\n",
       "   0.031518664211034775,\n",
       "   0.12798872590065002,\n",
       "   -0.0035047754645347595,\n",
       "   0.18124261498451233,\n",
       "   0.20460206270217896,\n",
       "   -0.10051032900810242,\n",
       "   -0.42367255687713623,\n",
       "   0.13038824498653412,\n",
       "   -0.14669544994831085,\n",
       "   0.20964224636554718,\n",
       "   -0.06844859570264816,\n",
       "   -0.2637392282485962,\n",
       "   0.0233916062861681,\n",
       "   -0.3223361372947693,\n",
       "   0.025356266647577286,\n",
       "   0.0760737881064415,\n",
       "   0.056849244982004166,\n",
       "   -0.5167763829231262,\n",
       "   0.35459113121032715,\n",
       "   -0.05172955244779587,\n",
       "   0.06382811069488525,\n",
       "   0.14279372990131378,\n",
       "   -0.003390185534954071,\n",
       "   0.12436962872743607,\n",
       "   -0.1528158187866211,\n",
       "   -0.16003181040287018,\n",
       "   -0.2587360739707947,\n",
       "   0.0729069858789444,\n",
       "   -0.30474424362182617,\n",
       "   0.15506570041179657,\n",
       "   0.16690288484096527,\n",
       "   0.20128056406974792,\n",
       "   -0.07304370403289795,\n",
       "   0.09384998679161072,\n",
       "   -0.22091582417488098,\n",
       "   0.0654425099492073,\n",
       "   -0.4045066237449646,\n",
       "   0.32622677087783813,\n",
       "   0.2813292145729065,\n",
       "   -0.31252074241638184,\n",
       "   -0.13841797411441803,\n",
       "   -0.1394723653793335,\n",
       "   -0.33873578906059265,\n",
       "   -0.07362598180770874,\n",
       "   0.16972598433494568,\n",
       "   -0.06862638145685196,\n",
       "   -0.17628565430641174,\n",
       "   -0.21169531345367432,\n",
       "   0.18761390447616577,\n",
       "   0.2377990484237671,\n",
       "   0.30580323934555054,\n",
       "   -0.541735053062439,\n",
       "   -0.19614119827747345,\n",
       "   0.08289608359336853,\n",
       "   0.13685950636863708,\n",
       "   -0.09357728064060211,\n",
       "   0.43649110198020935,\n",
       "   -0.12487809360027313,\n",
       "   0.2895357608795166,\n",
       "   -0.02241126447916031,\n",
       "   0.008828057907521725,\n",
       "   -0.4945319890975952,\n",
       "   0.15528523921966553,\n",
       "   -0.05711895599961281,\n",
       "   0.14097490906715393,\n",
       "   -0.0520247146487236,\n",
       "   0.09517773985862732,\n",
       "   0.5504318475723267,\n",
       "   -0.413796603679657,\n",
       "   0.30028361082077026,\n",
       "   0.12164165824651718,\n",
       "   0.5905410647392273,\n",
       "   0.019207019358873367,\n",
       "   0.13608112931251526,\n",
       "   -0.3568136990070343,\n",
       "   0.044689856469631195,\n",
       "   -0.20468366146087646,\n",
       "   0.020639188587665558,\n",
       "   -0.13248126208782196,\n",
       "   -0.3848797082901001,\n",
       "   -0.3477110266685486,\n",
       "   0.01702025718986988,\n",
       "   0.07902881503105164,\n",
       "   -0.07676669210195541,\n",
       "   -0.03057905286550522,\n",
       "   -0.16025790572166443,\n",
       "   0.045919209718704224,\n",
       "   -0.4216354489326477,\n",
       "   0.4108756482601166,\n",
       "   0.08581779897212982,\n",
       "   -0.06589200347661972,\n",
       "   0.048959676176309586,\n",
       "   -0.06127229332923889,\n",
       "   0.1084681823849678,\n",
       "   -0.2658383250236511,\n",
       "   0.37389516830444336,\n",
       "   -0.05418173596262932,\n",
       "   0.2312796413898468,\n",
       "   -0.5158988237380981,\n",
       "   0.18100157380104065,\n",
       "   -0.05507620796561241,\n",
       "   -0.04876968264579773,\n",
       "   -0.13609963655471802,\n",
       "   0.2062523066997528,\n",
       "   0.2940974831581116,\n",
       "   -0.2825431227684021,\n",
       "   -0.08931487798690796,\n",
       "   0.1259918510913849,\n",
       "   0.037133704870939255,\n",
       "   -0.004584472626447678,\n",
       "   0.19247187674045563,\n",
       "   0.21793203055858612,\n",
       "   0.1336880773305893,\n",
       "   -0.29485994577407837,\n",
       "   0.021680910140275955,\n",
       "   -0.34881189465522766,\n",
       "   -0.1665637344121933,\n",
       "   0.7536070942878723,\n",
       "   -0.045908115804195404,\n",
       "   -0.1069842129945755,\n",
       "   0.04415389895439148,\n",
       "   -0.14341269433498383,\n",
       "   0.3505243957042694,\n",
       "   0.30773475766181946,\n",
       "   0.07333313673734665,\n",
       "   -0.28633224964141846,\n",
       "   0.2774140238761902,\n",
       "   -0.15149636566638947,\n",
       "   0.011389419436454773,\n",
       "   0.19610410928726196,\n",
       "   0.020383931696414948,\n",
       "   0.1538536548614502,\n",
       "   0.24062402546405792,\n",
       "   0.32627028226852417,\n",
       "   0.1983088105916977,\n",
       "   -0.5969807505607605,\n",
       "   0.1544753909111023,\n",
       "   0.24537737667560577,\n",
       "   -0.056402385234832764,\n",
       "   -0.10178400576114655,\n",
       "   -9.280999183654785,\n",
       "   -0.008211980573832989,\n",
       "   -0.22246053814888,\n",
       "   0.28255695104599,\n",
       "   -0.1357842981815338,\n",
       "   0.02255398780107498,\n",
       "   -0.058959752321243286,\n",
       "   0.0827081948518753,\n",
       "   -0.3490270674228668,\n",
       "   0.14156951010227203,\n",
       "   0.0511251725256443,\n",
       "   0.037003643810749054,\n",
       "   0.5797572731971741,\n",
       "   -0.4449831545352936,\n",
       "   -0.07033456861972809,\n",
       "   -0.2966596782207489,\n",
       "   0.09307091683149338,\n",
       "   -0.2503170073032379,\n",
       "   0.04351317137479782,\n",
       "   0.053145330399274826,\n",
       "   0.042940251529216766,\n",
       "   0.1016978845000267,\n",
       "   -0.005131766200065613,\n",
       "   0.24255286157131195,\n",
       "   0.404163658618927,\n",
       "   0.2817429304122925,\n",
       "   -0.36682552099227905,\n",
       "   -0.30723634362220764,\n",
       "   0.3987463712692261,\n",
       "   -0.2663612961769104,\n",
       "   -0.1436215341091156,\n",
       "   0.3219505548477173,\n",
       "   -0.21467708051204681,\n",
       "   -0.012920182198286057,\n",
       "   0.5197743773460388,\n",
       "   -0.05033000558614731,\n",
       "   0.19049425423145294,\n",
       "   -0.013731002807617188,\n",
       "   0.021954987198114395,\n",
       "   -0.38184991478919983,\n",
       "   -0.49257567524909973,\n",
       "   -0.08942152559757233,\n",
       "   -0.3465167284011841,\n",
       "   0.25916266441345215,\n",
       "   -0.06432099640369415,\n",
       "   0.15702366828918457,\n",
       "   0.10262426733970642,\n",
       "   0.2639559805393219,\n",
       "   -0.1008630096912384,\n",
       "   -0.051775578409433365,\n",
       "   0.1281125545501709,\n",
       "   0.1597447246313095,\n",
       "   0.3612218499183655,\n",
       "   -0.04579706862568855,\n",
       "   0.15572409331798553,\n",
       "   0.14501693844795227,\n",
       "   0.06949785351753235,\n",
       "   0.4253213703632355,\n",
       "   -0.19818885624408722,\n",
       "   -0.07091191411018372,\n",
       "   0.08120523393154144,\n",
       "   -0.3069649636745453,\n",
       "   0.5813223719596863,\n",
       "   -0.39474838972091675,\n",
       "   -0.26802486181259155,\n",
       "   0.13773827254772186,\n",
       "   0.21234403550624847,\n",
       "   0.07204271852970123,\n",
       "   0.17776191234588623,\n",
       "   -0.3146454393863678,\n",
       "   -0.22207902371883392,\n",
       "   0.3809053599834442,\n",
       "   -0.1651838719844818,\n",
       "   -0.10402192175388336,\n",
       "   -0.1415926218032837,\n",
       "   0.0006810831837356091,\n",
       "   -0.034856319427490234,\n",
       "   0.03225504606962204,\n",
       "   0.12432122975587845,\n",
       "   0.036029282957315445,\n",
       "   -0.09786796569824219,\n",
       "   0.13107208907604218,\n",
       "   0.7350971698760986,\n",
       "   -0.12630487978458405,\n",
       "   -0.04537222161889076,\n",
       "   0.07449889183044434,\n",
       "   0.028340578079223633,\n",
       "   0.4409313201904297,\n",
       "   0.0446055643260479,\n",
       "   0.14423204958438873,\n",
       "   0.007860871031880379,\n",
       "   -0.0653071403503418,\n",
       "   0.08805759996175766,\n",
       "   0.1522849202156067,\n",
       "   0.14748632907867432,\n",
       "   0.33132752776145935,\n",
       "   -0.21360790729522705,\n",
       "   0.3747904300689697,\n",
       "   0.030666716396808624,\n",
       "   0.34952375292778015,\n",
       "   -0.00016951747238636017,\n",
       "   0.05225870758295059,\n",
       "   0.16729342937469482,\n",
       "   -0.3643808960914612,\n",
       "   0.26414111256599426,\n",
       "   -0.16352203488349915,\n",
       "   0.20189157128334045,\n",
       "   -0.21488428115844727,\n",
       "   0.2929266691207886,\n",
       "   -0.020521249622106552,\n",
       "   0.001336132176220417,\n",
       "   0.36190277338027954,\n",
       "   0.2978453040122986,\n",
       "   -0.08858579397201538,\n",
       "   0.2985842227935791,\n",
       "   0.15602806210517883,\n",
       "   -0.055181875824928284,\n",
       "   -0.10142102837562561,\n",
       "   0.4541209042072296,\n",
       "   -0.12852630019187927,\n",
       "   -0.012932762503623962,\n",
       "   -0.49733054637908936,\n",
       "   -0.15248000621795654,\n",
       "   -0.20644310116767883,\n",
       "   0.33944740891456604,\n",
       "   0.1366279572248459,\n",
       "   -0.3632943332195282,\n",
       "   0.013815301470458508,\n",
       "   0.06824729591608047,\n",
       "   -0.1540287584066391,\n",
       "   0.3162429928779602,\n",
       "   0.41283464431762695,\n",
       "   -0.4770158529281616,\n",
       "   0.4547549784183502,\n",
       "   0.19208885729312897,\n",
       "   -0.19714732468128204,\n",
       "   -0.1819489300251007,\n",
       "   0.3014405369758606,\n",
       "   -0.6145211458206177,\n",
       "   0.20278553664684296,\n",
       "   0.01793733984231949,\n",
       "   -0.14070448279380798,\n",
       "   0.1868198812007904,\n",
       "   -0.3969305157661438,\n",
       "   0.5365515351295471,\n",
       "   -0.208066925406456,\n",
       "   -0.3524591326713562,\n",
       "   -0.25644180178642273,\n",
       "   -0.06417781114578247,\n",
       "   -0.03768633306026459,\n",
       "   -0.09187248349189758,\n",
       "   0.47090351581573486,\n",
       "   0.45210930705070496,\n",
       "   -0.011662282049655914,\n",
       "   0.3087194859981537,\n",
       "   -0.050444379448890686,\n",
       "   0.14938941597938538,\n",
       "   -0.08899824321269989,\n",
       "   -0.08712977916002274,\n",
       "   0.30498209595680237,\n",
       "   -0.2803283631801605,\n",
       "   0.1358739584684372,\n",
       "   0.2201908528804779,\n",
       "   -0.2669580578804016,\n",
       "   -0.037612076848745346,\n",
       "   -0.320080041885376,\n",
       "   -0.08830080181360245,\n",
       "   -0.10875891149044037,\n",
       "   0.16118448972702026,\n",
       "   -0.24312564730644226,\n",
       "   -0.21227853000164032,\n",
       "   0.14210674166679382,\n",
       "   -0.05903191864490509,\n",
       "   0.22269602119922638,\n",
       "   -0.11649791896343231,\n",
       "   0.3954915702342987,\n",
       "   -0.369306743144989,\n",
       "   -0.20106133818626404,\n",
       "   -0.06846912205219269,\n",
       "   0.28492429852485657,\n",
       "   0.2513015866279602,\n",
       "   0.1010560467839241,\n",
       "   -0.2658403813838959,\n",
       "   -0.4231833517551422,\n",
       "   0.16404461860656738,\n",
       "   0.26750490069389343,\n",
       "   -0.10294157266616821,\n",
       "   0.22136908769607544,\n",
       "   -0.002261517569422722,\n",
       "   0.1812382936477661,\n",
       "   -0.11932003498077393,\n",
       "   0.0009457394480705261,\n",
       "   0.1542034149169922,\n",
       "   0.21422258019447327,\n",
       "   0.4453597664833069,\n",
       "   0.050382040441036224,\n",
       "   -0.02172943577170372,\n",
       "   -0.07024799287319183,\n",
       "   -0.05593535304069519,\n",
       "   -0.12208332866430283,\n",
       "   0.0423363521695137,\n",
       "   0.1664440929889679,\n",
       "   -0.020931586623191833,\n",
       "   -0.01906181126832962,\n",
       "   0.038001567125320435,\n",
       "   0.1104506328701973,\n",
       "   -0.03967352956533432,\n",
       "   0.659047544002533,\n",
       "   0.19383999705314636,\n",
       "   0.17131578922271729,\n",
       "   -0.008039074018597603],\n",
       "  [-0.053628433495759964,\n",
       "   -0.20546188950538635,\n",
       "   -0.08306899666786194,\n",
       "   0.0538092665374279,\n",
       "   -0.00755320955067873,\n",
       "   -0.5140759944915771,\n",
       "   0.2195970118045807,\n",
       "   -0.020551607012748718,\n",
       "   0.09421882778406143,\n",
       "   0.11344612389802933,\n",
       "   0.27188822627067566,\n",
       "   0.1452200710773468,\n",
       "   -0.4942185878753662,\n",
       "   0.3581477105617523,\n",
       "   -0.5362509489059448,\n",
       "   -0.14989067614078522,\n",
       "   -0.3637552261352539,\n",
       "   -0.23759379982948303,\n",
       "   -0.016823090612888336,\n",
       "   -0.018630962818861008,\n",
       "   0.08365551382303238,\n",
       "   0.047601163387298584,\n",
       "   -0.16935233771800995,\n",
       "   0.28337040543556213,\n",
       "   0.12189805507659912,\n",
       "   -0.4917046129703522,\n",
       "   0.5224188566207886,\n",
       "   0.0815831795334816,\n",
       "   0.07785189151763916,\n",
       "   0.8906058669090271,\n",
       "   0.10127485543489456,\n",
       "   -0.40201810002326965,\n",
       "   0.1718464195728302,\n",
       "   0.3042066991329193,\n",
       "   -0.3309769034385681,\n",
       "   0.42775553464889526,\n",
       "   -0.28686559200286865,\n",
       "   0.5523384809494019,\n",
       "   -0.2891203463077545,\n",
       "   0.37291017174720764,\n",
       "   -0.04067884758114815,\n",
       "   -0.05551299825310707,\n",
       "   -0.13687610626220703,\n",
       "   -0.29994192719459534,\n",
       "   0.24825917184352875,\n",
       "   0.1874232143163681,\n",
       "   0.10128316283226013,\n",
       "   -0.2612585723400116,\n",
       "   -0.32035768032073975,\n",
       "   -0.17901711165905,\n",
       "   0.06644228100776672,\n",
       "   0.2366742193698883,\n",
       "   -0.07292047888040543,\n",
       "   0.037128157913684845,\n",
       "   0.004287973046302795,\n",
       "   -0.13084159791469574,\n",
       "   0.2671530246734619,\n",
       "   -0.20510010421276093,\n",
       "   -0.3359946608543396,\n",
       "   0.6633269786834717,\n",
       "   -0.2897602915763855,\n",
       "   -0.16201446950435638,\n",
       "   -8.549261838197708e-05,\n",
       "   0.04405521601438522,\n",
       "   0.036858998239040375,\n",
       "   -0.017355384305119514,\n",
       "   0.037494808435440063,\n",
       "   -0.25034618377685547,\n",
       "   0.23044928908348083,\n",
       "   -0.19164256751537323,\n",
       "   0.2821589708328247,\n",
       "   0.12021800875663757,\n",
       "   -0.3389100432395935,\n",
       "   -0.15472237765789032,\n",
       "   -0.2756357192993164,\n",
       "   -0.07882487773895264,\n",
       "   0.1807767152786255,\n",
       "   0.06167593598365784,\n",
       "   -0.043881550431251526,\n",
       "   -0.42978301644325256,\n",
       "   -0.1224265843629837,\n",
       "   0.48675164580345154,\n",
       "   -0.0017869099974632263,\n",
       "   0.532903254032135,\n",
       "   -0.20114372670650482,\n",
       "   0.19622421264648438,\n",
       "   0.17177839577198029,\n",
       "   0.6112409234046936,\n",
       "   -0.23867341876029968,\n",
       "   0.4171763062477112,\n",
       "   -0.3697172701358795,\n",
       "   0.037881039083004,\n",
       "   0.23435933887958527,\n",
       "   -0.0038295253179967403,\n",
       "   0.09341978281736374,\n",
       "   0.254961758852005,\n",
       "   -0.34543123841285706,\n",
       "   -0.13669238984584808,\n",
       "   -0.07532760500907898,\n",
       "   0.313283771276474,\n",
       "   0.17452260851860046,\n",
       "   0.29236456751823425,\n",
       "   -0.4504218101501465,\n",
       "   -0.03181268647313118,\n",
       "   -0.10080822557210922,\n",
       "   0.053152430802583694,\n",
       "   0.2828296422958374,\n",
       "   -0.7349061369895935,\n",
       "   -0.10437431931495667,\n",
       "   0.5179678797721863,\n",
       "   0.28394776582717896,\n",
       "   0.04426063597202301,\n",
       "   0.06968265771865845,\n",
       "   -0.25437670946121216,\n",
       "   -0.2528292238712311,\n",
       "   -0.32133617997169495,\n",
       "   0.23128587007522583,\n",
       "   -0.2702181935310364,\n",
       "   -0.11190065741539001,\n",
       "   0.5673361420631409,\n",
       "   0.21817134320735931,\n",
       "   -0.6245008111000061,\n",
       "   -0.14326849579811096,\n",
       "   -0.2814623713493347,\n",
       "   0.01965472102165222,\n",
       "   0.08600930869579315,\n",
       "   -0.012269850820302963,\n",
       "   -0.31468820571899414,\n",
       "   0.08279576897621155,\n",
       "   0.4800521731376648,\n",
       "   -0.23965208232402802,\n",
       "   0.3956538438796997,\n",
       "   -0.008915469981729984,\n",
       "   -0.15879030525684357,\n",
       "   0.1531282663345337,\n",
       "   0.20323225855827332,\n",
       "   -0.3276326060295105,\n",
       "   0.7224743366241455,\n",
       "   -0.5931819081306458,\n",
       "   0.218348890542984,\n",
       "   -0.08925054967403412,\n",
       "   0.1211244985461235,\n",
       "   0.462675541639328,\n",
       "   0.11335767805576324,\n",
       "   0.4537339210510254,\n",
       "   -0.27412745356559753,\n",
       "   -0.17266836762428284,\n",
       "   -0.3696818947792053,\n",
       "   -0.3810664713382721,\n",
       "   0.01423464436084032,\n",
       "   -0.5636179447174072,\n",
       "   -0.04017111659049988,\n",
       "   -0.23603153228759766,\n",
       "   0.36006611585617065,\n",
       "   -0.5233234167098999,\n",
       "   -0.21323132514953613,\n",
       "   0.028268342837691307,\n",
       "   0.55010986328125,\n",
       "   0.6047947406768799,\n",
       "   0.28056082129478455,\n",
       "   0.15005290508270264,\n",
       "   -0.21077582240104675,\n",
       "   -0.4876576066017151,\n",
       "   0.2879696488380432,\n",
       "   -0.09444671124219894,\n",
       "   0.15870557725429535,\n",
       "   -0.3503021001815796,\n",
       "   0.17914970219135284,\n",
       "   -0.17136511206626892,\n",
       "   0.3119804859161377,\n",
       "   0.48437604308128357,\n",
       "   0.12355881184339523,\n",
       "   0.19923923909664154,\n",
       "   0.10623814165592194,\n",
       "   -0.15028779208660126,\n",
       "   -0.4986594617366791,\n",
       "   0.2123170793056488,\n",
       "   -0.4560202360153198,\n",
       "   -0.05247616022825241,\n",
       "   0.002665884792804718,\n",
       "   0.24121488630771637,\n",
       "   -0.38238710165023804,\n",
       "   -0.3125019669532776,\n",
       "   -0.5896744728088379,\n",
       "   -0.21716350317001343,\n",
       "   -0.11549344658851624,\n",
       "   0.21698270738124847,\n",
       "   -0.26323434710502625,\n",
       "   -0.07301415503025055,\n",
       "   -0.06658970564603806,\n",
       "   -0.0639989823102951,\n",
       "   -0.09268242865800858,\n",
       "   -0.5809712409973145,\n",
       "   0.16412889957427979,\n",
       "   -0.08313966542482376,\n",
       "   0.034542664885520935,\n",
       "   -0.12032540142536163,\n",
       "   0.2079605758190155,\n",
       "   -0.02846001833677292,\n",
       "   -0.14473554491996765,\n",
       "   -0.09370671957731247,\n",
       "   -0.3057252764701843,\n",
       "   0.2168678492307663,\n",
       "   0.22809286415576935,\n",
       "   0.15786443650722504,\n",
       "   0.3060407340526581,\n",
       "   -0.28940290212631226,\n",
       "   0.2635148763656616,\n",
       "   -0.35048776865005493,\n",
       "   -0.04635332152247429,\n",
       "   0.18372482061386108,\n",
       "   -0.13899172842502594,\n",
       "   -0.3103474974632263,\n",
       "   0.2299630492925644,\n",
       "   -0.03741799294948578,\n",
       "   -0.19104573130607605,\n",
       "   -0.16201290488243103,\n",
       "   -0.08577948808670044,\n",
       "   -0.16318336129188538,\n",
       "   -0.08298169076442719,\n",
       "   -0.148847758769989,\n",
       "   0.07608779519796371,\n",
       "   0.4617289900779724,\n",
       "   -0.26464149355888367,\n",
       "   -0.5016830563545227,\n",
       "   -0.36229825019836426,\n",
       "   0.2589700520038605,\n",
       "   0.2869434654712677,\n",
       "   0.5618934035301208,\n",
       "   -0.06565504521131516,\n",
       "   -0.18244314193725586,\n",
       "   -0.2258090227842331,\n",
       "   0.36543965339660645,\n",
       "   -0.0060097030363976955,\n",
       "   0.13818834722042084,\n",
       "   0.4065084755420685,\n",
       "   0.023546874523162842,\n",
       "   -0.026606377214193344,\n",
       "   -0.14492566883563995,\n",
       "   -0.8568018674850464,\n",
       "   0.15001320838928223,\n",
       "   0.17898762226104736,\n",
       "   -0.13954409956932068,\n",
       "   0.4933997392654419,\n",
       "   -0.09181042015552521,\n",
       "   -0.14665916562080383,\n",
       "   -0.233723983168602,\n",
       "   0.3207128942012787,\n",
       "   0.389182448387146,\n",
       "   -0.3181246519088745,\n",
       "   -0.17354285717010498,\n",
       "   -0.361479789018631,\n",
       "   0.0012782514095306396,\n",
       "   -0.4159020483493805,\n",
       "   0.585823118686676,\n",
       "   -0.141286700963974,\n",
       "   0.52716463804245,\n",
       "   0.01672186702489853,\n",
       "   0.21780255436897278,\n",
       "   -0.6339843273162842,\n",
       "   0.20317141711711884,\n",
       "   -0.07165417820215225,\n",
       "   0.005372103303670883,\n",
       "   -0.04978392273187637,\n",
       "   -0.3973105251789093,\n",
       "   -0.27036264538764954,\n",
       "   0.09139536321163177,\n",
       "   0.2877705991268158,\n",
       "   -0.46449124813079834,\n",
       "   0.34780776500701904,\n",
       "   -0.05130694806575775,\n",
       "   -0.22551843523979187,\n",
       "   -0.06966561079025269,\n",
       "   0.15498080849647522,\n",
       "   -0.19721829891204834,\n",
       "   0.07669790089130402,\n",
       "   -0.060481853783130646,\n",
       "   0.00046518445014953613,\n",
       "   -0.4507625102996826,\n",
       "   0.06659838557243347,\n",
       "   0.4352515935897827,\n",
       "   0.42048847675323486,\n",
       "   0.3691731095314026,\n",
       "   0.2501049041748047,\n",
       "   -0.5123943090438843,\n",
       "   0.3424644470214844,\n",
       "   0.337398499250412,\n",
       "   0.34797272086143494,\n",
       "   -0.2168130874633789,\n",
       "   -0.26511162519454956,\n",
       "   0.1633341759443283,\n",
       "   0.258056640625,\n",
       "   -0.707870602607727,\n",
       "   0.1640683263540268,\n",
       "   -0.11145317554473877,\n",
       "   -0.010197889059782028,\n",
       "   -0.05493519827723503,\n",
       "   0.3640066981315613,\n",
       "   -0.3444761335849762,\n",
       "   -0.25349700450897217,\n",
       "   -0.37674251198768616,\n",
       "   0.08505486696958542,\n",
       "   0.25624576210975647,\n",
       "   0.13639160990715027,\n",
       "   -0.18084189295768738,\n",
       "   -0.8595173954963684,\n",
       "   0.17695659399032593,\n",
       "   -0.1289864331483841,\n",
       "   -0.0677836686372757,\n",
       "   0.5088977217674255,\n",
       "   0.4143110513687134,\n",
       "   0.22648970782756805,\n",
       "   0.19789594411849976,\n",
       "   -0.3591471314430237,\n",
       "   0.14089614152908325,\n",
       "   0.06850355863571167,\n",
       "   -0.3873603641986847,\n",
       "   -0.517837643623352,\n",
       "   -0.23305046558380127,\n",
       "   -0.1453213393688202,\n",
       "   0.49716803431510925,\n",
       "   0.5020580291748047,\n",
       "   -0.5886228084564209,\n",
       "   0.19678479433059692,\n",
       "   -0.0651586577296257,\n",
       "   -0.34925898909568787,\n",
       "   0.013919498771429062,\n",
       "   0.3298570513725281,\n",
       "   0.14004014432430267,\n",
       "   -0.19393083453178406,\n",
       "   0.01717285066843033,\n",
       "   0.18248069286346436,\n",
       "   -0.06340719014406204,\n",
       "   -0.5034670233726501,\n",
       "   -0.39561477303504944,\n",
       "   -0.12905916571617126,\n",
       "   -0.27686193585395813,\n",
       "   0.24908895790576935,\n",
       "   -0.11404205858707428,\n",
       "   0.32252877950668335,\n",
       "   0.1821846067905426,\n",
       "   -0.008789725601673126,\n",
       "   0.24241062998771667,\n",
       "   -0.2529586851596832,\n",
       "   0.40485215187072754,\n",
       "   -0.1789896935224533,\n",
       "   -0.05180545523762703,\n",
       "   -0.22442427277565002,\n",
       "   -0.06441351771354675,\n",
       "   0.4386650621891022,\n",
       "   0.008010797202587128,\n",
       "   0.08875048160552979,\n",
       "   -0.023501835763454437,\n",
       "   0.07542267441749573,\n",
       "   0.2902790307998657,\n",
       "   0.25015103816986084,\n",
       "   0.020081738010048866,\n",
       "   0.05902765691280365,\n",
       "   0.0960393100976944,\n",
       "   0.21346552670001984,\n",
       "   0.149638831615448,\n",
       "   -0.1420275717973709,\n",
       "   -0.5195450186729431,\n",
       "   -0.1071547418832779,\n",
       "   0.03891272097826004,\n",
       "   0.04277048259973526,\n",
       "   0.1718829870223999,\n",
       "   -0.0015733316540718079,\n",
       "   0.3909705579280853,\n",
       "   -0.23781214654445648,\n",
       "   0.742463231086731,\n",
       "   -0.3355291783809662,\n",
       "   0.2569003403186798,\n",
       "   -0.2982504069805145,\n",
       "   -0.06990724802017212,\n",
       "   -0.07204805314540863,\n",
       "   -0.2993987798690796,\n",
       "   0.006115833297371864,\n",
       "   0.02781815640628338,\n",
       "   -0.45587438344955444,\n",
       "   -0.09735442698001862,\n",
       "   0.21095095574855804,\n",
       "   0.6865813136100769,\n",
       "   -0.18912450969219208,\n",
       "   0.23749594390392303,\n",
       "   -0.47736242413520813,\n",
       "   0.1705969274044037,\n",
       "   -0.08132607489824295,\n",
       "   0.3654671907424927,\n",
       "   0.13374386727809906,\n",
       "   0.4204627573490143,\n",
       "   -0.023232080042362213,\n",
       "   0.3555743098258972,\n",
       "   0.07926107943058014,\n",
       "   -0.2681882679462433,\n",
       "   -0.1540038287639618,\n",
       "   -0.3109882175922394,\n",
       "   0.15976473689079285,\n",
       "   0.07967285811901093,\n",
       "   -0.19272752106189728,\n",
       "   -0.3793775737285614,\n",
       "   0.1990978717803955,\n",
       "   -0.06992671638727188,\n",
       "   -0.19074353575706482,\n",
       "   -0.1263803392648697,\n",
       "   -0.5467635989189148,\n",
       "   0.01399881299585104,\n",
       "   0.1852327138185501,\n",
       "   0.182278111577034,\n",
       "   0.03600446134805679,\n",
       "   -0.08682482689619064,\n",
       "   -0.3155367076396942,\n",
       "   -0.18816640973091125,\n",
       "   0.5235683917999268,\n",
       "   0.3203895092010498,\n",
       "   -0.39599892497062683,\n",
       "   0.23779204487800598,\n",
       "   -0.12753741443157196,\n",
       "   0.10524669289588928,\n",
       "   0.036331117153167725,\n",
       "   0.09812194108963013,\n",
       "   -0.1376531422138214,\n",
       "   -0.17168346047401428,\n",
       "   -0.1537409871816635,\n",
       "   0.2586640417575836,\n",
       "   -0.16837778687477112,\n",
       "   0.08551982045173645,\n",
       "   -0.31158867478370667,\n",
       "   -0.4288038909435272,\n",
       "   0.5075846910476685,\n",
       "   0.001755598932504654,\n",
       "   -0.3010876774787903,\n",
       "   0.00902608036994934,\n",
       "   0.2797880470752716,\n",
       "   -0.4239887297153473,\n",
       "   0.5470952391624451,\n",
       "   0.3156229853630066,\n",
       "   0.44325917959213257,\n",
       "   -0.09581688046455383,\n",
       "   0.1689172089099884,\n",
       "   0.17567482590675354,\n",
       "   -0.25448280572891235,\n",
       "   -0.049544475972652435,\n",
       "   -0.46157345175743103,\n",
       "   -0.23219461739063263,\n",
       "   -0.30620691180229187,\n",
       "   0.27848103642463684,\n",
       "   -0.36728063225746155,\n",
       "   0.28286993503570557,\n",
       "   -0.009174332022666931,\n",
       "   0.27188998460769653,\n",
       "   -0.19910255074501038,\n",
       "   -0.05488840863108635,\n",
       "   -0.3975692391395569,\n",
       "   0.3716479539871216,\n",
       "   -0.12217612564563751,\n",
       "   -0.27177584171295166,\n",
       "   -0.24256259202957153,\n",
       "   -0.14728525280952454,\n",
       "   -0.20084869861602783,\n",
       "   0.07986044883728027,\n",
       "   0.24943837523460388,\n",
       "   0.33698856830596924,\n",
       "   0.47297903895378113,\n",
       "   -0.038900505751371384,\n",
       "   0.0003793463110923767,\n",
       "   0.23036211729049683,\n",
       "   0.37665221095085144,\n",
       "   0.08140769600868225,\n",
       "   -0.06075776368379593,\n",
       "   0.5382667183876038,\n",
       "   0.06421865522861481,\n",
       "   0.20213301479816437,\n",
       "   0.41386595368385315,\n",
       "   -0.19301608204841614,\n",
       "   -0.21939028799533844,\n",
       "   -0.16473546624183655,\n",
       "   0.48991909623146057,\n",
       "   -0.5034284591674805,\n",
       "   -0.07250379770994186,\n",
       "   -0.12600815296173096,\n",
       "   -0.10426293313503265,\n",
       "   -0.13484913110733032,\n",
       "   -0.17648667097091675,\n",
       "   0.5261837840080261,\n",
       "   -0.29050129652023315,\n",
       "   0.6408197283744812,\n",
       "   -0.05856490880250931,\n",
       "   0.06075053662061691,\n",
       "   0.30638498067855835,\n",
       "   -0.2671172320842743,\n",
       "   -0.27884548902511597,\n",
       "   0.24276196956634521,\n",
       "   0.11250515282154083,\n",
       "   -0.03693275526165962,\n",
       "   0.021093379706144333,\n",
       "   -0.1577623337507248,\n",
       "   -0.14244306087493896,\n",
       "   -0.30425509810447693,\n",
       "   0.12864717841148376,\n",
       "   -0.5330513119697571,\n",
       "   -0.18862155079841614,\n",
       "   0.1428266167640686,\n",
       "   0.031086770817637444,\n",
       "   -0.2284582555294037,\n",
       "   0.8762874007225037,\n",
       "   0.5793570280075073,\n",
       "   -0.4325779676437378,\n",
       "   0.18803808093070984,\n",
       "   -0.21602210402488708,\n",
       "   -0.20937368273735046,\n",
       "   -0.6137093305587769,\n",
       "   -0.023942694067955017,\n",
       "   0.0376744270324707,\n",
       "   -0.1254625767469406,\n",
       "   -0.20988577604293823,\n",
       "   0.3158891201019287,\n",
       "   -0.12539811432361603,\n",
       "   -0.009359423071146011,\n",
       "   0.20354558527469635,\n",
       "   0.6587964296340942,\n",
       "   0.24253302812576294,\n",
       "   -0.21142543852329254,\n",
       "   -0.25033846497535706,\n",
       "   -0.044923726469278336,\n",
       "   0.273796409368515,\n",
       "   0.760678768157959,\n",
       "   0.09243753552436829,\n",
       "   0.08066035807132721,\n",
       "   0.33663469552993774,\n",
       "   -0.3980688154697418,\n",
       "   0.2919398844242096,\n",
       "   -0.004101406782865524,\n",
       "   0.07624003291130066,\n",
       "   0.8717880249023438,\n",
       "   -0.04986675828695297,\n",
       "   -0.17173413932323456,\n",
       "   0.09651818126440048,\n",
       "   -0.41367802023887634,\n",
       "   0.1812724769115448,\n",
       "   0.05077086761593819,\n",
       "   -0.09530939161777496,\n",
       "   -0.045974068343639374,\n",
       "   0.12423889338970184,\n",
       "   -0.16614370048046112,\n",
       "   0.3890906274318695,\n",
       "   0.2579617500305176,\n",
       "   0.28161191940307617,\n",
       "   0.11072981357574463,\n",
       "   0.2618429660797119,\n",
       "   0.000868555624037981,\n",
       "   -0.06953424215316772,\n",
       "   -0.7481271028518677,\n",
       "   0.15289048850536346,\n",
       "   0.22463274002075195,\n",
       "   -0.0036384910345077515,\n",
       "   -0.12603428959846497,\n",
       "   -8.999353408813477,\n",
       "   0.3217075765132904,\n",
       "   -0.044646523892879486,\n",
       "   0.16836154460906982,\n",
       "   0.3016793727874756,\n",
       "   -0.03146413713693619,\n",
       "   0.029475238174200058,\n",
       "   -0.6308783292770386,\n",
       "   -0.7517756819725037,\n",
       "   0.18763794004917145,\n",
       "   0.010063417255878448,\n",
       "   0.09255556762218475,\n",
       "   0.41599908471107483,\n",
       "   -0.28053486347198486,\n",
       "   0.1616559475660324,\n",
       "   -0.2639472484588623,\n",
       "   0.22217777371406555,\n",
       "   -0.5653947591781616,\n",
       "   0.364921897649765,\n",
       "   -0.08855648338794708,\n",
       "   0.0613088384270668,\n",
       "   0.44148775935173035,\n",
       "   -0.2206115573644638,\n",
       "   0.4304499328136444,\n",
       "   0.12455376237630844,\n",
       "   0.34147781133651733,\n",
       "   -0.012088023126125336,\n",
       "   -0.060788657516241074,\n",
       "   0.2713713049888611,\n",
       "   -0.10275986045598984,\n",
       "   -0.13614407181739807,\n",
       "   0.36437076330184937,\n",
       "   0.002964276820421219,\n",
       "   -0.2301507145166397,\n",
       "   0.47138240933418274,\n",
       "   -0.2655622661113739,\n",
       "   0.2382424771785736,\n",
       "   0.0981232225894928,\n",
       "   0.1551550328731537,\n",
       "   -0.5550956726074219,\n",
       "   -0.20755426585674286,\n",
       "   -0.04118381068110466,\n",
       "   -0.17978762090206146,\n",
       "   0.2723587453365326,\n",
       "   0.1341431438922882,\n",
       "   0.01953957974910736,\n",
       "   0.3302074372768402,\n",
       "   0.41794058680534363,\n",
       "   -0.14288091659545898,\n",
       "   -0.21366262435913086,\n",
       "   0.26190030574798584,\n",
       "   0.4650757908821106,\n",
       "   0.0870150700211525,\n",
       "   0.189468115568161,\n",
       "   0.34810739755630493,\n",
       "   0.23744040727615356,\n",
       "   -0.048566997051239014,\n",
       "   0.6342607140541077,\n",
       "   -0.27459728717803955,\n",
       "   -0.3747442960739136,\n",
       "   0.39568522572517395,\n",
       "   -0.19444778561592102,\n",
       "   0.8582136034965515,\n",
       "   0.016868487000465393,\n",
       "   0.002172771841287613,\n",
       "   0.29701757431030273,\n",
       "   0.07636339962482452,\n",
       "   -0.08508987724781036,\n",
       "   0.20625121891498566,\n",
       "   -0.41840478777885437,\n",
       "   -0.4455191493034363,\n",
       "   -0.2013373076915741,\n",
       "   0.10678382217884064,\n",
       "   -0.5432327389717102,\n",
       "   -0.286034494638443,\n",
       "   -0.22817830741405487,\n",
       "   -0.19513845443725586,\n",
       "   -0.007889818400144577,\n",
       "   -0.020244568586349487,\n",
       "   0.017201531678438187,\n",
       "   -0.44013699889183044,\n",
       "   0.6403217315673828,\n",
       "   0.7450941205024719,\n",
       "   -0.35215115547180176,\n",
       "   -0.13234218955039978,\n",
       "   -0.22960549592971802,\n",
       "   -0.0628032386302948,\n",
       "   0.5212649703025818,\n",
       "   0.23034268617630005,\n",
       "   -0.14702080190181732,\n",
       "   -0.08800306171178818,\n",
       "   0.2400398850440979,\n",
       "   0.2587001621723175,\n",
       "   0.24559563398361206,\n",
       "   -0.21311619877815247,\n",
       "   0.416113018989563,\n",
       "   -0.03294992446899414,\n",
       "   0.01695289835333824,\n",
       "   -0.0796751156449318,\n",
       "   0.007561853155493736,\n",
       "   -0.06319014728069305,\n",
       "   -0.14362996816635132,\n",
       "   -0.6314523220062256,\n",
       "   -0.23479849100112915,\n",
       "   -0.03001689538359642,\n",
       "   -0.1388438493013382,\n",
       "   -0.06113136187195778,\n",
       "   -0.37784022092819214,\n",
       "   0.5381003022193909,\n",
       "   0.31611013412475586,\n",
       "   -0.1939796805381775,\n",
       "   0.3004313111305237,\n",
       "   0.5870152711868286,\n",
       "   -0.015773478895425797,\n",
       "   0.272172212600708,\n",
       "   0.39699387550354004,\n",
       "   -0.20656096935272217,\n",
       "   -0.3515062928199768,\n",
       "   -0.024801140651106834,\n",
       "   -0.0773540660738945,\n",
       "   -0.13374055922031403,\n",
       "   -0.2900840938091278,\n",
       "   -0.32052141427993774,\n",
       "   -0.2635995149612427,\n",
       "   0.2411082237958908,\n",
       "   0.10319620370864868,\n",
       "   -0.22554440796375275,\n",
       "   -0.5072115659713745,\n",
       "   0.017501864582300186,\n",
       "   -0.028519509360194206,\n",
       "   0.6414296627044678,\n",
       "   0.3313780725002289,\n",
       "   -0.33629387617111206,\n",
       "   0.39463570713996887,\n",
       "   0.007589522749185562,\n",
       "   -0.052944499999284744,\n",
       "   -0.2747684121131897,\n",
       "   0.1670638471841812,\n",
       "   -0.45583051443099976,\n",
       "   0.011317556723952293,\n",
       "   -0.25334712862968445,\n",
       "   -0.16745778918266296,\n",
       "   0.8323476910591125,\n",
       "   0.06274230778217316,\n",
       "   0.6852892637252808,\n",
       "   0.03830650448799133,\n",
       "   -0.03053838014602661,\n",
       "   -0.21385082602500916,\n",
       "   -0.052978094667196274,\n",
       "   -0.12857568264007568,\n",
       "   -0.07907024770975113,\n",
       "   0.5193735361099243,\n",
       "   0.36844316124916077,\n",
       "   0.08162873238325119,\n",
       "   0.205412358045578,\n",
       "   -0.4686743915081024,\n",
       "   3.1620264053344727e-05,\n",
       "   0.12128911167383194,\n",
       "   -0.6480852365493774,\n",
       "   0.03944750502705574,\n",
       "   0.17222583293914795,\n",
       "   -0.012855201959609985,\n",
       "   0.0363197959959507,\n",
       "   0.011874653398990631,\n",
       "   -0.3375821113586426,\n",
       "   -0.12635093927383423,\n",
       "   -0.11699516326189041,\n",
       "   0.09132549166679382,\n",
       "   0.031856782734394073,\n",
       "   -0.1854023039340973,\n",
       "   -0.17000265419483185,\n",
       "   -0.033672772347927094,\n",
       "   0.11955328285694122,\n",
       "   0.19259686768054962,\n",
       "   0.06709850579500198,\n",
       "   -0.04844575747847557,\n",
       "   0.2534991204738617,\n",
       "   -0.2253170907497406,\n",
       "   0.3690546751022339,\n",
       "   0.2651667594909668,\n",
       "   0.5691766142845154,\n",
       "   0.28862911462783813,\n",
       "   -0.15276749432086945,\n",
       "   -0.7804216146469116,\n",
       "   -0.2748808264732361,\n",
       "   0.49056121706962585,\n",
       "   0.45452290773391724,\n",
       "   0.05469168722629547,\n",
       "   -0.13031694293022156,\n",
       "   0.38475310802459717,\n",
       "   0.019470863044261932,\n",
       "   0.051459163427352905,\n",
       "   -0.032505206763744354,\n",
       "   0.22893409430980682,\n",
       "   -0.04318123310804367,\n",
       "   0.4371698796749115,\n",
       "   0.08685034513473511,\n",
       "   0.31041666865348816,\n",
       "   -0.3181530237197876,\n",
       "   -0.11609188467264175,\n",
       "   0.19027084112167358,\n",
       "   0.22529128193855286,\n",
       "   -0.022953294217586517,\n",
       "   -0.43045422434806824,\n",
       "   0.2519223988056183,\n",
       "   -0.03491301089525223,\n",
       "   -0.0811808705329895,\n",
       "   0.2382938712835312,\n",
       "   0.38286837935447693,\n",
       "   -0.08544893562793732,\n",
       "   0.2137894630432129],\n",
       "  [0.2804655134677887,\n",
       "   -0.1907060444355011,\n",
       "   -0.07161229103803635,\n",
       "   0.13770967721939087,\n",
       "   0.046841733157634735,\n",
       "   -0.18801815807819366,\n",
       "   0.24970123171806335,\n",
       "   -0.018011078238487244,\n",
       "   -0.417228102684021,\n",
       "   0.2166614532470703,\n",
       "   0.24365930259227753,\n",
       "   0.19053496420383453,\n",
       "   -0.5529881119728088,\n",
       "   0.4117094576358795,\n",
       "   -0.3722991645336151,\n",
       "   0.27769607305526733,\n",
       "   0.2829170227050781,\n",
       "   -0.20407457649707794,\n",
       "   0.040250398218631744,\n",
       "   0.11470457911491394,\n",
       "   0.0803893581032753,\n",
       "   0.05077836662530899,\n",
       "   -0.30934083461761475,\n",
       "   -0.004048837348818779,\n",
       "   -0.08218865096569061,\n",
       "   -0.502585768699646,\n",
       "   0.212014302611351,\n",
       "   0.15799151360988617,\n",
       "   0.5911697149276733,\n",
       "   0.43270114064216614,\n",
       "   0.15369082987308502,\n",
       "   -0.27464982867240906,\n",
       "   0.47410887479782104,\n",
       "   0.04739467054605484,\n",
       "   -0.2523057162761688,\n",
       "   0.39995408058166504,\n",
       "   -0.08065534383058548,\n",
       "   0.41212329268455505,\n",
       "   -0.13912659883499146,\n",
       "   0.14276157319545746,\n",
       "   0.215436652302742,\n",
       "   -0.21752890944480896,\n",
       "   -0.15143758058547974,\n",
       "   -0.5182113647460938,\n",
       "   0.28600502014160156,\n",
       "   -0.41041356325149536,\n",
       "   0.14714454114437103,\n",
       "   -0.31390729546546936,\n",
       "   -0.2017965018749237,\n",
       "   0.035711225122213364,\n",
       "   -0.005969796795397997,\n",
       "   0.37444961071014404,\n",
       "   -0.6001504063606262,\n",
       "   -0.16806063055992126,\n",
       "   -0.09315270185470581,\n",
       "   -0.32471194863319397,\n",
       "   0.15698370337486267,\n",
       "   -0.0006217285990715027,\n",
       "   -0.4299667775630951,\n",
       "   0.6649530529975891,\n",
       "   -0.3831707835197449,\n",
       "   0.4111851751804352,\n",
       "   -0.0182939525693655,\n",
       "   0.2554060220718384,\n",
       "   0.1453137844800949,\n",
       "   0.3480277359485626,\n",
       "   -0.09926337003707886,\n",
       "   0.005301386117935181,\n",
       "   0.24564117193222046,\n",
       "   -0.472210168838501,\n",
       "   0.12551593780517578,\n",
       "   0.2110871821641922,\n",
       "   -0.3383448123931885,\n",
       "   -0.04597637802362442,\n",
       "   0.22009718418121338,\n",
       "   -0.4041934013366699,\n",
       "   0.08835441619157791,\n",
       "   -0.13787689805030823,\n",
       "   0.36143848299980164,\n",
       "   -0.25795990228652954,\n",
       "   0.19070200622081757,\n",
       "   0.6091406345367432,\n",
       "   -0.04646334797143936,\n",
       "   0.5479344725608826,\n",
       "   0.08290398120880127,\n",
       "   0.033863767981529236,\n",
       "   -0.05032045394182205,\n",
       "   0.38688990473747253,\n",
       "   0.1379457265138626,\n",
       "   0.36166971921920776,\n",
       "   -0.24461819231510162,\n",
       "   -0.15714029967784882,\n",
       "   0.23653195798397064,\n",
       "   -0.21548375487327576,\n",
       "   -0.21891862154006958,\n",
       "   0.30615895986557007,\n",
       "   -0.2593446969985962,\n",
       "   -0.38854265213012695,\n",
       "   -0.46611711382865906,\n",
       "   0.2384142130613327,\n",
       "   -0.05474325269460678,\n",
       "   0.08159210532903671,\n",
       "   -0.3059583008289337,\n",
       "   -0.13717281818389893,\n",
       "   -0.0364375114440918,\n",
       "   0.14795999228954315,\n",
       "   -0.20264019072055817,\n",
       "   -0.5799487829208374,\n",
       "   -0.25558745861053467,\n",
       "   0.5630896091461182,\n",
       "   0.025393851101398468,\n",
       "   0.11531218886375427,\n",
       "   -0.16514135897159576,\n",
       "   0.07766440510749817,\n",
       "   -0.022659482434391975,\n",
       "   -0.059841666370630264,\n",
       "   -0.15539312362670898,\n",
       "   -0.42498230934143066,\n",
       "   -0.35624662041664124,\n",
       "   0.207573801279068,\n",
       "   0.1738976538181305,\n",
       "   0.1441880166530609,\n",
       "   -0.07639679312705994,\n",
       "   -0.22204676270484924,\n",
       "   0.10295382142066956,\n",
       "   -0.09547211229801178,\n",
       "   -0.19198843836784363,\n",
       "   -0.3428853154182434,\n",
       "   -0.08687970042228699,\n",
       "   0.4575647711753845,\n",
       "   -0.21950040757656097,\n",
       "   -0.18788956105709076,\n",
       "   0.06836862862110138,\n",
       "   -0.13431087136268616,\n",
       "   0.13328929245471954,\n",
       "   -0.07013245671987534,\n",
       "   -0.31547319889068604,\n",
       "   0.26853272318840027,\n",
       "   -0.9699922800064087,\n",
       "   0.11136534810066223,\n",
       "   -0.27547743916511536,\n",
       "   -0.371446818113327,\n",
       "   0.4972289204597473,\n",
       "   0.16314581036567688,\n",
       "   0.24034243822097778,\n",
       "   -0.18157821893692017,\n",
       "   -0.2793081998825073,\n",
       "   -0.18836413323879242,\n",
       "   -0.057484064251184464,\n",
       "   0.29946112632751465,\n",
       "   -0.6719813942909241,\n",
       "   -0.044774238020181656,\n",
       "   0.07126803696155548,\n",
       "   0.5851696133613586,\n",
       "   -0.24970464408397675,\n",
       "   -0.1243690550327301,\n",
       "   -0.06188764050602913,\n",
       "   0.44257184863090515,\n",
       "   0.606418251991272,\n",
       "   0.39309462904930115,\n",
       "   0.22473157942295074,\n",
       "   -0.1375158131122589,\n",
       "   -0.3045889735221863,\n",
       "   0.007434725761413574,\n",
       "   0.019765272736549377,\n",
       "   0.043835945427417755,\n",
       "   -0.363198459148407,\n",
       "   0.20249789953231812,\n",
       "   0.10180553793907166,\n",
       "   0.06803581863641739,\n",
       "   0.3316957652568817,\n",
       "   0.005490195006132126,\n",
       "   -0.02987661212682724,\n",
       "   0.2276054322719574,\n",
       "   -0.05782091245055199,\n",
       "   -0.07238643616437912,\n",
       "   0.039593327790498734,\n",
       "   -0.11480797827243805,\n",
       "   0.13026127219200134,\n",
       "   0.11777284741401672,\n",
       "   0.11285477131605148,\n",
       "   -0.07290926575660706,\n",
       "   -0.2236161231994629,\n",
       "   -0.4147084951400757,\n",
       "   0.17711979150772095,\n",
       "   -0.002991744317114353,\n",
       "   0.4152747392654419,\n",
       "   -0.2980603575706482,\n",
       "   0.6583303213119507,\n",
       "   -0.2813658118247986,\n",
       "   -0.06928707659244537,\n",
       "   -0.1353728026151657,\n",
       "   -0.6296660900115967,\n",
       "   0.20903317630290985,\n",
       "   -0.19579960405826569,\n",
       "   -0.2244705855846405,\n",
       "   0.2507988214492798,\n",
       "   0.04155085235834122,\n",
       "   0.20382007956504822,\n",
       "   0.020759202539920807,\n",
       "   -0.3248457610607147,\n",
       "   0.12609733641147614,\n",
       "   0.052763164043426514,\n",
       "   -0.05727864056825638,\n",
       "   -0.02086421474814415,\n",
       "   0.20032329857349396,\n",
       "   -0.31260067224502563,\n",
       "   0.21712684631347656,\n",
       "   -0.16525889933109283,\n",
       "   -0.2902182638645172,\n",
       "   0.3714309632778168,\n",
       "   -0.48044121265411377,\n",
       "   -0.3374648690223694,\n",
       "   -0.1835435926914215,\n",
       "   -0.2497251182794571,\n",
       "   -0.28815892338752747,\n",
       "   0.3424365818500519,\n",
       "   -0.05960443615913391,\n",
       "   -0.08653926849365234,\n",
       "   -0.021655496209859848,\n",
       "   0.028078434988856316,\n",
       "   -0.07577267289161682,\n",
       "   0.3977006673812866,\n",
       "   -0.4505549371242523,\n",
       "   -0.13066673278808594,\n",
       "   -0.22321350872516632,\n",
       "   0.15359969437122345,\n",
       "   0.24810069799423218,\n",
       "   0.24921554327011108,\n",
       "   -0.1708306223154068,\n",
       "   -0.19797566533088684,\n",
       "   -0.11478959023952484,\n",
       "   0.17906701564788818,\n",
       "   -0.29987919330596924,\n",
       "   -0.012273967266082764,\n",
       "   0.21376821398735046,\n",
       "   0.12966442108154297,\n",
       "   0.12815646827220917,\n",
       "   -0.16880574822425842,\n",
       "   -0.8358518481254578,\n",
       "   0.03342053294181824,\n",
       "   0.12065446376800537,\n",
       "   0.07747834920883179,\n",
       "   0.07218118011951447,\n",
       "   -0.06992574781179428,\n",
       "   0.0673842802643776,\n",
       "   -0.2633572220802307,\n",
       "   0.37408724427223206,\n",
       "   0.3364999294281006,\n",
       "   -0.0828646570444107,\n",
       "   0.18065781891345978,\n",
       "   -0.16770994663238525,\n",
       "   0.10610989481210709,\n",
       "   -0.279092401266098,\n",
       "   0.5945283770561218,\n",
       "   -0.5345038175582886,\n",
       "   0.002979593351483345,\n",
       "   0.22131076455116272,\n",
       "   -0.005304984748363495,\n",
       "   -0.4740186333656311,\n",
       "   0.10756251215934753,\n",
       "   0.13850954174995422,\n",
       "   0.16551095247268677,\n",
       "   0.10023978352546692,\n",
       "   -0.2255021333694458,\n",
       "   -0.20865751802921295,\n",
       "   0.09439346194267273,\n",
       "   0.3515361249446869,\n",
       "   -0.36058294773101807,\n",
       "   0.6094030737876892,\n",
       "   0.20584896206855774,\n",
       "   -0.3989740312099457,\n",
       "   -0.266248494386673,\n",
       "   0.41697269678115845,\n",
       "   -0.12624713778495789,\n",
       "   0.17250409722328186,\n",
       "   -0.0454845204949379,\n",
       "   -0.35755977034568787,\n",
       "   -0.2762341797351837,\n",
       "   -0.04514993727207184,\n",
       "   0.6578980088233948,\n",
       "   0.2072744220495224,\n",
       "   -0.021165236830711365,\n",
       "   0.15300895273685455,\n",
       "   -0.49907389283180237,\n",
       "   -0.22494131326675415,\n",
       "   -0.14443041384220123,\n",
       "   0.3264308273792267,\n",
       "   -0.40263473987579346,\n",
       "   -0.3188907504081726,\n",
       "   -0.18746046721935272,\n",
       "   0.23551779985427856,\n",
       "   -0.2375941276550293,\n",
       "   -0.08646389096975327,\n",
       "   -0.14811848104000092,\n",
       "   -0.04754456877708435,\n",
       "   -0.3354331851005554,\n",
       "   0.3778954744338989,\n",
       "   -0.3946055769920349,\n",
       "   -0.12479196488857269,\n",
       "   -0.10522531718015671,\n",
       "   0.186055988073349,\n",
       "   0.28010740876197815,\n",
       "   0.30427923798561096,\n",
       "   -0.19937443733215332,\n",
       "   -0.4731130003929138,\n",
       "   0.2818133533000946,\n",
       "   0.4218592345714569,\n",
       "   0.5846206545829773,\n",
       "   0.37713485956192017,\n",
       "   0.5932139754295349,\n",
       "   0.19461534917354584,\n",
       "   -0.3596399128437042,\n",
       "   -0.3920895457267761,\n",
       "   0.1504473090171814,\n",
       "   0.06536246836185455,\n",
       "   -0.0789676308631897,\n",
       "   -0.4464738965034485,\n",
       "   -0.33356279134750366,\n",
       "   0.370476096868515,\n",
       "   0.444586843252182,\n",
       "   0.23213043808937073,\n",
       "   -0.16872291266918182,\n",
       "   0.21552985906600952,\n",
       "   0.12799331545829773,\n",
       "   -0.09816664457321167,\n",
       "   0.06661930680274963,\n",
       "   0.10992494225502014,\n",
       "   -0.185551717877388,\n",
       "   -0.040740594267845154,\n",
       "   0.04666121304035187,\n",
       "   -0.2762298285961151,\n",
       "   0.0663219690322876,\n",
       "   -0.46238070726394653,\n",
       "   -0.1945382058620453,\n",
       "   0.1255946159362793,\n",
       "   -0.37924307584762573,\n",
       "   0.5554713606834412,\n",
       "   -0.36829638481140137,\n",
       "   0.35640963912010193,\n",
       "   0.12620586156845093,\n",
       "   0.13492977619171143,\n",
       "   0.2428821474313736,\n",
       "   -0.37255969643592834,\n",
       "   0.28832581639289856,\n",
       "   0.04158544912934303,\n",
       "   -0.12451424449682236,\n",
       "   0.06824344396591187,\n",
       "   0.0007362030446529388,\n",
       "   -0.03811429440975189,\n",
       "   -0.2318277359008789,\n",
       "   -0.11581222712993622,\n",
       "   -0.20385080575942993,\n",
       "   -0.13817696273326874,\n",
       "   0.24272207915782928,\n",
       "   0.3383961617946625,\n",
       "   0.26184654235839844,\n",
       "   0.05679122358560562,\n",
       "   0.2653263807296753,\n",
       "   0.42104917764663696,\n",
       "   0.2510709762573242,\n",
       "   0.18245704472064972,\n",
       "   -0.24122245609760284,\n",
       "   -0.18292658030986786,\n",
       "   0.11523285508155823,\n",
       "   -0.1146690621972084,\n",
       "   0.09393617510795593,\n",
       "   0.35631608963012695,\n",
       "   -0.17196153104305267,\n",
       "   0.05633055791258812,\n",
       "   0.7249979376792908,\n",
       "   -0.38900843262672424,\n",
       "   0.5116393566131592,\n",
       "   0.006958179175853729,\n",
       "   0.07264634221792221,\n",
       "   0.11919630318880081,\n",
       "   -0.19112716615200043,\n",
       "   0.015644000843167305,\n",
       "   0.17429907619953156,\n",
       "   -0.6224802136421204,\n",
       "   0.21334391832351685,\n",
       "   0.1548260748386383,\n",
       "   0.7324672341346741,\n",
       "   -0.3307556211948395,\n",
       "   0.09260222315788269,\n",
       "   -0.06254194676876068,\n",
       "   -0.25699377059936523,\n",
       "   -0.09689109027385712,\n",
       "   0.06716389209032059,\n",
       "   0.05510414019227028,\n",
       "   0.538845956325531,\n",
       "   -0.15824812650680542,\n",
       "   0.22277669608592987,\n",
       "   -0.32308101654052734,\n",
       "   -0.16120533645153046,\n",
       "   -0.26929861307144165,\n",
       "   -0.43106698989868164,\n",
       "   0.09347168356180191,\n",
       "   0.015274100005626678,\n",
       "   -0.3619495928287506,\n",
       "   -0.17242754995822906,\n",
       "   -0.09507357329130173,\n",
       "   -0.1693221628665924,\n",
       "   -0.13111333549022675,\n",
       "   -0.3933829367160797,\n",
       "   -0.5529776215553284,\n",
       "   -0.025292638689279556,\n",
       "   0.40649503469467163,\n",
       "   0.09032616019248962,\n",
       "   0.1717405617237091,\n",
       "   0.13950762152671814,\n",
       "   -0.3407815396785736,\n",
       "   -0.08669842034578323,\n",
       "   0.1390005350112915,\n",
       "   0.4264952540397644,\n",
       "   -0.6357011198997498,\n",
       "   0.25658807158470154,\n",
       "   -0.23800040781497955,\n",
       "   0.16183233261108398,\n",
       "   0.2438851147890091,\n",
       "   -0.03987184911966324,\n",
       "   0.5732672214508057,\n",
       "   -0.1786930114030838,\n",
       "   -0.3088391423225403,\n",
       "   0.16188105940818787,\n",
       "   0.26147061586380005,\n",
       "   0.08322955667972565,\n",
       "   -0.36345118284225464,\n",
       "   -0.4475911557674408,\n",
       "   -0.003007112070918083,\n",
       "   -0.22247013449668884,\n",
       "   0.043032608926296234,\n",
       "   0.18311002850532532,\n",
       "   0.5187603831291199,\n",
       "   -0.6175909042358398,\n",
       "   0.3594651520252228,\n",
       "   0.24510154128074646,\n",
       "   0.2309103012084961,\n",
       "   0.08362815529108047,\n",
       "   0.0690816193819046,\n",
       "   -0.13061070442199707,\n",
       "   -0.24879011511802673,\n",
       "   0.2996729016304016,\n",
       "   -0.4183172285556793,\n",
       "   -0.19879521429538727,\n",
       "   -0.07655681669712067,\n",
       "   0.31150388717651367,\n",
       "   0.0532987117767334,\n",
       "   -0.14320310950279236,\n",
       "   -0.45446041226387024,\n",
       "   0.44997334480285645,\n",
       "   -0.12363798171281815,\n",
       "   -0.1160009577870369,\n",
       "   -0.49027764797210693,\n",
       "   0.4913444221019745,\n",
       "   -0.4879086911678314,\n",
       "   -0.17198660969734192,\n",
       "   -0.020738374441862106,\n",
       "   -0.09735149890184402,\n",
       "   -0.06010109931230545,\n",
       "   -0.20680300891399384,\n",
       "   -0.16154973208904266,\n",
       "   0.3298478126525879,\n",
       "   0.37927788496017456,\n",
       "   -0.02678542584180832,\n",
       "   -0.028745897114276886,\n",
       "   0.5901309251785278,\n",
       "   0.11200925707817078,\n",
       "   -0.20354236662387848,\n",
       "   0.018518716096878052,\n",
       "   0.4563841223716736,\n",
       "   0.3196357786655426,\n",
       "   0.2732473909854889,\n",
       "   0.7404157519340515,\n",
       "   0.2273896485567093,\n",
       "   -0.22645583748817444,\n",
       "   0.1136474758386612,\n",
       "   0.1966157853603363,\n",
       "   -0.5145407915115356,\n",
       "   -0.025699174031615257,\n",
       "   -0.2851204574108124,\n",
       "   -0.33010122179985046,\n",
       "   -0.3218332827091217,\n",
       "   -0.3202170729637146,\n",
       "   0.35393810272216797,\n",
       "   0.06106815114617348,\n",
       "   0.8587260246276855,\n",
       "   -0.0726579800248146,\n",
       "   -0.2960418164730072,\n",
       "   0.2978821396827698,\n",
       "   0.07465600222349167,\n",
       "   0.13059543073177338,\n",
       "   0.09960132092237473,\n",
       "   0.25301653146743774,\n",
       "   -0.3091277480125427,\n",
       "   -0.07268331944942474,\n",
       "   -0.013109471648931503,\n",
       "   0.026924334466457367,\n",
       "   -0.43132534623146057,\n",
       "   -0.09348899126052856,\n",
       "   -0.08733423054218292,\n",
       "   -0.13201609253883362,\n",
       "   -0.1711329221725464,\n",
       "   0.22280272841453552,\n",
       "   -0.13218116760253906,\n",
       "   0.6625162363052368,\n",
       "   0.41879966855049133,\n",
       "   0.0576385036110878,\n",
       "   -0.09143858402967453,\n",
       "   -0.048500701785087585,\n",
       "   -0.048836514353752136,\n",
       "   -0.3522734045982361,\n",
       "   0.011073287576436996,\n",
       "   0.37982994318008423,\n",
       "   -0.2128392457962036,\n",
       "   -0.016248412430286407,\n",
       "   0.322691947221756,\n",
       "   0.13656793534755707,\n",
       "   0.029070351272821426,\n",
       "   0.1894683688879013,\n",
       "   0.17845788598060608,\n",
       "   0.17018693685531616,\n",
       "   -0.6106258630752563,\n",
       "   -0.529874324798584,\n",
       "   -0.0423976294696331,\n",
       "   -0.14069995284080505,\n",
       "   0.8901746869087219,\n",
       "   0.10680030286312103,\n",
       "   -0.20574617385864258,\n",
       "   0.4974724352359772,\n",
       "   -0.0882292240858078,\n",
       "   0.046275850385427475,\n",
       "   0.35831981897354126,\n",
       "   -0.16294541954994202,\n",
       "   0.6583920121192932,\n",
       "   -0.1615625023841858,\n",
       "   -0.011414125561714172,\n",
       "   -0.166847825050354,\n",
       "   -0.3905298411846161,\n",
       "   0.15429796278476715,\n",
       "   0.004722610116004944,\n",
       "   0.14956775307655334,\n",
       "   0.138554185628891,\n",
       "   0.17286646366119385,\n",
       "   -0.2976164221763611,\n",
       "   0.3166877329349518,\n",
       "   0.02075900509953499,\n",
       "   0.3975759744644165,\n",
       "   0.25752490758895874,\n",
       "   0.43607866764068604,\n",
       "   0.1868618279695511,\n",
       "   -0.18917429447174072,\n",
       "   -0.997937798500061,\n",
       "   0.2724827527999878,\n",
       "   -0.23106588423252106,\n",
       "   -0.1364389955997467,\n",
       "   -0.11473613977432251,\n",
       "   -9.012744903564453,\n",
       "   0.5318706631660461,\n",
       "   -0.0748978853225708,\n",
       "   0.12643779814243317,\n",
       "   -0.31927138566970825,\n",
       "   0.1577758938074112,\n",
       "   -0.36587756872177124,\n",
       "   -0.2825008034706116,\n",
       "   -0.43590936064720154,\n",
       "   0.2379770278930664,\n",
       "   -0.09316716343164444,\n",
       "   -0.0151117704808712,\n",
       "   0.18104490637779236,\n",
       "   -0.24381522834300995,\n",
       "   0.19390200078487396,\n",
       "   -0.21703220903873444,\n",
       "   0.15240097045898438,\n",
       "   -0.22333279252052307,\n",
       "   0.30029019713401794,\n",
       "   -0.2028493583202362,\n",
       "   0.3240405023097992,\n",
       "   0.4477398693561554,\n",
       "   0.17055249214172363,\n",
       "   0.37975606322288513,\n",
       "   -0.1764664649963379,\n",
       "   0.37136492133140564,\n",
       "   -0.17747747898101807,\n",
       "   0.04994634538888931,\n",
       "   0.10881511121988297,\n",
       "   -0.3875493109226227,\n",
       "   -0.38509446382522583,\n",
       "   0.23167452216148376,\n",
       "   -0.09221157431602478,\n",
       "   0.09799398481845856,\n",
       "   0.22706238925457,\n",
       "   -0.13233304023742676,\n",
       "   0.057732198387384415,\n",
       "   -0.47539088129997253,\n",
       "   0.06873941421508789,\n",
       "   -0.5873451232910156,\n",
       "   -0.48588696122169495,\n",
       "   -0.3095931112766266,\n",
       "   -0.12415549159049988,\n",
       "   0.24487276375293732,\n",
       "   0.45385438203811646,\n",
       "   0.1769465208053589,\n",
       "   -0.04016951099038124,\n",
       "   0.453677773475647,\n",
       "   0.04983227699995041,\n",
       "   0.26083824038505554,\n",
       "   0.3254071772098541,\n",
       "   0.24420563876628876,\n",
       "   0.31810057163238525,\n",
       "   -0.1716526299715042,\n",
       "   0.07670070230960846,\n",
       "   0.06019172817468643,\n",
       "   0.07876381278038025,\n",
       "   0.3510848879814148,\n",
       "   -0.0628800094127655,\n",
       "   -0.17710937559604645,\n",
       "   0.08651476353406906,\n",
       "   0.2034110575914383,\n",
       "   0.8572867512702942,\n",
       "   0.10273761302232742,\n",
       "   -0.27229297161102295,\n",
       "   0.5857034921646118,\n",
       "   0.05783231556415558,\n",
       "   0.23818224668502808,\n",
       "   0.12721656262874603,\n",
       "   -0.3882475197315216,\n",
       "   -0.3170219659805298,\n",
       "   -0.25692012906074524,\n",
       "   0.1275334656238556,\n",
       "   -0.4526258409023285,\n",
       "   0.34988486766815186,\n",
       "   -0.6254687309265137,\n",
       "   0.08023449033498764,\n",
       "   0.0881357342004776,\n",
       "   -0.17659881711006165,\n",
       "   0.08983911573886871,\n",
       "   -0.4586467146873474,\n",
       "   0.6088524460792542,\n",
       "   0.5694794654846191,\n",
       "   -0.03240617364645004,\n",
       "   0.21027293801307678,\n",
       "   -0.1990334838628769,\n",
       "   -0.21990302205085754,\n",
       "   0.33829542994499207,\n",
       "   0.321599543094635,\n",
       "   -0.21131375432014465,\n",
       "   -0.3494700491428375,\n",
       "   0.37737786769866943,\n",
       "   -0.05590306594967842,\n",
       "   0.2643143832683563,\n",
       "   0.12503057718276978,\n",
       "   0.5956054925918579,\n",
       "   -0.15501494705677032,\n",
       "   0.11176441609859467,\n",
       "   0.0015483051538467407,\n",
       "   0.008603272959589958,\n",
       "   -0.09948660433292389,\n",
       "   0.002257179468870163,\n",
       "   -0.38198524713516235,\n",
       "   -0.2612496018409729,\n",
       "   -0.2898063361644745,\n",
       "   -0.06655298918485641,\n",
       "   -0.0792739987373352,\n",
       "   -0.2403056025505066,\n",
       "   0.24676452577114105,\n",
       "   -0.09016142785549164,\n",
       "   -0.26377448439598083,\n",
       "   0.2654886543750763,\n",
       "   0.1712493896484375,\n",
       "   -0.4030730128288269,\n",
       "   0.014071501791477203,\n",
       "   0.177170991897583,\n",
       "   -0.11043942719697952,\n",
       "   -0.11119504272937775,\n",
       "   0.25318044424057007,\n",
       "   0.022094756364822388,\n",
       "   -0.17217770218849182,\n",
       "   -0.23512417078018188,\n",
       "   -0.012981131672859192,\n",
       "   -0.2636480927467346,\n",
       "   0.17397886514663696,\n",
       "   0.22605325281620026,\n",
       "   0.163349449634552,\n",
       "   -0.17419752478599548,\n",
       "   -0.02237537130713463,\n",
       "   0.014006845653057098,\n",
       "   0.21922627091407776,\n",
       "   0.5423415303230286,\n",
       "   -0.43577539920806885,\n",
       "   0.44261229038238525,\n",
       "   -0.12476135790348053,\n",
       "   -0.1077108085155487,\n",
       "   0.052580781280994415,\n",
       "   -0.1353222280740738,\n",
       "   -0.32154330611228943,\n",
       "   0.39030057191848755,\n",
       "   -0.9367334246635437,\n",
       "   0.025651700794696808,\n",
       "   0.399007111787796,\n",
       "   0.29872992634773254,\n",
       "   0.41469088196754456,\n",
       "   0.13804130256175995,\n",
       "   -0.27229613065719604,\n",
       "   0.20985615253448486,\n",
       "   0.12815557420253754,\n",
       "   0.12096159160137177,\n",
       "   -0.4018988609313965,\n",
       "   0.3790361285209656,\n",
       "   0.1752096712589264,\n",
       "   0.16110047698020935,\n",
       "   -0.13688011467456818,\n",
       "   -0.4379035532474518,\n",
       "   -0.00880267471075058,\n",
       "   0.1684071123600006,\n",
       "   -0.2878926396369934,\n",
       "   0.07405617833137512,\n",
       "   0.0023682396858930588,\n",
       "   0.11198627948760986,\n",
       "   0.17422398924827576,\n",
       "   -0.20434680581092834,\n",
       "   -0.273308664560318,\n",
       "   -0.25948506593704224,\n",
       "   -0.12906935811042786,\n",
       "   -0.1874723732471466,\n",
       "   0.17341361939907074,\n",
       "   -0.05481588840484619,\n",
       "   -0.23183177411556244,\n",
       "   -0.0014097550883889198,\n",
       "   -0.18889835476875305,\n",
       "   0.3089778423309326,\n",
       "   0.07258927077054977,\n",
       "   -0.1413806825876236,\n",
       "   -0.2812406122684479,\n",
       "   -0.01857024058699608,\n",
       "   1.0896400213241577,\n",
       "   -0.10443135350942612,\n",
       "   0.21196837723255157,\n",
       "   0.23415715992450714,\n",
       "   0.12423428893089294,\n",
       "   -0.4747679829597473,\n",
       "   -0.1567743420600891,\n",
       "   0.2599402666091919,\n",
       "   0.6076132655143738,\n",
       "   0.3590201437473297,\n",
       "   -0.3854682147502899,\n",
       "   0.1943424791097641,\n",
       "   0.5166791677474976,\n",
       "   0.027860134840011597,\n",
       "   -0.26105666160583496,\n",
       "   0.43198779225349426,\n",
       "   -0.21414591372013092,\n",
       "   0.4434344470500946,\n",
       "   0.08585688471794128,\n",
       "   0.3925279378890991,\n",
       "   -0.21910831332206726,\n",
       "   -0.14944559335708618,\n",
       "   -0.06552648544311523,\n",
       "   0.27253299951553345,\n",
       "   0.2136155217885971,\n",
       "   -0.16300378739833832,\n",
       "   -0.03665303438901901,\n",
       "   0.16648593544960022,\n",
       "   -0.1487255096435547,\n",
       "   0.21673940122127533,\n",
       "   -0.03554807975888252,\n",
       "   0.3411962389945984,\n",
       "   -0.03323620557785034],\n",
       "  [0.20413926243782043,\n",
       "   -0.3568565547466278,\n",
       "   0.12449613213539124,\n",
       "   0.054010987281799316,\n",
       "   0.06376802921295166,\n",
       "   -0.24773818254470825,\n",
       "   0.5897761583328247,\n",
       "   -0.419308602809906,\n",
       "   -0.34902137517929077,\n",
       "   0.5029518008232117,\n",
       "   -0.16014355421066284,\n",
       "   -0.06938183307647705,\n",
       "   -0.24661898612976074,\n",
       "   0.3892163336277008,\n",
       "   -0.5427660346031189,\n",
       "   0.12053626030683517,\n",
       "   -0.06470128148794174,\n",
       "   0.2772040367126465,\n",
       "   0.16415272653102875,\n",
       "   0.15801775455474854,\n",
       "   -0.5916623473167419,\n",
       "   -0.08048640191555023,\n",
       "   -0.13380831480026245,\n",
       "   0.08695016801357269,\n",
       "   0.07232575118541718,\n",
       "   0.0937020555138588,\n",
       "   -0.04333222657442093,\n",
       "   0.19702079892158508,\n",
       "   0.32434868812561035,\n",
       "   0.8598074316978455,\n",
       "   0.1778278797864914,\n",
       "   -0.013364952988922596,\n",
       "   -0.13868407905101776,\n",
       "   0.13171322643756866,\n",
       "   -0.12296135723590851,\n",
       "   0.34700310230255127,\n",
       "   -0.1082531288266182,\n",
       "   0.24402478337287903,\n",
       "   -0.25672677159309387,\n",
       "   0.31961145997047424,\n",
       "   -0.08875967562198639,\n",
       "   -0.1807582676410675,\n",
       "   0.2191535234451294,\n",
       "   -0.48362910747528076,\n",
       "   0.10042275488376617,\n",
       "   -0.2928465008735657,\n",
       "   0.41351383924484253,\n",
       "   -0.006401801481842995,\n",
       "   -0.1270589977502823,\n",
       "   0.3766429126262665,\n",
       "   -0.19644199311733246,\n",
       "   0.3314371109008789,\n",
       "   -0.2768297493457794,\n",
       "   -0.22581444680690765,\n",
       "   0.15392805635929108,\n",
       "   -0.14204008877277374,\n",
       "   0.13676102459430695,\n",
       "   -0.13211342692375183,\n",
       "   -0.22413620352745056,\n",
       "   0.3188760280609131,\n",
       "   -0.5182229280471802,\n",
       "   -0.05533135309815407,\n",
       "   0.22974206507205963,\n",
       "   0.09438206255435944,\n",
       "   -0.02983640879392624,\n",
       "   0.08631889522075653,\n",
       "   -0.12409526109695435,\n",
       "   -0.03973867744207382,\n",
       "   -0.08609996736049652,\n",
       "   0.03943730145692825,\n",
       "   -0.04575232416391373,\n",
       "   0.026875920593738556,\n",
       "   -0.4478413462638855,\n",
       "   -0.36511629819869995,\n",
       "   -0.3423396646976471,\n",
       "   -0.28576308488845825,\n",
       "   -0.4527857303619385,\n",
       "   0.16042789816856384,\n",
       "   -0.03481631726026535,\n",
       "   -0.15802103281021118,\n",
       "   -0.003913607448339462,\n",
       "   0.06728514283895493,\n",
       "   0.20450027287006378,\n",
       "   0.14259488880634308,\n",
       "   -0.024848349392414093,\n",
       "   0.1560516059398651,\n",
       "   0.02093067206442356,\n",
       "   0.30117979645729065,\n",
       "   0.09652350097894669,\n",
       "   0.4251945912837982,\n",
       "   0.08147674053907394,\n",
       "   -0.1423412710428238,\n",
       "   0.17658142745494843,\n",
       "   0.1800917088985443,\n",
       "   -0.3845258355140686,\n",
       "   -0.025154151022434235,\n",
       "   -0.11571838706731796,\n",
       "   0.03066398575901985,\n",
       "   -0.892704427242279,\n",
       "   0.1313207447528839,\n",
       "   -0.21450091898441315,\n",
       "   0.41365787386894226,\n",
       "   -0.10435228049755096,\n",
       "   0.4074256420135498,\n",
       "   -0.21160157024860382,\n",
       "   0.3413998484611511,\n",
       "   0.2136160284280777,\n",
       "   -0.20169538259506226,\n",
       "   -0.19992509484291077,\n",
       "   0.5722947120666504,\n",
       "   -0.008428946137428284,\n",
       "   0.31510666012763977,\n",
       "   0.3865639567375183,\n",
       "   -0.07625605911016464,\n",
       "   -0.24153485894203186,\n",
       "   -0.01160871610045433,\n",
       "   -0.2475467473268509,\n",
       "   -0.16888150572776794,\n",
       "   -0.13505113124847412,\n",
       "   0.5064759850502014,\n",
       "   0.27662205696105957,\n",
       "   0.07928942143917084,\n",
       "   0.2261594980955124,\n",
       "   -0.42538732290267944,\n",
       "   0.22959187626838684,\n",
       "   0.34865278005599976,\n",
       "   -0.1847512423992157,\n",
       "   -0.025077015161514282,\n",
       "   -0.23050813376903534,\n",
       "   0.16532616317272186,\n",
       "   -0.20433497428894043,\n",
       "   0.25575950741767883,\n",
       "   0.5871420502662659,\n",
       "   0.10397335886955261,\n",
       "   0.03240283578634262,\n",
       "   0.24689772725105286,\n",
       "   -0.41678035259246826,\n",
       "   0.7314234375953674,\n",
       "   -0.2583544850349426,\n",
       "   -0.3693717122077942,\n",
       "   -0.30445513129234314,\n",
       "   0.16840779781341553,\n",
       "   0.32467636466026306,\n",
       "   0.1035400927066803,\n",
       "   0.11477284133434296,\n",
       "   -0.2844044268131256,\n",
       "   -0.3931370973587036,\n",
       "   0.011565417051315308,\n",
       "   -0.04105694219470024,\n",
       "   0.16180816292762756,\n",
       "   -0.08739478141069412,\n",
       "   0.18896259367465973,\n",
       "   -0.349526047706604,\n",
       "   0.05521789565682411,\n",
       "   -0.17611686885356903,\n",
       "   -0.326985239982605,\n",
       "   -0.16051216423511505,\n",
       "   0.37652355432510376,\n",
       "   0.5480064153671265,\n",
       "   0.28314831852912903,\n",
       "   0.17984339594841003,\n",
       "   -0.16101020574569702,\n",
       "   -0.5059981346130371,\n",
       "   -0.2387552410364151,\n",
       "   0.25182226300239563,\n",
       "   -0.03414918854832649,\n",
       "   -0.25865620374679565,\n",
       "   0.4247259497642517,\n",
       "   -0.07105749845504761,\n",
       "   0.49106019735336304,\n",
       "   0.057455722242593765,\n",
       "   0.058250248432159424,\n",
       "   0.02032753825187683,\n",
       "   0.12551048398017883,\n",
       "   -0.29210352897644043,\n",
       "   0.35960593819618225,\n",
       "   0.13779926300048828,\n",
       "   -0.19401277601718903,\n",
       "   0.045493580400943756,\n",
       "   0.1334555298089981,\n",
       "   -0.00953601486980915,\n",
       "   0.21172499656677246,\n",
       "   -0.34051239490509033,\n",
       "   -0.6508038640022278,\n",
       "   -0.24383220076560974,\n",
       "   0.11268001049757004,\n",
       "   0.10437879711389542,\n",
       "   -0.326040118932724,\n",
       "   0.048887453973293304,\n",
       "   -0.4278193712234497,\n",
       "   -0.08329938352108002,\n",
       "   -0.023324929177761078,\n",
       "   -0.03078525699675083,\n",
       "   0.042940493673086166,\n",
       "   -0.4753342270851135,\n",
       "   0.2134477198123932,\n",
       "   -0.12406425178050995,\n",
       "   0.31512221693992615,\n",
       "   0.17109325528144836,\n",
       "   -0.040649645030498505,\n",
       "   0.1687086522579193,\n",
       "   -0.19998513162136078,\n",
       "   0.20386116206645966,\n",
       "   -0.007402489893138409,\n",
       "   0.06613150984048843,\n",
       "   0.6220703721046448,\n",
       "   -0.22548900544643402,\n",
       "   0.010340392589569092,\n",
       "   -0.027643604204058647,\n",
       "   -0.31194818019866943,\n",
       "   -0.04112840071320534,\n",
       "   -0.4198856055736542,\n",
       "   -0.3629028797149658,\n",
       "   -0.026895008981227875,\n",
       "   0.04736173152923584,\n",
       "   -0.33966144919395447,\n",
       "   -0.034982457756996155,\n",
       "   0.054009076207876205,\n",
       "   -0.46790245175361633,\n",
       "   0.03892003372311592,\n",
       "   0.16825588047504425,\n",
       "   0.06522747129201889,\n",
       "   0.033154308795928955,\n",
       "   -0.023891376331448555,\n",
       "   -0.8163594007492065,\n",
       "   0.10240848362445831,\n",
       "   0.059489741921424866,\n",
       "   0.14238376915454865,\n",
       "   0.23671048879623413,\n",
       "   0.10433439910411835,\n",
       "   -0.34860527515411377,\n",
       "   0.10581013560295105,\n",
       "   0.03212343901395798,\n",
       "   -0.03559419885277748,\n",
       "   0.02276766113936901,\n",
       "   -0.27733519673347473,\n",
       "   0.030338279902935028,\n",
       "   -0.08208562433719635,\n",
       "   -0.4185674488544464,\n",
       "   -0.34359127283096313,\n",
       "   0.46280398964881897,\n",
       "   0.4831521511077881,\n",
       "   0.1133771687746048,\n",
       "   0.028936509042978287,\n",
       "   0.1454242318868637,\n",
       "   0.004311174154281616,\n",
       "   -0.3877111077308655,\n",
       "   0.29990097880363464,\n",
       "   0.35847967863082886,\n",
       "   -0.2841600179672241,\n",
       "   0.015583544969558716,\n",
       "   -0.25904881954193115,\n",
       "   -0.004991844296455383,\n",
       "   -0.28895288705825806,\n",
       "   -0.04581151157617569,\n",
       "   -0.06114640086889267,\n",
       "   0.15516085922718048,\n",
       "   0.14080417156219482,\n",
       "   0.27539587020874023,\n",
       "   -0.5157513618469238,\n",
       "   0.4342174232006073,\n",
       "   0.3770255744457245,\n",
       "   0.015593549236655235,\n",
       "   -0.1938226819038391,\n",
       "   -0.06328614801168442,\n",
       "   -0.15428480505943298,\n",
       "   -0.23986726999282837,\n",
       "   0.42759546637535095,\n",
       "   -0.410514771938324,\n",
       "   0.20167900621891022,\n",
       "   -0.05899010971188545,\n",
       "   0.08890873193740845,\n",
       "   -0.0780816599726677,\n",
       "   0.2528349459171295,\n",
       "   0.27163028717041016,\n",
       "   0.5206979513168335,\n",
       "   0.18808750808238983,\n",
       "   -0.45106491446495056,\n",
       "   0.06647666543722153,\n",
       "   -0.14869758486747742,\n",
       "   0.03736923635005951,\n",
       "   0.3471043109893799,\n",
       "   0.09061824530363083,\n",
       "   0.6289494037628174,\n",
       "   -0.5643742084503174,\n",
       "   -0.1921772062778473,\n",
       "   0.1111370325088501,\n",
       "   0.5242506265640259,\n",
       "   -0.10742606222629547,\n",
       "   -0.2080475389957428,\n",
       "   -0.14202065765857697,\n",
       "   0.05132124572992325,\n",
       "   -0.2066919207572937,\n",
       "   0.24216559529304504,\n",
       "   0.5033595561981201,\n",
       "   0.23848514258861542,\n",
       "   -0.0863099917769432,\n",
       "   0.3812202215194702,\n",
       "   -0.46800974011421204,\n",
       "   -0.08760680258274078,\n",
       "   0.11819588392972946,\n",
       "   0.3141050934791565,\n",
       "   -0.01590350642800331,\n",
       "   -0.0938861146569252,\n",
       "   -0.2288699597120285,\n",
       "   0.14701968431472778,\n",
       "   0.20584949851036072,\n",
       "   0.2829587757587433,\n",
       "   -0.24028386175632477,\n",
       "   -0.08887562900781631,\n",
       "   0.313750684261322,\n",
       "   0.026497390121221542,\n",
       "   -0.2225056290626526,\n",
       "   0.12060527503490448,\n",
       "   0.19975945353507996,\n",
       "   -0.18518699705600739,\n",
       "   -0.1601898968219757,\n",
       "   -0.415047824382782,\n",
       "   0.26232221722602844,\n",
       "   0.05513100326061249,\n",
       "   0.17069987952709198,\n",
       "   0.12081310153007507,\n",
       "   -0.4014418125152588,\n",
       "   0.5467467904090881,\n",
       "   0.18703463673591614,\n",
       "   0.35632508993148804,\n",
       "   -0.02366921305656433,\n",
       "   0.10427053272724152,\n",
       "   -0.17535848915576935,\n",
       "   0.08704540133476257,\n",
       "   0.1980893760919571,\n",
       "   0.22477813065052032,\n",
       "   0.16756241023540497,\n",
       "   -0.5818105936050415,\n",
       "   -0.4519622325897217,\n",
       "   -0.30383580923080444,\n",
       "   0.08357900381088257,\n",
       "   0.6449503302574158,\n",
       "   -0.8334817290306091,\n",
       "   -0.18829910457134247,\n",
       "   0.037401050329208374,\n",
       "   0.12920233607292175,\n",
       "   -0.0051945969462394714,\n",
       "   -0.42626500129699707,\n",
       "   0.12813638150691986,\n",
       "   0.21312636137008667,\n",
       "   0.11222327500581741,\n",
       "   0.07903319597244263,\n",
       "   0.18491661548614502,\n",
       "   0.24816209077835083,\n",
       "   -0.31952714920043945,\n",
       "   0.0850493311882019,\n",
       "   0.0966372862458229,\n",
       "   -0.11551810801029205,\n",
       "   0.648393988609314,\n",
       "   0.21044182777404785,\n",
       "   -0.2650131285190582,\n",
       "   0.05628201365470886,\n",
       "   0.374272882938385,\n",
       "   -0.09004131704568863,\n",
       "   -0.1311430037021637,\n",
       "   0.03401847928762436,\n",
       "   -0.0014097094535827637,\n",
       "   -0.6454713344573975,\n",
       "   -0.4906447231769562,\n",
       "   0.03945174440741539,\n",
       "   -0.2663962244987488,\n",
       "   0.16466113924980164,\n",
       "   -0.021851226687431335,\n",
       "   0.0472850538790226,\n",
       "   0.7948012351989746,\n",
       "   -0.17563864588737488,\n",
       "   -0.13631980121135712,\n",
       "   -0.4299537241458893,\n",
       "   -0.16805991530418396,\n",
       "   -0.09924182295799255,\n",
       "   -0.19902735948562622,\n",
       "   -0.23036935925483704,\n",
       "   0.22935214638710022,\n",
       "   -0.27252259850502014,\n",
       "   0.39323702454566956,\n",
       "   0.23371796309947968,\n",
       "   0.3429061770439148,\n",
       "   -0.3241473138332367,\n",
       "   0.1983012706041336,\n",
       "   -0.19419053196907043,\n",
       "   0.15269309282302856,\n",
       "   0.1407817155122757,\n",
       "   0.4601118862628937,\n",
       "   -0.00990414246916771,\n",
       "   0.5689185261726379,\n",
       "   0.013101819902658463,\n",
       "   0.3602551519870758,\n",
       "   0.3600276708602905,\n",
       "   0.009258795529603958,\n",
       "   0.027540728449821472,\n",
       "   -0.41947513818740845,\n",
       "   0.42504170536994934,\n",
       "   0.24219776690006256,\n",
       "   -0.3798333406448364,\n",
       "   -0.14216256141662598,\n",
       "   -0.17126935720443726,\n",
       "   0.3080235421657562,\n",
       "   -0.39227598905563354,\n",
       "   -0.45599496364593506,\n",
       "   -0.02117743343114853,\n",
       "   -0.33905577659606934,\n",
       "   0.2278280109167099,\n",
       "   0.17317527532577515,\n",
       "   -0.054038193076848984,\n",
       "   0.011920034885406494,\n",
       "   -0.27222806215286255,\n",
       "   -0.167164146900177,\n",
       "   -0.10849061608314514,\n",
       "   0.1750209629535675,\n",
       "   -0.2726094722747803,\n",
       "   -0.12625908851623535,\n",
       "   -0.28978613018989563,\n",
       "   -0.167713463306427,\n",
       "   0.33397987484931946,\n",
       "   0.24075138568878174,\n",
       "   -0.22731715440750122,\n",
       "   -0.3267615735530853,\n",
       "   0.06564611196517944,\n",
       "   -0.09753459692001343,\n",
       "   0.3558501899242401,\n",
       "   -0.005295421928167343,\n",
       "   -0.2050648033618927,\n",
       "   -0.33144906163215637,\n",
       "   0.4007318615913391,\n",
       "   0.4013426899909973,\n",
       "   -0.19581496715545654,\n",
       "   0.0505303293466568,\n",
       "   0.08564905822277069,\n",
       "   -0.1917765736579895,\n",
       "   0.0412963405251503,\n",
       "   -0.017527246847748756,\n",
       "   0.2220342755317688,\n",
       "   0.24723489582538605,\n",
       "   -0.14993692934513092,\n",
       "   0.1226348802447319,\n",
       "   -0.3551817834377289,\n",
       "   -0.1917121410369873,\n",
       "   -0.4368211627006531,\n",
       "   -0.26596271991729736,\n",
       "   0.2016138881444931,\n",
       "   0.3064132630825043,\n",
       "   -0.2506868541240692,\n",
       "   -0.23162640631198883,\n",
       "   0.07353179901838303,\n",
       "   0.3896805942058563,\n",
       "   -0.2491997480392456,\n",
       "   0.12911087274551392,\n",
       "   -0.15384942293167114,\n",
       "   0.18101732432842255,\n",
       "   0.42398253083229065,\n",
       "   -0.3057173490524292,\n",
       "   -0.31334418058395386,\n",
       "   -0.03743360564112663,\n",
       "   -0.20662203431129456,\n",
       "   -0.3660804331302643,\n",
       "   0.3636654019355774,\n",
       "   0.20085999369621277,\n",
       "   0.11704540252685547,\n",
       "   0.0052788835018873215,\n",
       "   -0.37774205207824707,\n",
       "   0.26421597599983215,\n",
       "   0.5350067615509033,\n",
       "   -0.05167818069458008,\n",
       "   0.07734845578670502,\n",
       "   0.4077049493789673,\n",
       "   0.17531824111938477,\n",
       "   0.5437820553779602,\n",
       "   0.4609123468399048,\n",
       "   0.11989139020442963,\n",
       "   -0.03599335253238678,\n",
       "   -0.21057935059070587,\n",
       "   0.37365221977233887,\n",
       "   -0.6677947640419006,\n",
       "   0.16323332488536835,\n",
       "   0.07735449820756912,\n",
       "   -0.1888723373413086,\n",
       "   -0.11793283373117447,\n",
       "   -0.27148428559303284,\n",
       "   0.19119523465633392,\n",
       "   0.12069323658943176,\n",
       "   0.2579183280467987,\n",
       "   -0.0018990074750036001,\n",
       "   0.09523718059062958,\n",
       "   0.10531581938266754,\n",
       "   -0.28823569416999817,\n",
       "   -0.12673963606357574,\n",
       "   0.2178034484386444,\n",
       "   0.3007885217666626,\n",
       "   -0.14732520282268524,\n",
       "   -0.09553965926170349,\n",
       "   0.08822391927242279,\n",
       "   0.23567670583724976,\n",
       "   -0.26523515582084656,\n",
       "   -0.04557747021317482,\n",
       "   -0.2973257005214691,\n",
       "   0.026096194982528687,\n",
       "   -0.007501967251300812,\n",
       "   -0.10835964977741241,\n",
       "   -0.2948361337184906,\n",
       "   0.6132367253303528,\n",
       "   0.07140573859214783,\n",
       "   -0.2508571445941925,\n",
       "   0.07564787566661835,\n",
       "   -0.1265706568956375,\n",
       "   -0.046422895044088364,\n",
       "   -0.39509230852127075,\n",
       "   0.0316447839140892,\n",
       "   -0.46707841753959656,\n",
       "   0.1060182973742485,\n",
       "   -0.21391132473945618,\n",
       "   -0.2483641803264618,\n",
       "   0.0953255295753479,\n",
       "   -0.048055246472358704,\n",
       "   0.5193485021591187,\n",
       "   0.5089401602745056,\n",
       "   -0.054517313838005066,\n",
       "   0.06415529549121857,\n",
       "   -0.1605854332447052,\n",
       "   0.6895296573638916,\n",
       "   -0.3064868450164795,\n",
       "   0.20508486032485962,\n",
       "   0.15800811350345612,\n",
       "   -0.07156464457511902,\n",
       "   -0.03594733029603958,\n",
       "   -0.06489010155200958,\n",
       "   0.1277032047510147,\n",
       "   0.14497160911560059,\n",
       "   -0.03592335432767868,\n",
       "   0.5531871318817139,\n",
       "   0.006970832124352455,\n",
       "   -0.18992461264133453,\n",
       "   -0.05781835690140724,\n",
       "   -0.7433053255081177,\n",
       "   -0.07452104985713959,\n",
       "   0.16472192108631134,\n",
       "   0.008282236754894257,\n",
       "   0.05042366310954094,\n",
       "   0.2214846909046173,\n",
       "   -0.012744246050715446,\n",
       "   -0.03967233747243881,\n",
       "   0.013196425512433052,\n",
       "   0.16274113953113556,\n",
       "   0.37912049889564514,\n",
       "   0.06910282373428345,\n",
       "   0.16026011109352112,\n",
       "   -0.007719211280345917,\n",
       "   -0.17578217387199402,\n",
       "   -0.014501102268695831,\n",
       "   -0.07659531384706497,\n",
       "   0.20302976667881012,\n",
       "   -0.5195913314819336,\n",
       "   -9.118025779724121,\n",
       "   0.2834792137145996,\n",
       "   -0.30704665184020996,\n",
       "   0.4747934639453888,\n",
       "   -0.35564374923706055,\n",
       "   0.13476483523845673,\n",
       "   0.3029317259788513,\n",
       "   -0.04679128900170326,\n",
       "   -0.16644954681396484,\n",
       "   -0.0947626382112503,\n",
       "   0.12792900204658508,\n",
       "   -0.07252579182386398,\n",
       "   -0.025974847376346588,\n",
       "   -0.006740991026163101,\n",
       "   0.17865563929080963,\n",
       "   -0.09961643815040588,\n",
       "   0.1913093626499176,\n",
       "   -0.1388712078332901,\n",
       "   -0.2716744840145111,\n",
       "   -0.009677514433860779,\n",
       "   -0.04578503593802452,\n",
       "   -0.33503851294517517,\n",
       "   -0.20709823071956635,\n",
       "   0.8308443427085876,\n",
       "   0.030986370518803596,\n",
       "   0.2872358560562134,\n",
       "   0.07659140974283218,\n",
       "   -0.2223941683769226,\n",
       "   -0.11557997763156891,\n",
       "   -0.07059013843536377,\n",
       "   -0.05235845223069191,\n",
       "   0.15311306715011597,\n",
       "   -0.14616061747074127,\n",
       "   0.23150674998760223,\n",
       "   0.06975050270557404,\n",
       "   -0.16529951989650726,\n",
       "   -0.09772966802120209,\n",
       "   0.1797570288181305,\n",
       "   -0.01980668306350708,\n",
       "   -0.3082613945007324,\n",
       "   0.2065785974264145,\n",
       "   0.031823039054870605,\n",
       "   -0.07340377569198608,\n",
       "   0.44814854860305786,\n",
       "   -0.06679166853427887,\n",
       "   0.0588715523481369,\n",
       "   0.09893379360437393,\n",
       "   -0.1048305332660675,\n",
       "   0.014865249395370483,\n",
       "   0.03466993197798729,\n",
       "   0.5009469985961914,\n",
       "   0.22170527279376984,\n",
       "   0.6296210885047913,\n",
       "   0.35476425290107727,\n",
       "   0.10546611249446869,\n",
       "   0.20796847343444824,\n",
       "   0.16451619565486908,\n",
       "   0.4388740360736847,\n",
       "   -0.27578967809677124,\n",
       "   -0.6501744985580444,\n",
       "   0.03113856166601181,\n",
       "   -0.010248184204101562,\n",
       "   0.3951869606971741,\n",
       "   0.05085264891386032,\n",
       "   -0.2547386884689331,\n",
       "   0.22206778824329376,\n",
       "   -0.36587077379226685,\n",
       "   -0.07988763600587845,\n",
       "   -0.007169615477323532,\n",
       "   -0.2556038796901703,\n",
       "   -0.5023273825645447,\n",
       "   -0.39411285519599915,\n",
       "   0.2662329375743866,\n",
       "   -0.24248774349689484,\n",
       "   0.0784069299697876,\n",
       "   -0.1149921789765358,\n",
       "   -0.04550924897193909,\n",
       "   0.27304527163505554,\n",
       "   0.11502464860677719,\n",
       "   0.33978772163391113,\n",
       "   -0.6479333639144897,\n",
       "   0.22305557131767273,\n",
       "   0.4331173002719879,\n",
       "   0.03172321245074272,\n",
       "   0.3302575945854187,\n",
       "   -0.20069445669651031,\n",
       "   -0.22776250541210175,\n",
       "   0.030992873013019562,\n",
       "   -0.05551512911915779,\n",
       "   0.33876633644104004,\n",
       "   -0.3440079092979431,\n",
       "   0.18899217247962952,\n",
       "   -0.0561874657869339,\n",
       "   0.22342592477798462,\n",
       "   -0.3041533827781677,\n",
       "   0.38474178314208984,\n",
       "   -0.0940118059515953,\n",
       "   -0.08709009736776352,\n",
       "   -0.0127676110714674,\n",
       "   0.13414359092712402,\n",
       "   0.2561579942703247,\n",
       "   -0.07115165889263153,\n",
       "   0.09551410377025604,\n",
       "   -0.5203419923782349,\n",
       "   -0.48477789759635925,\n",
       "   -0.030245011672377586,\n",
       "   0.022203808650374413,\n",
       "   -0.052354685962200165,\n",
       "   0.248883917927742,\n",
       "   0.3535904288291931,\n",
       "   -0.18211549520492554,\n",
       "   0.06022229045629501,\n",
       "   0.05252034589648247,\n",
       "   0.0021441858261823654,\n",
       "   0.18971765041351318,\n",
       "   0.28272274136543274,\n",
       "   -0.008366188034415245,\n",
       "   -0.4214588403701782,\n",
       "   -0.00651131197810173,\n",
       "   -0.23788109421730042,\n",
       "   0.09491783380508423,\n",
       "   -0.7180013060569763,\n",
       "   0.11016731709241867,\n",
       "   -0.3432796597480774,\n",
       "   -0.05652053281664848,\n",
       "   0.047038931399583817,\n",
       "   0.006499491631984711,\n",
       "   0.08746570348739624,\n",
       "   0.10795756429433823,\n",
       "   0.03601651266217232,\n",
       "   0.31577378511428833,\n",
       "   0.3419637084007263,\n",
       "   0.556755542755127,\n",
       "   -0.1516328603029251,\n",
       "   0.12990015745162964,\n",
       "   -0.07309011369943619,\n",
       "   0.1593722254037857,\n",
       "   -0.0859527513384819,\n",
       "   -0.19974127411842346,\n",
       "   0.16645197570323944,\n",
       "   -0.5543859601020813,\n",
       "   -0.040412236005067825,\n",
       "   0.1639697104692459,\n",
       "   0.19336774945259094,\n",
       "   0.35774102807044983,\n",
       "   -0.17118944227695465,\n",
       "   -0.3495044708251953,\n",
       "   0.3526315689086914,\n",
       "   0.03847867622971535,\n",
       "   -0.16705170273780823,\n",
       "   -0.25239279866218567,\n",
       "   0.1104174479842186,\n",
       "   0.2670479118824005,\n",
       "   6.813555955886841e-05,\n",
       "   0.29747146368026733,\n",
       "   -0.34629517793655396,\n",
       "   -0.1728081852197647,\n",
       "   0.3486037850379944,\n",
       "   -0.044970232993364334,\n",
       "   0.06309535354375839,\n",
       "   -0.3987112045288086,\n",
       "   -0.08995184302330017,\n",
       "   0.055363062769174576,\n",
       "   -0.5251463055610657,\n",
       "   -0.21295443177223206,\n",
       "   -0.07076636701822281,\n",
       "   -0.2876071631908417,\n",
       "   -0.4250190854072571,\n",
       "   0.13233092427253723,\n",
       "   0.17970244586467743,\n",
       "   0.33210405707359314,\n",
       "   0.019392957910895348,\n",
       "   0.307645320892334,\n",
       "   -0.049907997250556946,\n",
       "   0.18875832855701447,\n",
       "   -0.059084128588438034,\n",
       "   -0.06327161192893982,\n",
       "   -0.321871817111969,\n",
       "   0.31719207763671875,\n",
       "   -0.06633514910936356,\n",
       "   0.09714176505804062,\n",
       "   0.6615356206893921,\n",
       "   0.0792454406619072,\n",
       "   -0.49192264676094055,\n",
       "   -0.3306052088737488,\n",
       "   0.2516070306301117,\n",
       "   0.528806746006012,\n",
       "   0.19661377370357513,\n",
       "   -0.10171952843666077,\n",
       "   0.005397181957960129,\n",
       "   0.3191366493701935,\n",
       "   0.22122596204280853,\n",
       "   -0.08569315075874329,\n",
       "   -0.05345280095934868,\n",
       "   0.17569617927074432,\n",
       "   0.16806477308273315,\n",
       "   0.11552321910858154,\n",
       "   0.47911983728408813,\n",
       "   -0.2773286700248718,\n",
       "   -0.20398283004760742,\n",
       "   -0.2447502464056015,\n",
       "   0.4472590982913971,\n",
       "   0.05992002412676811,\n",
       "   0.14667096734046936,\n",
       "   -0.02295643836259842,\n",
       "   -0.015727035701274872,\n",
       "   -0.11855358630418777,\n",
       "   0.056117527186870575,\n",
       "   -0.025354892015457153,\n",
       "   0.16087941825389862,\n",
       "   0.23543034493923187],\n",
       "  [0.43803051114082336,\n",
       "   0.21090026199817657,\n",
       "   -0.05267065018415451,\n",
       "   -0.12622539699077606,\n",
       "   -0.05874515324831009,\n",
       "   0.054927412420511246,\n",
       "   0.32029154896736145,\n",
       "   0.199144184589386,\n",
       "   -0.17336906492710114,\n",
       "   -0.00048588216304779053,\n",
       "   -0.1617426872253418,\n",
       "   -0.09440386295318604,\n",
       "   -0.19045305252075195,\n",
       "   0.2249530702829361,\n",
       "   -0.36981385946273804,\n",
       "   -0.023018095642328262,\n",
       "   0.31124600768089294,\n",
       "   -0.048478707671165466,\n",
       "   0.05093800276517868,\n",
       "   -0.03317848592996597,\n",
       "   -0.24663205444812775,\n",
       "   -0.18018189072608948,\n",
       "   -0.3680281937122345,\n",
       "   0.19830217957496643,\n",
       "   -0.2498985081911087,\n",
       "   -0.03530951961874962,\n",
       "   0.25435933470726013,\n",
       "   0.2446199208498001,\n",
       "   -0.0007474720478057861,\n",
       "   0.5119879841804504,\n",
       "   -0.12161312997341156,\n",
       "   -0.3450152277946472,\n",
       "   0.2330550104379654,\n",
       "   0.14886079728603363,\n",
       "   0.23916557431221008,\n",
       "   -0.04996955394744873,\n",
       "   -0.07122359424829483,\n",
       "   0.07648076862096786,\n",
       "   -0.20264528691768646,\n",
       "   0.23693011701107025,\n",
       "   0.012702059000730515,\n",
       "   -0.02176375314593315,\n",
       "   0.10153022408485413,\n",
       "   0.06759724766016006,\n",
       "   0.6030601859092712,\n",
       "   -0.29588231444358826,\n",
       "   -0.10587024688720703,\n",
       "   -0.00755181722342968,\n",
       "   -0.18611672520637512,\n",
       "   0.19917309284210205,\n",
       "   -0.25504007935523987,\n",
       "   0.0751461461186409,\n",
       "   -0.3627380430698395,\n",
       "   -0.009145044721662998,\n",
       "   0.17082472145557404,\n",
       "   -0.17435842752456665,\n",
       "   0.14619281888008118,\n",
       "   -0.041095685213804245,\n",
       "   -0.21357151865959167,\n",
       "   0.05461299791932106,\n",
       "   -0.2720884084701538,\n",
       "   0.13409698009490967,\n",
       "   0.2232130765914917,\n",
       "   0.03171563893556595,\n",
       "   -0.279173344373703,\n",
       "   -0.209393709897995,\n",
       "   -0.007777906954288483,\n",
       "   0.2024708092212677,\n",
       "   -0.19757124781608582,\n",
       "   -0.04267437011003494,\n",
       "   -0.11777583509683609,\n",
       "   -0.13324321806430817,\n",
       "   -0.032353851944208145,\n",
       "   0.018398921936750412,\n",
       "   0.21759507060050964,\n",
       "   -0.30084356665611267,\n",
       "   0.13714556396007538,\n",
       "   -0.1988222897052765,\n",
       "   0.050969213247299194,\n",
       "   0.09462494403123856,\n",
       "   0.30476582050323486,\n",
       "   0.3393493592739105,\n",
       "   -0.019480004906654358,\n",
       "   -0.07364857941865921,\n",
       "   -0.05286877602338791,\n",
       "   -0.12930609285831451,\n",
       "   -0.004092080518603325,\n",
       "   0.13941451907157898,\n",
       "   0.21452558040618896,\n",
       "   0.17876945436000824,\n",
       "   0.20739397406578064,\n",
       "   -0.2784983813762665,\n",
       "   0.017806880176067352,\n",
       "   -0.15293098986148834,\n",
       "   -0.41025322675704956,\n",
       "   -0.38746699690818787,\n",
       "   -0.040022388100624084,\n",
       "   -0.10330233722925186,\n",
       "   -0.08789059519767761,\n",
       "   0.2192867547273636,\n",
       "   0.16402196884155273,\n",
       "   -0.06744550168514252,\n",
       "   -0.0366520993411541,\n",
       "   -0.03238905966281891,\n",
       "   0.06484390795230865,\n",
       "   0.3736611008644104,\n",
       "   0.019250433892011642,\n",
       "   -0.43482959270477295,\n",
       "   -0.27180010080337524,\n",
       "   0.48058032989501953,\n",
       "   0.29454270005226135,\n",
       "   -0.15215334296226501,\n",
       "   0.2519216537475586,\n",
       "   0.07214261591434479,\n",
       "   -0.0666705071926117,\n",
       "   -0.10644858330488205,\n",
       "   -0.22674477100372314,\n",
       "   -0.046892307698726654,\n",
       "   -0.16510483622550964,\n",
       "   0.006545664742588997,\n",
       "   0.4208676815032959,\n",
       "   0.1405261605978012,\n",
       "   0.3808511793613434,\n",
       "   0.04547524452209473,\n",
       "   0.2868904173374176,\n",
       "   0.19731348752975464,\n",
       "   0.004574405029416084,\n",
       "   -0.15157592296600342,\n",
       "   -0.13122963905334473,\n",
       "   0.16861297190189362,\n",
       "   -0.3945450186729431,\n",
       "   -0.08477780222892761,\n",
       "   0.23227329552173615,\n",
       "   0.020448895171284676,\n",
       "   0.03927679359912872,\n",
       "   0.06817270815372467,\n",
       "   -0.5179183483123779,\n",
       "   0.3321281671524048,\n",
       "   -0.8057743906974792,\n",
       "   -0.015311598777770996,\n",
       "   -0.05798099562525749,\n",
       "   0.1577792763710022,\n",
       "   0.22208181023597717,\n",
       "   -0.008002549409866333,\n",
       "   -0.008230810984969139,\n",
       "   -0.16372495889663696,\n",
       "   -0.45709115266799927,\n",
       "   -0.26431190967559814,\n",
       "   0.3087526559829712,\n",
       "   0.18788643181324005,\n",
       "   -0.40488168597221375,\n",
       "   -0.06350308656692505,\n",
       "   -0.039999425411224365,\n",
       "   0.1918584704399109,\n",
       "   -0.03950212895870209,\n",
       "   0.03195418417453766,\n",
       "   -0.08923664689064026,\n",
       "   0.16681423783302307,\n",
       "   0.3545747995376587,\n",
       "   0.18125848472118378,\n",
       "   -0.1423753947019577,\n",
       "   -0.4386010468006134,\n",
       "   0.027753904461860657,\n",
       "   -0.06387430429458618,\n",
       "   0.05645040422677994,\n",
       "   0.3692862093448639,\n",
       "   -0.2795342803001404,\n",
       "   0.06912165880203247,\n",
       "   0.14247490465641022,\n",
       "   0.19478262960910797,\n",
       "   0.30899369716644287,\n",
       "   -0.051172658801078796,\n",
       "   -0.16798630356788635,\n",
       "   0.18936963379383087,\n",
       "   -0.10407859086990356,\n",
       "   0.20873063802719116,\n",
       "   0.024733206257224083,\n",
       "   -0.38827335834503174,\n",
       "   -0.03560175746679306,\n",
       "   0.1583177149295807,\n",
       "   0.16325202584266663,\n",
       "   -0.011504493653774261,\n",
       "   -0.05520860105752945,\n",
       "   -0.36992183327674866,\n",
       "   -0.10951162129640579,\n",
       "   -0.3616718053817749,\n",
       "   0.20348623394966125,\n",
       "   -0.3994660973548889,\n",
       "   -0.002219792455434799,\n",
       "   -0.20102646946907043,\n",
       "   0.032751020044088364,\n",
       "   -0.029845841228961945,\n",
       "   -0.112245112657547,\n",
       "   0.03374448046088219,\n",
       "   -0.35441190004348755,\n",
       "   -0.03676103800535202,\n",
       "   0.06937779486179352,\n",
       "   0.34447306394577026,\n",
       "   0.25859349966049194,\n",
       "   0.15039129555225372,\n",
       "   -0.34424445033073425,\n",
       "   -0.0835123136639595,\n",
       "   0.1750015765428543,\n",
       "   -0.007753821089863777,\n",
       "   0.11199793219566345,\n",
       "   0.20388558506965637,\n",
       "   0.010104186832904816,\n",
       "   -0.057964958250522614,\n",
       "   -0.08703497797250748,\n",
       "   -0.11421170830726624,\n",
       "   0.29567861557006836,\n",
       "   -0.10118889808654785,\n",
       "   0.09944596886634827,\n",
       "   0.2533280849456787,\n",
       "   -0.11213906854391098,\n",
       "   -0.4262566864490509,\n",
       "   0.19721700251102448,\n",
       "   -0.25105878710746765,\n",
       "   -0.38164523243904114,\n",
       "   -0.20430411398410797,\n",
       "   0.2432870864868164,\n",
       "   -0.11683954298496246,\n",
       "   0.4173729419708252,\n",
       "   -0.15499413013458252,\n",
       "   -0.37581437826156616,\n",
       "   0.2192123979330063,\n",
       "   0.3188772201538086,\n",
       "   0.4073411226272583,\n",
       "   0.17026256024837494,\n",
       "   -0.061883870512247086,\n",
       "   0.18801598250865936,\n",
       "   0.13301613926887512,\n",
       "   0.08371837437152863,\n",
       "   -0.31158530712127686,\n",
       "   0.15769745409488678,\n",
       "   -0.39442741870880127,\n",
       "   -0.1828213930130005,\n",
       "   0.10773347318172455,\n",
       "   -0.3037922978401184,\n",
       "   -0.17745698988437653,\n",
       "   0.25191470980644226,\n",
       "   0.2011757493019104,\n",
       "   0.06110173091292381,\n",
       "   -0.022104982286691666,\n",
       "   -0.18423780798912048,\n",
       "   -0.01032920554280281,\n",
       "   -0.21459826827049255,\n",
       "   0.4501461386680603,\n",
       "   0.7791001796722412,\n",
       "   0.04754435643553734,\n",
       "   -0.03791997209191322,\n",
       "   -0.4102311134338379,\n",
       "   0.2836063802242279,\n",
       "   -0.30398616194725037,\n",
       "   0.12932926416397095,\n",
       "   -0.08981732279062271,\n",
       "   0.29589200019836426,\n",
       "   -0.05062814801931381,\n",
       "   0.1031932458281517,\n",
       "   -0.12104981392621994,\n",
       "   0.2321394383907318,\n",
       "   0.16863101720809937,\n",
       "   0.2162065953016281,\n",
       "   0.23430487513542175,\n",
       "   -0.1485918164253235,\n",
       "   -0.3467665910720825,\n",
       "   -0.004560813307762146,\n",
       "   0.08379513770341873,\n",
       "   -0.35075628757476807,\n",
       "   -0.19479304552078247,\n",
       "   -0.06932717561721802,\n",
       "   -0.11023721098899841,\n",
       "   0.18135800957679749,\n",
       "   0.44031330943107605,\n",
       "   0.32171982526779175,\n",
       "   0.008398057892918587,\n",
       "   0.3061648905277252,\n",
       "   -0.4940994679927826,\n",
       "   0.03551328927278519,\n",
       "   -0.3165452778339386,\n",
       "   0.11276326328516006,\n",
       "   0.036523137241601944,\n",
       "   0.1899537742137909,\n",
       "   0.4397030770778656,\n",
       "   -0.23830074071884155,\n",
       "   -0.20870955288410187,\n",
       "   0.15968334674835205,\n",
       "   0.11831549555063248,\n",
       "   0.04039246588945389,\n",
       "   -0.21522080898284912,\n",
       "   0.0284798052161932,\n",
       "   0.18114084005355835,\n",
       "   -0.24297630786895752,\n",
       "   0.031362637877464294,\n",
       "   0.295543372631073,\n",
       "   0.1646290123462677,\n",
       "   -0.1199544370174408,\n",
       "   0.09964796900749207,\n",
       "   -0.025733601301908493,\n",
       "   -0.16909319162368774,\n",
       "   0.037822023034095764,\n",
       "   0.005222201347351074,\n",
       "   0.02671799808740616,\n",
       "   -0.05419598147273064,\n",
       "   -0.2698010802268982,\n",
       "   -0.06399305909872055,\n",
       "   0.006289325654506683,\n",
       "   0.3596092760562897,\n",
       "   0.18837009370326996,\n",
       "   0.16775941848754883,\n",
       "   0.048630744218826294,\n",
       "   -0.09324608743190765,\n",
       "   -0.07806316018104553,\n",
       "   -0.05535025894641876,\n",
       "   -0.28791922330856323,\n",
       "   -0.3368678092956543,\n",
       "   -0.429518461227417,\n",
       "   -0.14738108217716217,\n",
       "   -0.1460045725107193,\n",
       "   -0.018825583159923553,\n",
       "   0.2905736267566681,\n",
       "   0.10377655178308487,\n",
       "   -0.4138585925102234,\n",
       "   0.4599876403808594,\n",
       "   0.015808217227458954,\n",
       "   0.4257695972919464,\n",
       "   -0.22173751890659332,\n",
       "   0.2576162815093994,\n",
       "   0.14323250949382782,\n",
       "   -0.1635952889919281,\n",
       "   -0.31659266352653503,\n",
       "   -0.16561704874038696,\n",
       "   0.1661922186613083,\n",
       "   0.041961491107940674,\n",
       "   -0.41187894344329834,\n",
       "   -0.27600258588790894,\n",
       "   0.17035041749477386,\n",
       "   0.6534947156906128,\n",
       "   -0.6824848651885986,\n",
       "   -0.13542482256889343,\n",
       "   0.08721192926168442,\n",
       "   0.27125638723373413,\n",
       "   0.20926259458065033,\n",
       "   -0.1208391860127449,\n",
       "   -0.1576595902442932,\n",
       "   0.032329339534044266,\n",
       "   0.260153204202652,\n",
       "   0.05075021833181381,\n",
       "   0.08390245586633682,\n",
       "   0.014329101890325546,\n",
       "   -0.21115636825561523,\n",
       "   -0.13296352326869965,\n",
       "   0.02965470403432846,\n",
       "   -0.30675268173217773,\n",
       "   0.3503211438655853,\n",
       "   -0.13680574297904968,\n",
       "   -0.0036113830283284187,\n",
       "   -0.12180429697036743,\n",
       "   0.1505790501832962,\n",
       "   0.11526450514793396,\n",
       "   0.2967047691345215,\n",
       "   -0.152210533618927,\n",
       "   -0.205536350607872,\n",
       "   -0.06789236515760422,\n",
       "   -0.11760534346103668,\n",
       "   0.0012067295610904694,\n",
       "   0.026814859360456467,\n",
       "   0.6970221996307373,\n",
       "   -0.19296199083328247,\n",
       "   -0.20109017193317413,\n",
       "   0.571153461933136,\n",
       "   0.3741534352302551,\n",
       "   -0.03553846850991249,\n",
       "   -0.28386014699935913,\n",
       "   0.1314203143119812,\n",
       "   -0.2202506959438324,\n",
       "   -0.06158127635717392,\n",
       "   0.007960664108395576,\n",
       "   -0.2794681489467621,\n",
       "   -0.2727738320827484,\n",
       "   -0.001176580786705017,\n",
       "   0.24786241352558136,\n",
       "   0.21426552534103394,\n",
       "   -0.18941809237003326,\n",
       "   0.05922728776931763,\n",
       "   0.1029796153306961,\n",
       "   -0.0782879963517189,\n",
       "   -0.06953830271959305,\n",
       "   0.07679559290409088,\n",
       "   -0.0024041756987571716,\n",
       "   -0.44323408603668213,\n",
       "   0.08208049833774567,\n",
       "   0.3659498691558838,\n",
       "   -0.05342340096831322,\n",
       "   -0.13028551638126373,\n",
       "   -0.0533626414835453,\n",
       "   -0.4038729965686798,\n",
       "   -0.05483860522508621,\n",
       "   0.1883615404367447,\n",
       "   -0.3929041028022766,\n",
       "   0.050193194299936295,\n",
       "   0.05341628938913345,\n",
       "   0.029554493725299835,\n",
       "   -0.11488306522369385,\n",
       "   -0.2406918853521347,\n",
       "   -0.42196303606033325,\n",
       "   0.041309602558612823,\n",
       "   -0.04805609583854675,\n",
       "   0.0749545693397522,\n",
       "   0.2477075308561325,\n",
       "   -0.18263301253318787,\n",
       "   -0.24363791942596436,\n",
       "   -0.13797691464424133,\n",
       "   -0.5324181914329529,\n",
       "   -0.0017105340957641602,\n",
       "   -0.4782511591911316,\n",
       "   0.07046859711408615,\n",
       "   -0.17923936247825623,\n",
       "   0.13301271200180054,\n",
       "   0.14417549967765808,\n",
       "   0.19596058130264282,\n",
       "   0.28581446409225464,\n",
       "   0.25897926092147827,\n",
       "   -0.402160108089447,\n",
       "   -0.12183123826980591,\n",
       "   0.3362334668636322,\n",
       "   0.17743773758411407,\n",
       "   -0.10306421667337418,\n",
       "   -0.04938947409391403,\n",
       "   0.12972953915596008,\n",
       "   -0.07298317551612854,\n",
       "   0.01808234117925167,\n",
       "   -0.12282019108533859,\n",
       "   0.05954300984740257,\n",
       "   -0.2724788188934326,\n",
       "   0.15533839166164398,\n",
       "   0.13148340582847595,\n",
       "   0.07626043260097504,\n",
       "   0.06830249726772308,\n",
       "   0.14919257164001465,\n",
       "   -0.08339177817106247,\n",
       "   -0.33649614453315735,\n",
       "   0.038802146911621094,\n",
       "   -0.15943820774555206,\n",
       "   0.15880019962787628,\n",
       "   -0.046206772327423096,\n",
       "   0.24676641821861267,\n",
       "   0.07561318576335907,\n",
       "   0.18306370079517365,\n",
       "   -0.03845829516649246,\n",
       "   0.07522816956043243,\n",
       "   -0.4264800250530243,\n",
       "   0.5815992951393127,\n",
       "   -0.2399565577507019,\n",
       "   0.4808429181575775,\n",
       "   -0.2913370728492737,\n",
       "   -0.20187908411026,\n",
       "   -0.12091843783855438,\n",
       "   -0.2218814194202423,\n",
       "   -0.033799655735492706,\n",
       "   -0.32990700006484985,\n",
       "   0.11025439202785492,\n",
       "   0.11386339366436005,\n",
       "   0.039802853018045425,\n",
       "   0.0053192004561424255,\n",
       "   0.13600941002368927,\n",
       "   0.6356152296066284,\n",
       "   0.09758838266134262,\n",
       "   0.009849369525909424,\n",
       "   0.05500698834657669,\n",
       "   0.052208926528692245,\n",
       "   0.24203146994113922,\n",
       "   0.30488112568855286,\n",
       "   0.48102492094039917,\n",
       "   0.3098185062408447,\n",
       "   -0.3901369571685791,\n",
       "   -0.12614576518535614,\n",
       "   0.13722257316112518,\n",
       "   -0.32691001892089844,\n",
       "   0.1829027384519577,\n",
       "   0.17920441925525665,\n",
       "   0.08139146864414215,\n",
       "   -0.3141728639602661,\n",
       "   0.09027013182640076,\n",
       "   0.3150789737701416,\n",
       "   0.13265936076641083,\n",
       "   0.17703281342983246,\n",
       "   0.07696737349033356,\n",
       "   0.08245819061994553,\n",
       "   -0.02344704419374466,\n",
       "   -0.19062843918800354,\n",
       "   0.23024244606494904,\n",
       "   0.32595163583755493,\n",
       "   0.13202336430549622,\n",
       "   0.2564108967781067,\n",
       "   -0.01114470511674881,\n",
       "   -0.08377419412136078,\n",
       "   -0.0597517155110836,\n",
       "   0.04745222628116608,\n",
       "   0.21019168198108673,\n",
       "   0.0762118399143219,\n",
       "   0.1798323690891266,\n",
       "   0.11683405190706253,\n",
       "   0.2646828293800354,\n",
       "   -0.08653203397989273,\n",
       "   0.4500013291835785,\n",
       "   0.2586762607097626,\n",
       "   0.0017353016883134842,\n",
       "   0.07030840218067169,\n",
       "   -0.08616570383310318,\n",
       "   0.08902305364608765,\n",
       "   -0.4497078061103821,\n",
       "   -0.03960361331701279,\n",
       "   -0.29388701915740967,\n",
       "   -0.05764048546552658,\n",
       "   -0.27893632650375366,\n",
       "   0.17882314324378967,\n",
       "   0.11600087583065033,\n",
       "   -0.1501641571521759,\n",
       "   0.321296751499176,\n",
       "   0.03059149719774723,\n",
       "   0.3201100528240204,\n",
       "   -0.2542575001716614,\n",
       "   -0.3275570571422577,\n",
       "   0.1803496778011322,\n",
       "   -0.3381633460521698,\n",
       "   0.28386738896369934,\n",
       "   -0.23182275891304016,\n",
       "   -0.19019360840320587,\n",
       "   0.24319888651371002,\n",
       "   0.06920710951089859,\n",
       "   0.24478955566883087,\n",
       "   0.4446459710597992,\n",
       "   -0.2517571449279785,\n",
       "   0.27903008460998535,\n",
       "   -0.07325544953346252,\n",
       "   -0.028923753648996353,\n",
       "   0.09787841886281967,\n",
       "   -0.10459631681442261,\n",
       "   -0.19441694021224976,\n",
       "   0.2514326274394989,\n",
       "   -0.12311781942844391,\n",
       "   0.08852331340312958,\n",
       "   -0.030952423810958862,\n",
       "   -0.39934083819389343,\n",
       "   0.4047291576862335,\n",
       "   0.12288513779640198,\n",
       "   0.45952683687210083,\n",
       "   0.4476197063922882,\n",
       "   0.20812705159187317,\n",
       "   0.046648286283016205,\n",
       "   -0.19744470715522766,\n",
       "   -0.41053372621536255,\n",
       "   0.12212560325860977,\n",
       "   0.16474966704845428,\n",
       "   0.1450331211090088,\n",
       "   -0.34454289078712463,\n",
       "   -9.322747230529785,\n",
       "   0.24078291654586792,\n",
       "   -0.4447128176689148,\n",
       "   0.39564138650894165,\n",
       "   -0.38408374786376953,\n",
       "   0.0364982932806015,\n",
       "   -0.055955901741981506,\n",
       "   -0.032186489552259445,\n",
       "   -0.030994823202490807,\n",
       "   0.10308728367090225,\n",
       "   0.09320297837257385,\n",
       "   0.1578974574804306,\n",
       "   0.20670783519744873,\n",
       "   0.016843628138303757,\n",
       "   -0.12958012521266937,\n",
       "   -0.1082371324300766,\n",
       "   0.2999611496925354,\n",
       "   -0.48483309149742126,\n",
       "   0.11482174694538116,\n",
       "   0.021416017785668373,\n",
       "   0.02962305396795273,\n",
       "   -0.2874419093132019,\n",
       "   -0.09994463622570038,\n",
       "   0.28398263454437256,\n",
       "   -0.1782420128583908,\n",
       "   -0.01958383619785309,\n",
       "   0.14008831977844238,\n",
       "   -0.01783723011612892,\n",
       "   0.44972550868988037,\n",
       "   0.016170667484402657,\n",
       "   0.14663000404834747,\n",
       "   -0.13026849925518036,\n",
       "   -0.3358596861362457,\n",
       "   0.1471475213766098,\n",
       "   0.11855454742908478,\n",
       "   -0.18689920008182526,\n",
       "   0.11877818405628204,\n",
       "   -0.6414978504180908,\n",
       "   0.6867640018463135,\n",
       "   -0.3936839997768402,\n",
       "   0.04680967330932617,\n",
       "   -0.17705699801445007,\n",
       "   0.005523025989532471,\n",
       "   0.05874693766236305,\n",
       "   0.20094263553619385,\n",
       "   0.062096431851387024,\n",
       "   -0.024835236370563507,\n",
       "   0.05178956314921379,\n",
       "   -0.04807150736451149,\n",
       "   0.4306739270687103,\n",
       "   0.13396841287612915,\n",
       "   -0.154842808842659,\n",
       "   0.01418590173125267,\n",
       "   -0.13235284388065338,\n",
       "   -0.24366731941699982,\n",
       "   0.0006667310371994972,\n",
       "   -0.0028248317539691925,\n",
       "   0.09867916256189346,\n",
       "   0.2533518075942993,\n",
       "   -0.37478214502334595,\n",
       "   -0.009525656700134277,\n",
       "   0.5556351542472839,\n",
       "   0.675635814666748,\n",
       "   -0.005514390766620636,\n",
       "   -0.1698327362537384,\n",
       "   0.5373855233192444,\n",
       "   -0.23973236978054047,\n",
       "   0.045053914189338684,\n",
       "   0.007625593803822994,\n",
       "   0.2808559536933899,\n",
       "   -0.21621114015579224,\n",
       "   -0.15523949265480042,\n",
       "   0.16535809636116028,\n",
       "   -0.18761245906352997,\n",
       "   -0.07248149067163467,\n",
       "   -0.3388786315917969,\n",
       "   0.03757452219724655,\n",
       "   0.11687026917934418,\n",
       "   -0.025732100009918213,\n",
       "   0.3523543179035187,\n",
       "   -0.11610652506351471,\n",
       "   0.13035596907138824,\n",
       "   0.47674691677093506,\n",
       "   -0.5442945957183838,\n",
       "   0.018883079290390015,\n",
       "   0.2023543119430542,\n",
       "   -0.15825769305229187,\n",
       "   0.04982084780931473,\n",
       "   -0.35557442903518677,\n",
       "   0.2081817239522934,\n",
       "   -0.49924352765083313,\n",
       "   0.12082979828119278,\n",
       "   0.11671959608793259,\n",
       "   -0.0479990616440773,\n",
       "   -0.008167039602994919,\n",
       "   0.4135655462741852,\n",
       "   0.0046642981469631195,\n",
       "   -0.08934316039085388,\n",
       "   0.17423182725906372,\n",
       "   0.14946292340755463,\n",
       "   -0.16346970200538635,\n",
       "   0.20827563107013702,\n",
       "   0.05947357043623924,\n",
       "   -0.40804314613342285,\n",
       "   0.2730100154876709,\n",
       "   0.2885678708553314,\n",
       "   0.21905876696109772,\n",
       "   -0.027915356680750847,\n",
       "   0.035527974367141724,\n",
       "   -0.3440448045730591,\n",
       "   -0.15635690093040466,\n",
       "   0.03787922114133835,\n",
       "   0.27698102593421936,\n",
       "   0.12806947529315948,\n",
       "   -0.07923857867717743,\n",
       "   0.22679218649864197,\n",
       "   0.15942823886871338,\n",
       "   0.2387417107820511,\n",
       "   0.1828162670135498,\n",
       "   -0.17555168271064758,\n",
       "   0.2552700638771057,\n",
       "   -0.49288421869277954,\n",
       "   0.23916170001029968,\n",
       "   -0.015249635092914104,\n",
       "   -0.0480167381465435,\n",
       "   0.23036609590053558,\n",
       "   -0.027865979820489883,\n",
       "   0.26922163367271423,\n",
       "   0.0889364555478096,\n",
       "   -0.1374385505914688,\n",
       "   0.09915449470281601,\n",
       "   0.43327245116233826,\n",
       "   -0.21602007746696472,\n",
       "   0.10483339428901672,\n",
       "   -0.04306734353303909,\n",
       "   0.10380151867866516,\n",
       "   -0.44680365920066833,\n",
       "   0.019532348960638046,\n",
       "   0.14524540305137634,\n",
       "   0.40383511781692505,\n",
       "   -0.2994629442691803,\n",
       "   0.09826628863811493,\n",
       "   0.07292871177196503,\n",
       "   -0.10378282517194748,\n",
       "   0.3837405741214752,\n",
       "   -0.16728124022483826,\n",
       "   -0.16628292202949524,\n",
       "   0.10470827668905258,\n",
       "   0.3505886197090149,\n",
       "   0.029159823432564735,\n",
       "   -0.1318756639957428,\n",
       "   0.31319665908813477,\n",
       "   0.4171103239059448,\n",
       "   -0.11177976429462433,\n",
       "   0.13979628682136536,\n",
       "   -0.31767114996910095,\n",
       "   0.07381905615329742,\n",
       "   -0.19556018710136414,\n",
       "   0.01573535054922104,\n",
       "   0.19369179010391235,\n",
       "   -0.22244426608085632,\n",
       "   0.04331906884908676,\n",
       "   -0.24917803704738617,\n",
       "   0.04778739809989929,\n",
       "   0.018834056332707405,\n",
       "   -0.08522102236747742,\n",
       "   0.1222827285528183,\n",
       "   -0.2836298942565918,\n",
       "   0.04088716208934784,\n",
       "   -0.3267851769924164,\n",
       "   -0.1663251668214798,\n",
       "   0.2025246024131775,\n",
       "   -0.03806758671998978,\n",
       "   -0.0773312896490097,\n",
       "   -0.14201374351978302,\n",
       "   -0.16003647446632385,\n",
       "   -0.15139296650886536,\n",
       "   -0.1806674301624298,\n",
       "   0.3577120304107666,\n",
       "   -0.06347163766622543,\n",
       "   0.1809980422258377,\n",
       "   0.3026464283466339,\n",
       "   -0.05101555585861206,\n",
       "   -0.4416080713272095,\n",
       "   -0.27330872416496277,\n",
       "   0.06614188849925995,\n",
       "   0.10400606691837311,\n",
       "   0.0020874235779047012,\n",
       "   0.1986573040485382,\n",
       "   0.07123616337776184,\n",
       "   0.10624607652425766,\n",
       "   -0.455865740776062,\n",
       "   -0.1340525597333908,\n",
       "   0.05632395297288895,\n",
       "   0.052114710211753845,\n",
       "   0.4160921275615692,\n",
       "   0.11906856298446655,\n",
       "   0.37769970297813416,\n",
       "   -0.03881414234638214,\n",
       "   0.08613443374633789,\n",
       "   -0.00318840891122818,\n",
       "   0.1740264594554901,\n",
       "   0.00782086793333292,\n",
       "   0.07826609164476395,\n",
       "   -0.10115702450275421,\n",
       "   0.03784406930208206,\n",
       "   -0.15570496022701263,\n",
       "   -0.417528361082077,\n",
       "   0.07057555764913559,\n",
       "   0.03555366396903992,\n",
       "   0.11988193541765213],\n",
       "  [-0.1611807495355606,\n",
       "   0.2548774182796478,\n",
       "   -0.3390738368034363,\n",
       "   -0.12996602058410645,\n",
       "   0.5241676568984985,\n",
       "   -0.2399575561285019,\n",
       "   0.39605024456977844,\n",
       "   -0.08184128254652023,\n",
       "   0.09046106785535812,\n",
       "   0.47681131958961487,\n",
       "   0.36458608508110046,\n",
       "   0.08496425300836563,\n",
       "   -0.21909403800964355,\n",
       "   0.20740464329719543,\n",
       "   -0.6709024906158447,\n",
       "   -0.5020636320114136,\n",
       "   0.07063878327608109,\n",
       "   -0.4079698920249939,\n",
       "   -0.3414826989173889,\n",
       "   -0.04170192778110504,\n",
       "   0.28540709614753723,\n",
       "   0.30541563034057617,\n",
       "   -0.09589704871177673,\n",
       "   -0.027225840836763382,\n",
       "   0.16270078718662262,\n",
       "   -0.07065771520137787,\n",
       "   0.32896339893341064,\n",
       "   0.7368724942207336,\n",
       "   0.10515323281288147,\n",
       "   0.39846834540367126,\n",
       "   0.33371472358703613,\n",
       "   0.050209686160087585,\n",
       "   0.019614722579717636,\n",
       "   0.03776215761899948,\n",
       "   0.5269656181335449,\n",
       "   0.577707052230835,\n",
       "   -0.6188894510269165,\n",
       "   0.4102226495742798,\n",
       "   0.022482147440314293,\n",
       "   0.5657497048377991,\n",
       "   -0.015379894524812698,\n",
       "   -0.10566213726997375,\n",
       "   -0.009173404425382614,\n",
       "   -0.03448773920536041,\n",
       "   0.36081773042678833,\n",
       "   0.030257530510425568,\n",
       "   -0.03941423445940018,\n",
       "   -0.12640218436717987,\n",
       "   -0.3816390931606293,\n",
       "   -0.016927212476730347,\n",
       "   0.017097000032663345,\n",
       "   -0.3749006390571594,\n",
       "   -0.2622559368610382,\n",
       "   -0.2754162549972534,\n",
       "   0.052439525723457336,\n",
       "   -0.18997007608413696,\n",
       "   0.18090753257274628,\n",
       "   -0.08110485970973969,\n",
       "   -0.20975974202156067,\n",
       "   0.8003363609313965,\n",
       "   -0.13251066207885742,\n",
       "   0.274300754070282,\n",
       "   -0.05620837211608887,\n",
       "   -0.4608656167984009,\n",
       "   0.2837351858615875,\n",
       "   0.20740868151187897,\n",
       "   0.0765499696135521,\n",
       "   -0.13816764950752258,\n",
       "   0.49509197473526,\n",
       "   0.07816614955663681,\n",
       "   -0.2305743396282196,\n",
       "   0.295487642288208,\n",
       "   -0.16200411319732666,\n",
       "   -0.2595372200012207,\n",
       "   -0.2688928246498108,\n",
       "   -0.38496339321136475,\n",
       "   0.24702483415603638,\n",
       "   0.055131975561380386,\n",
       "   0.024577997624874115,\n",
       "   -0.039387233555316925,\n",
       "   0.1445179134607315,\n",
       "   0.6435463428497314,\n",
       "   -0.10290692746639252,\n",
       "   0.3885154128074646,\n",
       "   -0.5587702989578247,\n",
       "   0.01672029122710228,\n",
       "   -0.5611448287963867,\n",
       "   0.03553328290581703,\n",
       "   0.43896302580833435,\n",
       "   0.7349921464920044,\n",
       "   0.19315221905708313,\n",
       "   -0.5062922835350037,\n",
       "   0.19691011309623718,\n",
       "   -0.3151562809944153,\n",
       "   -0.22122803330421448,\n",
       "   -0.0012899711728096008,\n",
       "   -0.379084974527359,\n",
       "   0.08574900776147842,\n",
       "   -0.5695251226425171,\n",
       "   -0.0939074158668518,\n",
       "   -0.33695676922798157,\n",
       "   -0.01994016394019127,\n",
       "   -0.06315137445926666,\n",
       "   -0.17143528163433075,\n",
       "   -0.15488532185554504,\n",
       "   0.12075021862983704,\n",
       "   -0.1299823522567749,\n",
       "   -0.7910168766975403,\n",
       "   -0.3719598054885864,\n",
       "   0.3715522289276123,\n",
       "   0.1942865550518036,\n",
       "   0.18164297938346863,\n",
       "   0.3181065320968628,\n",
       "   0.31228917837142944,\n",
       "   -0.3338969945907593,\n",
       "   -0.29936593770980835,\n",
       "   -0.14418046176433563,\n",
       "   0.3052655756473541,\n",
       "   0.26496949791908264,\n",
       "   -0.27686330676078796,\n",
       "   0.18579089641571045,\n",
       "   0.15355300903320312,\n",
       "   -0.23716069757938385,\n",
       "   -0.1779569685459137,\n",
       "   0.10366487503051758,\n",
       "   -0.35586968064308167,\n",
       "   -0.06384371221065521,\n",
       "   -0.36368417739868164,\n",
       "   0.20966123044490814,\n",
       "   -0.1785011738538742,\n",
       "   -0.2608455717563629,\n",
       "   -0.117802694439888,\n",
       "   -0.18397943675518036,\n",
       "   0.2641679644584656,\n",
       "   0.18773001432418823,\n",
       "   0.27085021138191223,\n",
       "   -0.8909932374954224,\n",
       "   0.505607545375824,\n",
       "   -0.9116329550743103,\n",
       "   -0.2875382900238037,\n",
       "   0.03411014378070831,\n",
       "   0.46750226616859436,\n",
       "   0.5237367153167725,\n",
       "   -0.3223755359649658,\n",
       "   -0.04780445247888565,\n",
       "   -0.5050094723701477,\n",
       "   -0.3783014416694641,\n",
       "   -0.0677695944905281,\n",
       "   0.35862982273101807,\n",
       "   0.156228706240654,\n",
       "   -0.47065457701683044,\n",
       "   0.11395487189292908,\n",
       "   0.1874782145023346,\n",
       "   -0.6924288272857666,\n",
       "   -0.1241803914308548,\n",
       "   -0.13461747765541077,\n",
       "   -0.38879093527793884,\n",
       "   0.03179120644927025,\n",
       "   0.6316360831260681,\n",
       "   0.5841689705848694,\n",
       "   -0.049560289829969406,\n",
       "   -0.5320765972137451,\n",
       "   -0.38924482464790344,\n",
       "   0.29332929849624634,\n",
       "   0.16334906220436096,\n",
       "   0.09418579936027527,\n",
       "   -0.01152196153998375,\n",
       "   0.17687784135341644,\n",
       "   -0.54714035987854,\n",
       "   0.49979716539382935,\n",
       "   0.5758669972419739,\n",
       "   -0.008185027167201042,\n",
       "   -0.010202914476394653,\n",
       "   0.12731947004795074,\n",
       "   -0.04549132287502289,\n",
       "   -0.05481703579425812,\n",
       "   0.785895586013794,\n",
       "   0.03676268458366394,\n",
       "   -0.25231820344924927,\n",
       "   -0.052078038454055786,\n",
       "   0.4017387330532074,\n",
       "   0.01542278379201889,\n",
       "   -0.43250271677970886,\n",
       "   -0.6307574510574341,\n",
       "   0.10409324616193771,\n",
       "   -0.5508577823638916,\n",
       "   0.2506116032600403,\n",
       "   -0.7649785280227661,\n",
       "   0.5634006857872009,\n",
       "   0.2590900659561157,\n",
       "   0.047441430389881134,\n",
       "   0.02584298700094223,\n",
       "   -0.4543452858924866,\n",
       "   0.15765418112277985,\n",
       "   -0.522941529750824,\n",
       "   0.3572024703025818,\n",
       "   0.0848221480846405,\n",
       "   0.1043473407626152,\n",
       "   -0.17223183810710907,\n",
       "   0.3263714611530304,\n",
       "   -0.07457486540079117,\n",
       "   0.2561500072479248,\n",
       "   -0.20128610730171204,\n",
       "   -0.0574471652507782,\n",
       "   0.18718773126602173,\n",
       "   0.32431158423423767,\n",
       "   -0.2923351228237152,\n",
       "   0.24213194847106934,\n",
       "   -0.32563701272010803,\n",
       "   -0.5171030759811401,\n",
       "   0.5864723324775696,\n",
       "   -0.9867945313453674,\n",
       "   -0.2978243827819824,\n",
       "   -0.22253191471099854,\n",
       "   0.46412473917007446,\n",
       "   -0.36717551946640015,\n",
       "   0.18604408204555511,\n",
       "   -0.07757475972175598,\n",
       "   0.04644836485385895,\n",
       "   0.08029427379369736,\n",
       "   -0.37335243821144104,\n",
       "   -0.08275236189365387,\n",
       "   0.04682134464383125,\n",
       "   -0.4816422760486603,\n",
       "   -0.6080594658851624,\n",
       "   0.7808125615119934,\n",
       "   0.004602642729878426,\n",
       "   0.48728930950164795,\n",
       "   0.07176947593688965,\n",
       "   -0.205489844083786,\n",
       "   0.23054549098014832,\n",
       "   -0.08261406421661377,\n",
       "   0.26670899987220764,\n",
       "   -0.36862102150917053,\n",
       "   0.20790690183639526,\n",
       "   0.41685041785240173,\n",
       "   -0.12099795043468475,\n",
       "   -0.005754808895289898,\n",
       "   -0.47165223956108093,\n",
       "   -0.6004127264022827,\n",
       "   0.14185190200805664,\n",
       "   0.24783499538898468,\n",
       "   -0.17491218447685242,\n",
       "   0.09398319572210312,\n",
       "   -0.07574472576379776,\n",
       "   -0.18352998793125153,\n",
       "   -0.2479405701160431,\n",
       "   0.19859609007835388,\n",
       "   -0.04839111119508743,\n",
       "   -0.20087817311286926,\n",
       "   -0.28997570276260376,\n",
       "   -0.2920900881290436,\n",
       "   -0.1576336771249771,\n",
       "   -0.17393799126148224,\n",
       "   0.5212513208389282,\n",
       "   0.00279898289591074,\n",
       "   -0.11521808803081512,\n",
       "   0.5686087012290955,\n",
       "   0.725494384765625,\n",
       "   -1.0454330444335938,\n",
       "   0.18580175936222076,\n",
       "   0.5087223052978516,\n",
       "   0.288749635219574,\n",
       "   -0.12014219164848328,\n",
       "   -0.3337426781654358,\n",
       "   -0.27898040413856506,\n",
       "   -0.33108267188072205,\n",
       "   -0.4359249174594879,\n",
       "   -0.8061233758926392,\n",
       "   0.29678988456726074,\n",
       "   0.1756453514099121,\n",
       "   -0.3080475926399231,\n",
       "   0.1063879206776619,\n",
       "   0.28880712389945984,\n",
       "   -0.037984449416399,\n",
       "   -0.11371471732854843,\n",
       "   0.18010735511779785,\n",
       "   -0.21715644001960754,\n",
       "   -0.8490080237388611,\n",
       "   -0.5200295448303223,\n",
       "   0.6392515897750854,\n",
       "   0.3721683621406555,\n",
       "   0.4477894604206085,\n",
       "   0.004984211642295122,\n",
       "   -0.5795624852180481,\n",
       "   -0.4607096314430237,\n",
       "   -0.05018433928489685,\n",
       "   0.36190345883369446,\n",
       "   -0.038710836321115494,\n",
       "   -0.572948157787323,\n",
       "   -0.32779449224472046,\n",
       "   0.015909016132354736,\n",
       "   -0.6231561303138733,\n",
       "   -0.11371569335460663,\n",
       "   0.130082368850708,\n",
       "   0.1216183677315712,\n",
       "   -0.057417500764131546,\n",
       "   0.24719005823135376,\n",
       "   -0.45256343483924866,\n",
       "   -0.174396812915802,\n",
       "   -0.26900652050971985,\n",
       "   -0.09504760056734085,\n",
       "   0.4732080399990082,\n",
       "   -0.07921633124351501,\n",
       "   -0.17598050832748413,\n",
       "   -0.004227905068546534,\n",
       "   0.3903290331363678,\n",
       "   0.36839550733566284,\n",
       "   0.459075927734375,\n",
       "   0.37255895137786865,\n",
       "   0.46738138794898987,\n",
       "   -0.09202762693166733,\n",
       "   -0.04713970795273781,\n",
       "   -0.021669656038284302,\n",
       "   -0.010674312710762024,\n",
       "   -0.2628561854362488,\n",
       "   -0.04701526463031769,\n",
       "   -0.19194288551807404,\n",
       "   -0.210114523768425,\n",
       "   -0.08524823933839798,\n",
       "   -0.3213309347629547,\n",
       "   0.2805410325527191,\n",
       "   -0.3578856885433197,\n",
       "   0.47136062383651733,\n",
       "   0.020952485501766205,\n",
       "   0.21679675579071045,\n",
       "   0.1199125349521637,\n",
       "   0.48704004287719727,\n",
       "   0.01572386361658573,\n",
       "   -0.07474103569984436,\n",
       "   0.17614032328128815,\n",
       "   -0.5614815354347229,\n",
       "   -0.22983339428901672,\n",
       "   0.1757364571094513,\n",
       "   -0.6686015725135803,\n",
       "   -0.2573501169681549,\n",
       "   -0.28017285466194153,\n",
       "   0.3230658173561096,\n",
       "   -0.8662567734718323,\n",
       "   0.6142314672470093,\n",
       "   0.20499292016029358,\n",
       "   0.4196791350841522,\n",
       "   0.2565876245498657,\n",
       "   -0.30124783515930176,\n",
       "   0.5664309859275818,\n",
       "   0.4279717803001404,\n",
       "   -0.06222610920667648,\n",
       "   0.4013037085533142,\n",
       "   -0.11381079256534576,\n",
       "   0.03217339888215065,\n",
       "   -0.4732300341129303,\n",
       "   -0.3058848977088928,\n",
       "   -0.032181739807128906,\n",
       "   -0.38531213998794556,\n",
       "   0.6459420919418335,\n",
       "   0.045085638761520386,\n",
       "   -0.06170365959405899,\n",
       "   0.3878108263015747,\n",
       "   0.09355397522449493,\n",
       "   0.6372923851013184,\n",
       "   0.24056567251682281,\n",
       "   -0.00353829562664032,\n",
       "   0.4737624526023865,\n",
       "   0.047441620379686356,\n",
       "   0.2823472023010254,\n",
       "   -0.16813501715660095,\n",
       "   -0.8622676730155945,\n",
       "   -0.20753347873687744,\n",
       "   -0.8606033325195312,\n",
       "   -0.279164582490921,\n",
       "   1.1827244758605957,\n",
       "   -0.23245535790920258,\n",
       "   0.22582446038722992,\n",
       "   -0.1917210817337036,\n",
       "   0.4585869312286377,\n",
       "   -0.3879397511482239,\n",
       "   -0.6489839553833008,\n",
       "   0.05028420686721802,\n",
       "   0.0007893256843090057,\n",
       "   -0.6255216598510742,\n",
       "   -0.2706291377544403,\n",
       "   0.3225241005420685,\n",
       "   0.2522118091583252,\n",
       "   -0.16726475954055786,\n",
       "   0.030149564146995544,\n",
       "   -0.30435487627983093,\n",
       "   0.01907203532755375,\n",
       "   0.4379345178604126,\n",
       "   -0.039125822484493256,\n",
       "   -0.6419829726219177,\n",
       "   0.3682540953159332,\n",
       "   0.32277241349220276,\n",
       "   0.39554762840270996,\n",
       "   -0.41644325852394104,\n",
       "   -0.13892899453639984,\n",
       "   0.34179919958114624,\n",
       "   -0.4647063910961151,\n",
       "   0.4481540322303772,\n",
       "   0.16648197174072266,\n",
       "   -0.12956546247005463,\n",
       "   -0.30280429124832153,\n",
       "   0.25494202971458435,\n",
       "   -0.04078935459256172,\n",
       "   -0.3203646242618561,\n",
       "   -0.6121867895126343,\n",
       "   -0.3908576965332031,\n",
       "   -0.3095928132534027,\n",
       "   0.27793022990226746,\n",
       "   0.08609707653522491,\n",
       "   0.10871464759111404,\n",
       "   -0.26142892241477966,\n",
       "   -0.4056856036186218,\n",
       "   0.014902310445904732,\n",
       "   0.2827586829662323,\n",
       "   0.476077675819397,\n",
       "   -0.40821096301078796,\n",
       "   0.06047012656927109,\n",
       "   -0.09930214285850525,\n",
       "   -0.005339264869689941,\n",
       "   0.040561940521001816,\n",
       "   -0.020959921181201935,\n",
       "   0.04225372523069382,\n",
       "   0.19403907656669617,\n",
       "   -0.165510356426239,\n",
       "   -0.026312533766031265,\n",
       "   0.276335746049881,\n",
       "   0.04080697149038315,\n",
       "   -0.03403600677847862,\n",
       "   -0.416474848985672,\n",
       "   0.0921068862080574,\n",
       "   -0.11870834231376648,\n",
       "   0.7137219309806824,\n",
       "   -0.018477460369467735,\n",
       "   0.8696327209472656,\n",
       "   -0.0678962767124176,\n",
       "   0.15488140285015106,\n",
       "   -0.19119779765605927,\n",
       "   0.13617154955863953,\n",
       "   -0.26006966829299927,\n",
       "   0.24030625820159912,\n",
       "   -0.2703897953033447,\n",
       "   -0.42936182022094727,\n",
       "   -0.6342846155166626,\n",
       "   -0.7949782013893127,\n",
       "   -0.03399836644530296,\n",
       "   -0.4401688873767853,\n",
       "   -0.42362895607948303,\n",
       "   -0.572286069393158,\n",
       "   0.028718063607811928,\n",
       "   -0.3551662564277649,\n",
       "   0.23839156329631805,\n",
       "   0.0560394749045372,\n",
       "   -0.35834380984306335,\n",
       "   0.08771753311157227,\n",
       "   0.29782629013061523,\n",
       "   -0.4206428527832031,\n",
       "   -0.6134311556816101,\n",
       "   -0.31061452627182007,\n",
       "   -0.36819180846214294,\n",
       "   0.008006449788808823,\n",
       "   -1.1116645336151123,\n",
       "   0.31659191846847534,\n",
       "   0.04180491715669632,\n",
       "   0.5331410765647888,\n",
       "   -0.13732551038265228,\n",
       "   -0.11776569485664368,\n",
       "   0.5778924226760864,\n",
       "   0.45156607031822205,\n",
       "   0.8493819236755371,\n",
       "   0.12393427640199661,\n",
       "   0.7081282734870911,\n",
       "   0.6008550524711609,\n",
       "   0.169865682721138,\n",
       "   1.0446957349777222,\n",
       "   0.04611189290881157,\n",
       "   -0.9326903223991394,\n",
       "   0.028310392051935196,\n",
       "   0.3845919370651245,\n",
       "   -0.17224493622779846,\n",
       "   -0.14738668501377106,\n",
       "   -0.6794404983520508,\n",
       "   0.05287381261587143,\n",
       "   -0.39586102962493896,\n",
       "   -0.23576277494430542,\n",
       "   0.033211592584848404,\n",
       "   -0.25991466641426086,\n",
       "   1.1132662296295166,\n",
       "   0.15778298676013947,\n",
       "   0.4002273976802826,\n",
       "   0.3910429775714874,\n",
       "   0.05080527067184448,\n",
       "   -0.2302638590335846,\n",
       "   -0.02312306873500347,\n",
       "   0.14389806985855103,\n",
       "   -0.6486459970474243,\n",
       "   0.13170602917671204,\n",
       "   -0.03208693861961365,\n",
       "   0.13205313682556152,\n",
       "   0.17821776866912842,\n",
       "   -0.4725235104560852,\n",
       "   -0.06972011923789978,\n",
       "   0.18372946977615356,\n",
       "   0.24692565202713013,\n",
       "   0.3128531575202942,\n",
       "   -0.6673086881637573,\n",
       "   1.039926290512085,\n",
       "   0.3275768756866455,\n",
       "   0.054062485694885254,\n",
       "   0.4357718229293823,\n",
       "   0.3967990279197693,\n",
       "   -0.07823684811592102,\n",
       "   -0.2924969792366028,\n",
       "   0.03814610838890076,\n",
       "   -0.02392362430691719,\n",
       "   -0.01219417154788971,\n",
       "   -0.10499157756567001,\n",
       "   0.41431179642677307,\n",
       "   0.1118888109922409,\n",
       "   0.15597602725028992,\n",
       "   -0.22246019542217255,\n",
       "   0.4373754560947418,\n",
       "   0.24591413140296936,\n",
       "   0.2015194296836853,\n",
       "   0.3628278374671936,\n",
       "   0.0650186762213707,\n",
       "   -0.05484318360686302,\n",
       "   0.7511823177337646,\n",
       "   0.6218433380126953,\n",
       "   0.08290039002895355,\n",
       "   0.007583357393741608,\n",
       "   -0.1664692759513855,\n",
       "   -0.020261619240045547,\n",
       "   0.26508548855781555,\n",
       "   -0.30006545782089233,\n",
       "   1.0192819833755493,\n",
       "   -0.25232964754104614,\n",
       "   0.5490957498550415,\n",
       "   -0.1917034089565277,\n",
       "   -0.303382009267807,\n",
       "   -0.24649721384048462,\n",
       "   -0.08461534231901169,\n",
       "   0.02281823754310608,\n",
       "   0.18356791138648987,\n",
       "   0.09961278736591339,\n",
       "   0.154793381690979,\n",
       "   0.3834020793437958,\n",
       "   -0.20922960340976715,\n",
       "   0.3950158357620239,\n",
       "   0.5017167329788208,\n",
       "   -0.1404813975095749,\n",
       "   0.34798264503479004,\n",
       "   -0.00677490234375,\n",
       "   -1.2905389070510864,\n",
       "   0.2572508454322815,\n",
       "   0.30709847807884216,\n",
       "   -0.26462435722351074,\n",
       "   -0.1025046855211258,\n",
       "   -8.461233139038086,\n",
       "   0.4207402467727661,\n",
       "   -0.06890000402927399,\n",
       "   0.3591921031475067,\n",
       "   0.3437114655971527,\n",
       "   0.6763907074928284,\n",
       "   0.25267940759658813,\n",
       "   -0.6744291186332703,\n",
       "   0.01684466190636158,\n",
       "   -0.038938865065574646,\n",
       "   -0.3312196433544159,\n",
       "   -0.33403676748275757,\n",
       "   0.5931398868560791,\n",
       "   -0.06325136870145798,\n",
       "   0.35472163558006287,\n",
       "   0.11524204909801483,\n",
       "   -0.08524613827466965,\n",
       "   0.23076772689819336,\n",
       "   0.6242288947105408,\n",
       "   -0.37884923815727234,\n",
       "   -0.07490311563014984,\n",
       "   0.45675212144851685,\n",
       "   0.250792920589447,\n",
       "   0.14842341840267181,\n",
       "   -0.08614393323659897,\n",
       "   0.1364707052707672,\n",
       "   0.3224852383136749,\n",
       "   0.019159696996212006,\n",
       "   0.12716493010520935,\n",
       "   -0.08718588203191757,\n",
       "   -0.14094462990760803,\n",
       "   0.07003910094499588,\n",
       "   -0.14966775476932526,\n",
       "   -0.1668170541524887,\n",
       "   0.3882991373538971,\n",
       "   -0.3328095078468323,\n",
       "   0.05080324038863182,\n",
       "   -0.1896195262670517,\n",
       "   0.8266004920005798,\n",
       "   -0.47719821333885193,\n",
       "   -0.05515516921877861,\n",
       "   -0.22593024373054504,\n",
       "   -0.3276796340942383,\n",
       "   -0.22564683854579926,\n",
       "   0.26996663212776184,\n",
       "   0.11049845814704895,\n",
       "   0.20235420763492584,\n",
       "   -0.18657252192497253,\n",
       "   -0.6951896548271179,\n",
       "   0.39505767822265625,\n",
       "   0.5411192774772644,\n",
       "   0.27807220816612244,\n",
       "   0.4399767816066742,\n",
       "   0.233804851770401,\n",
       "   -0.6291672587394714,\n",
       "   -0.2125922292470932,\n",
       "   -0.20704719424247742,\n",
       "   0.78133225440979,\n",
       "   0.06425884366035461,\n",
       "   -0.24195292592048645,\n",
       "   0.19828516244888306,\n",
       "   -0.38711774349212646,\n",
       "   1.0662946701049805,\n",
       "   -0.010817989706993103,\n",
       "   -0.16491761803627014,\n",
       "   0.4206256568431854,\n",
       "   0.008915752172470093,\n",
       "   0.3621167242527008,\n",
       "   0.06667636334896088,\n",
       "   -0.26550644636154175,\n",
       "   -0.12798291444778442,\n",
       "   -0.40621617436408997,\n",
       "   0.1557423174381256,\n",
       "   -0.8797316551208496,\n",
       "   0.10944870114326477,\n",
       "   -1.046329379081726,\n",
       "   -0.6048268675804138,\n",
       "   -0.057759884744882584,\n",
       "   -0.3227018415927887,\n",
       "   0.4113859236240387,\n",
       "   -0.27129387855529785,\n",
       "   0.5715921521186829,\n",
       "   0.27490946650505066,\n",
       "   -0.6586938500404358,\n",
       "   0.031068678945302963,\n",
       "   0.11090236902236938,\n",
       "   -0.24582579731941223,\n",
       "   0.2405472993850708,\n",
       "   -0.1413230150938034,\n",
       "   0.06177783012390137,\n",
       "   0.0893571674823761,\n",
       "   0.4066142439842224,\n",
       "   -0.01121453382074833,\n",
       "   -0.021839044988155365,\n",
       "   0.14420334994792938,\n",
       "   0.2681195139884949,\n",
       "   -0.20947501063346863,\n",
       "   -0.227868914604187,\n",
       "   0.06600215286016464,\n",
       "   0.00026322994381189346,\n",
       "   -0.6900690793991089,\n",
       "   -0.08256640285253525,\n",
       "   -0.46704405546188354,\n",
       "   0.4112977385520935,\n",
       "   -0.1239769235253334,\n",
       "   -0.30216044187545776,\n",
       "   0.4677993953227997,\n",
       "   -0.6343552470207214,\n",
       "   0.4759577512741089,\n",
       "   0.35090622305870056,\n",
       "   -0.41110432147979736,\n",
       "   0.48059162497520447,\n",
       "   0.9691584706306458,\n",
       "   -0.6198944449424744,\n",
       "   0.17560705542564392,\n",
       "   0.4637015163898468,\n",
       "   0.6311017870903015,\n",
       "   -0.11954249441623688,\n",
       "   0.28854331374168396,\n",
       "   -0.6181807518005371,\n",
       "   -0.26434826850891113,\n",
       "   -0.31100648641586304,\n",
       "   0.20582851767539978,\n",
       "   -0.08262588083744049,\n",
       "   0.5847302675247192,\n",
       "   -0.3460048735141754,\n",
       "   0.10258395969867706,\n",
       "   -0.08332253992557526,\n",
       "   0.38459110260009766,\n",
       "   0.01618046686053276,\n",
       "   0.2084379643201828,\n",
       "   -0.01862645521759987,\n",
       "   -0.24249927699565887,\n",
       "   0.7938210964202881,\n",
       "   0.27902865409851074,\n",
       "   0.01920473575592041,\n",
       "   -0.17275620996952057,\n",
       "   -0.48921358585357666,\n",
       "   -0.8415647745132446,\n",
       "   -0.3021147549152374,\n",
       "   -0.5301536917686462,\n",
       "   -0.35059666633605957,\n",
       "   0.8121253848075867,\n",
       "   0.46847644448280334,\n",
       "   0.7174308896064758,\n",
       "   -0.4090861976146698,\n",
       "   0.15861114859580994,\n",
       "   0.25298982858657837,\n",
       "   0.034555722028017044,\n",
       "   -0.3481198847293854,\n",
       "   -0.35953155159950256,\n",
       "   0.122152179479599,\n",
       "   0.8088836669921875,\n",
       "   0.6441747546195984,\n",
       "   0.5998246073722839,\n",
       "   -0.7174039483070374,\n",
       "   0.5150080919265747,\n",
       "   0.5524468421936035,\n",
       "   0.03858610987663269,\n",
       "   0.09241286665201187,\n",
       "   0.17091429233551025,\n",
       "   -0.08983251452445984,\n",
       "   0.228657066822052,\n",
       "   1.0073155164718628,\n",
       "   -0.15557783842086792,\n",
       "   0.3272565007209778,\n",
       "   -0.2570047676563263,\n",
       "   0.5124139785766602,\n",
       "   0.1297077238559723,\n",
       "   0.4151151776313782,\n",
       "   -0.06049371883273125,\n",
       "   -0.651508629322052,\n",
       "   0.45937490463256836,\n",
       "   0.31523627042770386,\n",
       "   -0.06234502047300339,\n",
       "   -0.4168168604373932,\n",
       "   0.08525668829679489,\n",
       "   -0.3192557692527771,\n",
       "   0.4633582830429077,\n",
       "   -0.06699118763208389,\n",
       "   0.3200339674949646,\n",
       "   0.48793932795524597,\n",
       "   -0.3741370737552643,\n",
       "   -0.18712741136550903,\n",
       "   -0.8024079203605652,\n",
       "   -0.12542974948883057,\n",
       "   0.29424241185188293,\n",
       "   0.20694340765476227,\n",
       "   0.08666457235813141,\n",
       "   -0.29756560921669006,\n",
       "   0.07736886292695999,\n",
       "   0.2699861526489258,\n",
       "   -0.0989709123969078,\n",
       "   0.15428197383880615,\n",
       "   0.06912261247634888,\n",
       "   0.21941211819648743,\n",
       "   -0.02383672073483467,\n",
       "   0.32676392793655396,\n",
       "   -0.06583917886018753,\n",
       "   -0.2202194631099701,\n",
       "   0.10840220749378204,\n",
       "   0.2581133544445038,\n",
       "   -0.33948975801467896,\n",
       "   -0.311161071062088,\n",
       "   -0.36273324489593506,\n",
       "   -0.1468496024608612,\n",
       "   0.10231982171535492,\n",
       "   0.667039692401886,\n",
       "   -0.07141982018947601,\n",
       "   0.23693789541721344,\n",
       "   0.03896501660346985],\n",
       "  [-0.25803691148757935,\n",
       "   -0.026580924168229103,\n",
       "   -0.00020901020616292953,\n",
       "   0.09458541870117188,\n",
       "   0.16868819296360016,\n",
       "   -0.07082974910736084,\n",
       "   0.13221094012260437,\n",
       "   -0.08993541449308395,\n",
       "   -0.3115692138671875,\n",
       "   0.3077641427516937,\n",
       "   -0.0980658158659935,\n",
       "   0.041830867528915405,\n",
       "   -0.31415510177612305,\n",
       "   -0.15492770075798035,\n",
       "   -0.5019869208335876,\n",
       "   -0.6071441173553467,\n",
       "   0.13161227107048035,\n",
       "   -0.5406485199928284,\n",
       "   -0.05289704352617264,\n",
       "   -0.07065141946077347,\n",
       "   0.05446381866931915,\n",
       "   -0.14147262275218964,\n",
       "   -0.23325802385807037,\n",
       "   -0.15816372632980347,\n",
       "   0.14160677790641785,\n",
       "   -0.5551355481147766,\n",
       "   -0.12598830461502075,\n",
       "   0.7447521090507507,\n",
       "   -0.2074798345565796,\n",
       "   0.45832914113998413,\n",
       "   0.28302544355392456,\n",
       "   -0.22520439326763153,\n",
       "   0.49485963582992554,\n",
       "   0.02336856536567211,\n",
       "   0.03805498033761978,\n",
       "   0.24787349998950958,\n",
       "   -0.28949305415153503,\n",
       "   0.5485415458679199,\n",
       "   0.07099641114473343,\n",
       "   0.3927096724510193,\n",
       "   0.1696021407842636,\n",
       "   0.2640782296657562,\n",
       "   0.050510503351688385,\n",
       "   0.3119933307170868,\n",
       "   -0.7102117538452148,\n",
       "   -0.17547506093978882,\n",
       "   -0.06917232275009155,\n",
       "   -0.5312823057174683,\n",
       "   -0.2797756493091583,\n",
       "   0.10280576348304749,\n",
       "   0.5880903601646423,\n",
       "   -0.13363146781921387,\n",
       "   -0.203806072473526,\n",
       "   -0.10658515989780426,\n",
       "   0.11379913985729218,\n",
       "   -0.4853318929672241,\n",
       "   0.15005217492580414,\n",
       "   0.1257888674736023,\n",
       "   -0.24781015515327454,\n",
       "   0.3740175664424896,\n",
       "   -0.060733236372470856,\n",
       "   -0.15053251385688782,\n",
       "   0.009287779219448566,\n",
       "   -0.05344768241047859,\n",
       "   0.0563703328371048,\n",
       "   0.04451405629515648,\n",
       "   0.3206363022327423,\n",
       "   -0.41704174876213074,\n",
       "   0.20449039340019226,\n",
       "   0.20016202330589294,\n",
       "   -0.19750796258449554,\n",
       "   -0.10165725648403168,\n",
       "   -0.2779812216758728,\n",
       "   -0.2658194899559021,\n",
       "   0.06533122807741165,\n",
       "   -0.08551543951034546,\n",
       "   -0.35670459270477295,\n",
       "   0.0650571659207344,\n",
       "   0.1420602798461914,\n",
       "   0.1788652241230011,\n",
       "   0.2696695029735565,\n",
       "   0.14091072976589203,\n",
       "   -0.20916782319545746,\n",
       "   0.5363059639930725,\n",
       "   0.10246153175830841,\n",
       "   0.1515151560306549,\n",
       "   -0.29991960525512695,\n",
       "   -0.01345745287835598,\n",
       "   -0.11866378039121628,\n",
       "   0.2297435998916626,\n",
       "   0.3306185007095337,\n",
       "   -0.5195227861404419,\n",
       "   0.5238831043243408,\n",
       "   -0.05531306192278862,\n",
       "   -0.1254168152809143,\n",
       "   -0.14465823769569397,\n",
       "   -0.48311638832092285,\n",
       "   -0.3693115711212158,\n",
       "   -0.9509903192520142,\n",
       "   0.08079403638839722,\n",
       "   -0.11169804632663727,\n",
       "   0.46738141775131226,\n",
       "   -0.04734347015619278,\n",
       "   -0.20623481273651123,\n",
       "   0.06429435312747955,\n",
       "   0.16789156198501587,\n",
       "   -0.5912739634513855,\n",
       "   -0.8214200139045715,\n",
       "   -0.38725531101226807,\n",
       "   0.6589659452438354,\n",
       "   0.036763910204172134,\n",
       "   0.3356194496154785,\n",
       "   -0.3125495910644531,\n",
       "   0.2828831374645233,\n",
       "   -0.41041600704193115,\n",
       "   -0.7497537732124329,\n",
       "   -0.0038992995396256447,\n",
       "   0.14202958345413208,\n",
       "   0.11027442663908005,\n",
       "   0.16176782548427582,\n",
       "   0.2506231665611267,\n",
       "   -0.050049979239702225,\n",
       "   -0.15158116817474365,\n",
       "   0.10352317988872528,\n",
       "   0.0014173551462590694,\n",
       "   -0.1292150914669037,\n",
       "   -0.21233105659484863,\n",
       "   -0.47230908274650574,\n",
       "   0.2650698721408844,\n",
       "   -0.16920684278011322,\n",
       "   -0.21612706780433655,\n",
       "   0.1138596385717392,\n",
       "   -0.10639369487762451,\n",
       "   -0.23700061440467834,\n",
       "   0.23395127058029175,\n",
       "   0.13658860325813293,\n",
       "   -0.5958526134490967,\n",
       "   -0.09192997962236404,\n",
       "   -1.182631015777588,\n",
       "   -0.45072853565216064,\n",
       "   -0.2806835472583771,\n",
       "   0.3938457667827606,\n",
       "   0.6745367646217346,\n",
       "   -0.2152610570192337,\n",
       "   0.14130261540412903,\n",
       "   -0.1315605193376541,\n",
       "   -0.5191884636878967,\n",
       "   0.00027627497911453247,\n",
       "   0.29900795221328735,\n",
       "   0.11295796930789948,\n",
       "   -0.1645888239145279,\n",
       "   -0.25166863203048706,\n",
       "   0.21291135251522064,\n",
       "   -0.03473488986492157,\n",
       "   -0.3240460753440857,\n",
       "   -0.037548284977674484,\n",
       "   -0.16651859879493713,\n",
       "   0.006056386977434158,\n",
       "   0.5024868845939636,\n",
       "   0.602590799331665,\n",
       "   0.17680227756500244,\n",
       "   -0.8164892196655273,\n",
       "   -0.13730889558792114,\n",
       "   0.4232577085494995,\n",
       "   0.17150601744651794,\n",
       "   -0.24418798089027405,\n",
       "   -0.06814273446798325,\n",
       "   0.465314656496048,\n",
       "   -0.3865962326526642,\n",
       "   0.11799407750368118,\n",
       "   0.332793653011322,\n",
       "   -0.23555684089660645,\n",
       "   0.5027728080749512,\n",
       "   0.35156986117362976,\n",
       "   0.1005171537399292,\n",
       "   -0.0970325618982315,\n",
       "   0.31536516547203064,\n",
       "   -0.028004609048366547,\n",
       "   -0.045008085668087006,\n",
       "   -0.24231812357902527,\n",
       "   0.23741866648197174,\n",
       "   -0.4947291910648346,\n",
       "   0.1707451194524765,\n",
       "   -0.5892705917358398,\n",
       "   0.30517756938934326,\n",
       "   -0.430869996547699,\n",
       "   -0.02956852689385414,\n",
       "   -0.6974027156829834,\n",
       "   0.16119933128356934,\n",
       "   0.06513407081365585,\n",
       "   0.07700635492801666,\n",
       "   -0.3220255672931671,\n",
       "   -0.5001996755599976,\n",
       "   -0.06798648834228516,\n",
       "   -0.9889782071113586,\n",
       "   0.36374905705451965,\n",
       "   -0.05773206800222397,\n",
       "   0.2952459454536438,\n",
       "   0.07275555282831192,\n",
       "   0.20161914825439453,\n",
       "   -0.005035781301558018,\n",
       "   0.15082693099975586,\n",
       "   -0.30186647176742554,\n",
       "   -0.031036432832479477,\n",
       "   -0.13230645656585693,\n",
       "   0.21461251378059387,\n",
       "   -0.7408348321914673,\n",
       "   0.42254748940467834,\n",
       "   -0.17302948236465454,\n",
       "   -0.6659634113311768,\n",
       "   -0.15661154687404633,\n",
       "   -0.6492680311203003,\n",
       "   0.09099496901035309,\n",
       "   -0.012517750263214111,\n",
       "   -0.5251415967941284,\n",
       "   -0.2879221439361572,\n",
       "   -0.22010564804077148,\n",
       "   0.1430017203092575,\n",
       "   -0.04812423139810562,\n",
       "   -0.13265115022659302,\n",
       "   0.4828249216079712,\n",
       "   0.22073164582252502,\n",
       "   0.3868746757507324,\n",
       "   -0.009591037407517433,\n",
       "   -0.36828336119651794,\n",
       "   0.3337438106536865,\n",
       "   0.34181004762649536,\n",
       "   0.5150430798530579,\n",
       "   0.4715133607387543,\n",
       "   -0.17704182863235474,\n",
       "   -0.16076113283634186,\n",
       "   0.13847443461418152,\n",
       "   -0.008947670459747314,\n",
       "   -0.41638636589050293,\n",
       "   0.26521867513656616,\n",
       "   0.06732255965471268,\n",
       "   -0.06145758926868439,\n",
       "   0.11447268724441528,\n",
       "   -0.7358317375183105,\n",
       "   -0.5847703814506531,\n",
       "   -0.04286659508943558,\n",
       "   -0.20643945038318634,\n",
       "   0.2911491096019745,\n",
       "   0.4463323652744293,\n",
       "   -0.10909515619277954,\n",
       "   0.030716311186552048,\n",
       "   -0.4020671546459198,\n",
       "   0.7018203735351562,\n",
       "   0.17239880561828613,\n",
       "   0.17631110548973083,\n",
       "   -0.3587038815021515,\n",
       "   -0.1786297857761383,\n",
       "   -0.25562867522239685,\n",
       "   0.19948804378509521,\n",
       "   -0.3929445147514343,\n",
       "   -0.43313395977020264,\n",
       "   -0.4303446114063263,\n",
       "   0.5622184872627258,\n",
       "   0.5689755082130432,\n",
       "   -0.7415936589241028,\n",
       "   0.4714929461479187,\n",
       "   0.08371050655841827,\n",
       "   0.5340214967727661,\n",
       "   0.27886053919792175,\n",
       "   -0.09496874362230301,\n",
       "   -0.11151721328496933,\n",
       "   -0.1551423817873001,\n",
       "   -0.43747401237487793,\n",
       "   -0.8668133020401001,\n",
       "   0.7113152742385864,\n",
       "   0.23860445618629456,\n",
       "   0.22847676277160645,\n",
       "   0.5276827216148376,\n",
       "   0.5207660794258118,\n",
       "   -0.14396077394485474,\n",
       "   0.1729499101638794,\n",
       "   0.10046200454235077,\n",
       "   -0.21848656237125397,\n",
       "   -0.052547283470630646,\n",
       "   -0.22555415332317352,\n",
       "   0.2805110812187195,\n",
       "   0.579409658908844,\n",
       "   0.09984757006168365,\n",
       "   0.2269897162914276,\n",
       "   -0.6186578273773193,\n",
       "   0.28054773807525635,\n",
       "   0.06699628382921219,\n",
       "   0.6560161709785461,\n",
       "   -0.10874094814062119,\n",
       "   0.20537519454956055,\n",
       "   -0.3896099627017975,\n",
       "   -0.04029674082994461,\n",
       "   -0.42956820130348206,\n",
       "   0.018299777060747147,\n",
       "   -0.01527865044772625,\n",
       "   0.06062811613082886,\n",
       "   -0.10542818158864975,\n",
       "   0.3536379933357239,\n",
       "   -0.4453755021095276,\n",
       "   -0.32608118653297424,\n",
       "   0.19720463454723358,\n",
       "   0.11971231549978256,\n",
       "   0.09464423358440399,\n",
       "   -0.0161264780908823,\n",
       "   -0.17233778536319733,\n",
       "   -0.06811153888702393,\n",
       "   0.20292839407920837,\n",
       "   -0.018775174394249916,\n",
       "   -0.4276677370071411,\n",
       "   0.7741923928260803,\n",
       "   0.460066556930542,\n",
       "   -0.07403812557458878,\n",
       "   -0.3844035267829895,\n",
       "   -0.6232851147651672,\n",
       "   0.4076051115989685,\n",
       "   -0.49342724680900574,\n",
       "   -0.28425493836402893,\n",
       "   -0.36117318272590637,\n",
       "   -0.05005588382482529,\n",
       "   -0.11414415389299393,\n",
       "   -0.24655534327030182,\n",
       "   0.11512202024459839,\n",
       "   -0.4298476278781891,\n",
       "   0.5296781063079834,\n",
       "   0.04383762925863266,\n",
       "   -0.06048695743083954,\n",
       "   -0.019612222909927368,\n",
       "   0.3970111012458801,\n",
       "   0.43416935205459595,\n",
       "   -0.17765618860721588,\n",
       "   -0.4391704499721527,\n",
       "   -0.30436041951179504,\n",
       "   0.3254065215587616,\n",
       "   0.5063283443450928,\n",
       "   0.15033061802387238,\n",
       "   0.6072776317596436,\n",
       "   0.011619038879871368,\n",
       "   0.03056727908551693,\n",
       "   -0.5295036435127258,\n",
       "   0.5136761665344238,\n",
       "   0.2867601811885834,\n",
       "   0.8457038402557373,\n",
       "   0.4255843162536621,\n",
       "   -0.12779243290424347,\n",
       "   0.3217325210571289,\n",
       "   0.1889231652021408,\n",
       "   -0.11684352159500122,\n",
       "   -0.21690301597118378,\n",
       "   -0.16602878272533417,\n",
       "   -0.31570515036582947,\n",
       "   -0.47001883387565613,\n",
       "   -0.47443655133247375,\n",
       "   -0.3219749331474304,\n",
       "   0.2920230031013489,\n",
       "   0.9017189741134644,\n",
       "   0.1279599964618683,\n",
       "   0.12219735234975815,\n",
       "   0.2040405422449112,\n",
       "   0.4134376049041748,\n",
       "   -0.05320638418197632,\n",
       "   0.32263678312301636,\n",
       "   0.17612679302692413,\n",
       "   -0.14216066896915436,\n",
       "   0.43504029512405396,\n",
       "   0.004195129498839378,\n",
       "   0.06537991017103195,\n",
       "   -0.11007685214281082,\n",
       "   0.13539519906044006,\n",
       "   -0.030017878860235214,\n",
       "   -0.10725199431180954,\n",
       "   0.9718596935272217,\n",
       "   -0.060901518911123276,\n",
       "   0.21223554015159607,\n",
       "   0.1830425262451172,\n",
       "   0.16434597969055176,\n",
       "   -0.15211187303066254,\n",
       "   -0.2818070650100708,\n",
       "   0.12234944850206375,\n",
       "   -0.6682170629501343,\n",
       "   -0.5247048139572144,\n",
       "   -0.06638244539499283,\n",
       "   -0.11201810836791992,\n",
       "   0.38740959763526917,\n",
       "   0.21633340418338776,\n",
       "   0.3654484152793884,\n",
       "   0.008620129898190498,\n",
       "   -0.3068380355834961,\n",
       "   0.27099817991256714,\n",
       "   -0.3368165194988251,\n",
       "   -0.3318994343280792,\n",
       "   -0.039687398821115494,\n",
       "   -0.0962384045124054,\n",
       "   0.4754260778427124,\n",
       "   -0.24192774295806885,\n",
       "   -0.028968732804059982,\n",
       "   -0.25039952993392944,\n",
       "   -0.5193381309509277,\n",
       "   -0.0943332239985466,\n",
       "   0.08773958683013916,\n",
       "   -0.14761310815811157,\n",
       "   -0.1388455033302307,\n",
       "   0.15040209889411926,\n",
       "   -0.20549219846725464,\n",
       "   -0.15395483374595642,\n",
       "   -0.5282682180404663,\n",
       "   -0.37486588954925537,\n",
       "   -0.02021164819598198,\n",
       "   0.5675984621047974,\n",
       "   -0.20962375402450562,\n",
       "   -0.17298905551433563,\n",
       "   -0.29576870799064636,\n",
       "   -0.5375962257385254,\n",
       "   0.10966823995113373,\n",
       "   0.33787938952445984,\n",
       "   0.41288384795188904,\n",
       "   -0.38690823316574097,\n",
       "   -0.3892616033554077,\n",
       "   -0.4302487373352051,\n",
       "   0.2850135862827301,\n",
       "   -0.11109940707683563,\n",
       "   -0.18036535382270813,\n",
       "   0.1416516900062561,\n",
       "   0.13072480261325836,\n",
       "   0.00870419293642044,\n",
       "   0.14146406948566437,\n",
       "   0.3455686569213867,\n",
       "   -0.0028866417706012726,\n",
       "   0.07913024723529816,\n",
       "   -0.8071610331535339,\n",
       "   0.13351574540138245,\n",
       "   0.32628941535949707,\n",
       "   0.00985140074044466,\n",
       "   0.14006006717681885,\n",
       "   0.22691956162452698,\n",
       "   0.02689719945192337,\n",
       "   -0.07606083899736404,\n",
       "   -0.29730650782585144,\n",
       "   0.26456454396247864,\n",
       "   0.37234464287757874,\n",
       "   0.15931232273578644,\n",
       "   0.23426426947116852,\n",
       "   -0.10927194356918335,\n",
       "   -0.17913398146629333,\n",
       "   -1.0576955080032349,\n",
       "   -0.20106421411037445,\n",
       "   0.16288086771965027,\n",
       "   0.2247883826494217,\n",
       "   0.2396276742219925,\n",
       "   0.42261260747909546,\n",
       "   -0.05976904183626175,\n",
       "   0.0374290831387043,\n",
       "   -0.1404040902853012,\n",
       "   0.4226800799369812,\n",
       "   0.01203048974275589,\n",
       "   0.06350362300872803,\n",
       "   -0.6553400158882141,\n",
       "   -0.21470925211906433,\n",
       "   -0.09395299106836319,\n",
       "   0.19316431879997253,\n",
       "   -0.16022202372550964,\n",
       "   -0.9528688192367554,\n",
       "   0.34300869703292847,\n",
       "   -0.10820508003234863,\n",
       "   0.1917925477027893,\n",
       "   0.28331607580184937,\n",
       "   0.14835909008979797,\n",
       "   0.38261210918426514,\n",
       "   -0.04849327355623245,\n",
       "   0.0996408760547638,\n",
       "   0.23071053624153137,\n",
       "   0.31873416900634766,\n",
       "   0.02529105171561241,\n",
       "   0.7659032344818115,\n",
       "   0.34675872325897217,\n",
       "   0.17333130538463593,\n",
       "   -0.4499678909778595,\n",
       "   0.007897242903709412,\n",
       "   0.6057373285293579,\n",
       "   -0.43923911452293396,\n",
       "   -0.6469760537147522,\n",
       "   -0.7218402028083801,\n",
       "   0.24914845824241638,\n",
       "   -0.2867041826248169,\n",
       "   -0.41735053062438965,\n",
       "   0.5088402628898621,\n",
       "   0.09954105317592621,\n",
       "   1.2018715143203735,\n",
       "   -0.05984172970056534,\n",
       "   -0.4512253701686859,\n",
       "   0.49399498105049133,\n",
       "   -0.14932119846343994,\n",
       "   0.4813222289085388,\n",
       "   0.12358332425355911,\n",
       "   0.18168017268180847,\n",
       "   -0.5983604788780212,\n",
       "   -0.14105761051177979,\n",
       "   -0.28807058930397034,\n",
       "   -0.07949576526880264,\n",
       "   -0.17483186721801758,\n",
       "   0.008431588299572468,\n",
       "   -0.15261723101139069,\n",
       "   -0.35728782415390015,\n",
       "   -0.13240282237529755,\n",
       "   0.34068575501441956,\n",
       "   -0.33242613077163696,\n",
       "   0.5240831971168518,\n",
       "   -0.49184536933898926,\n",
       "   0.24798426032066345,\n",
       "   0.13559813797473907,\n",
       "   -0.3383297622203827,\n",
       "   0.2462393045425415,\n",
       "   -0.6132382154464722,\n",
       "   0.2957674264907837,\n",
       "   0.20410595834255219,\n",
       "   0.4670618772506714,\n",
       "   -0.199848935008049,\n",
       "   0.5938975811004639,\n",
       "   0.31613683700561523,\n",
       "   -0.11597713083028793,\n",
       "   -0.0622035451233387,\n",
       "   0.5259624123573303,\n",
       "   0.04496236890554428,\n",
       "   0.11060695350170135,\n",
       "   -0.40935268998146057,\n",
       "   0.1071196049451828,\n",
       "   0.39742234349250793,\n",
       "   0.8131206035614014,\n",
       "   0.06368795037269592,\n",
       "   0.014794610440731049,\n",
       "   0.14655686914920807,\n",
       "   -0.2049531787633896,\n",
       "   0.5492188930511475,\n",
       "   0.16612835228443146,\n",
       "   -0.37950244545936584,\n",
       "   0.9487367272377014,\n",
       "   -0.34620580077171326,\n",
       "   -0.12041106820106506,\n",
       "   0.17074397206306458,\n",
       "   -0.10987567901611328,\n",
       "   0.43470755219459534,\n",
       "   -0.3553902804851532,\n",
       "   0.35323047637939453,\n",
       "   0.16970548033714294,\n",
       "   0.23507066071033478,\n",
       "   0.03813433647155762,\n",
       "   1.0640106201171875,\n",
       "   -0.39309361577033997,\n",
       "   0.2580021023750305,\n",
       "   0.08430952578783035,\n",
       "   0.2009059190750122,\n",
       "   0.6940431594848633,\n",
       "   0.20404447615146637,\n",
       "   -0.40772905945777893,\n",
       "   -0.09767687320709229,\n",
       "   0.3229101300239563,\n",
       "   -0.005515541881322861,\n",
       "   -0.44827720522880554,\n",
       "   -8.640650749206543,\n",
       "   0.676581859588623,\n",
       "   -0.1005440503358841,\n",
       "   -0.026228569447994232,\n",
       "   0.13484501838684082,\n",
       "   0.24147158861160278,\n",
       "   0.17126494646072388,\n",
       "   -0.47975069284439087,\n",
       "   -0.213277667760849,\n",
       "   -0.32257550954818726,\n",
       "   -0.12264928221702576,\n",
       "   -0.017789620906114578,\n",
       "   0.05406728386878967,\n",
       "   -0.08259564638137817,\n",
       "   0.4082428514957428,\n",
       "   -0.34142300486564636,\n",
       "   0.13753491640090942,\n",
       "   0.09191037714481354,\n",
       "   0.6397034525871277,\n",
       "   -0.36878079175949097,\n",
       "   -0.1683272123336792,\n",
       "   0.029922867193818092,\n",
       "   -0.059796884655952454,\n",
       "   0.7439448833465576,\n",
       "   -0.20346999168395996,\n",
       "   0.8705408573150635,\n",
       "   0.3551696836948395,\n",
       "   0.1408940553665161,\n",
       "   0.44728124141693115,\n",
       "   -0.12661117315292358,\n",
       "   -0.26788654923439026,\n",
       "   0.302653044462204,\n",
       "   -0.07628747820854187,\n",
       "   -0.1714259833097458,\n",
       "   -0.01335147675126791,\n",
       "   0.05043758451938629,\n",
       "   0.0854591578245163,\n",
       "   -0.21878394484519958,\n",
       "   -0.3192087709903717,\n",
       "   -0.8778048753738403,\n",
       "   0.23840442299842834,\n",
       "   -0.39746981859207153,\n",
       "   -0.0917515903711319,\n",
       "   0.15258720517158508,\n",
       "   0.5591399669647217,\n",
       "   0.31252700090408325,\n",
       "   -0.030518315732479095,\n",
       "   0.6418727040290833,\n",
       "   0.1253782957792282,\n",
       "   0.253182590007782,\n",
       "   0.4925341308116913,\n",
       "   0.40976613759994507,\n",
       "   0.20669706165790558,\n",
       "   0.3484806716442108,\n",
       "   -0.12764915823936462,\n",
       "   0.1588561087846756,\n",
       "   -0.3979650139808655,\n",
       "   0.6651546955108643,\n",
       "   0.08404435217380524,\n",
       "   -0.8051372766494751,\n",
       "   -0.1565980464220047,\n",
       "   -0.1741269826889038,\n",
       "   0.6929262280464172,\n",
       "   0.0776476040482521,\n",
       "   -0.4472367763519287,\n",
       "   -0.11799949407577515,\n",
       "   -0.42847341299057007,\n",
       "   -0.07523120194673538,\n",
       "   0.04148462042212486,\n",
       "   -0.5630868077278137,\n",
       "   -0.5472525358200073,\n",
       "   -0.26462265849113464,\n",
       "   0.04557152837514877,\n",
       "   -0.356069952249527,\n",
       "   0.2925111949443817,\n",
       "   -0.6638211607933044,\n",
       "   -0.13172748684883118,\n",
       "   0.1089501678943634,\n",
       "   -0.34364357590675354,\n",
       "   0.3819119334220886,\n",
       "   -0.18354400992393494,\n",
       "   0.34074318408966064,\n",
       "   0.4370785355567932,\n",
       "   -0.6676289439201355,\n",
       "   -0.10450448095798492,\n",
       "   0.010519236326217651,\n",
       "   -0.44917330145835876,\n",
       "   0.4352804124355316,\n",
       "   0.25668057799339294,\n",
       "   -0.09171643108129501,\n",
       "   -0.26746585965156555,\n",
       "   0.09788916260004044,\n",
       "   -0.05004037171602249,\n",
       "   -0.03508417308330536,\n",
       "   0.0742405354976654,\n",
       "   -0.04703901708126068,\n",
       "   -0.14557470381259918,\n",
       "   0.19091594219207764,\n",
       "   -0.0048566218465566635,\n",
       "   0.900496780872345,\n",
       "   -0.39042821526527405,\n",
       "   -0.20269934833049774,\n",
       "   -0.42893320322036743,\n",
       "   0.09756731241941452,\n",
       "   -0.0846402645111084,\n",
       "   -0.12718746066093445,\n",
       "   0.11188586056232452,\n",
       "   -0.2401631772518158,\n",
       "   -0.011290494352579117,\n",
       "   -0.03028695657849312,\n",
       "   -0.17167583107948303,\n",
       "   0.4323950707912445,\n",
       "   0.7464094758033752,\n",
       "   -0.02998064085841179,\n",
       "   -0.09443022310733795,\n",
       "   0.45872560143470764,\n",
       "   0.387508362531662,\n",
       "   -0.2126520872116089,\n",
       "   -0.07020799815654755,\n",
       "   -0.2500965893268585,\n",
       "   -0.3435051143169403,\n",
       "   -0.6404613852500916,\n",
       "   0.43854331970214844,\n",
       "   -0.3225850760936737,\n",
       "   0.32572993636131287,\n",
       "   0.4143334627151489,\n",
       "   -0.07656686753034592,\n",
       "   0.17829599976539612,\n",
       "   0.707331120967865,\n",
       "   -0.04004286229610443,\n",
       "   0.6580305099487305,\n",
       "   0.17214617133140564,\n",
       "   -0.42926788330078125,\n",
       "   0.1344699114561081,\n",
       "   0.17528879642486572,\n",
       "   -0.2140900194644928,\n",
       "   0.0015282463282346725,\n",
       "   -0.1957925260066986,\n",
       "   -0.35962235927581787,\n",
       "   -0.12266519665718079,\n",
       "   -0.6709005236625671,\n",
       "   -0.013883057981729507,\n",
       "   0.018594590947031975,\n",
       "   0.35368669033050537,\n",
       "   0.643311619758606,\n",
       "   -0.25192344188690186,\n",
       "   0.06898950040340424,\n",
       "   -0.14182935655117035,\n",
       "   -0.5137292146682739,\n",
       "   -0.45439204573631287,\n",
       "   -0.16873790323734283,\n",
       "   0.1894124448299408,\n",
       "   0.6594245433807373,\n",
       "   0.2438136339187622,\n",
       "   0.6103266477584839,\n",
       "   -0.2937888503074646,\n",
       "   0.33716273307800293,\n",
       "   1.0620768070220947,\n",
       "   -0.33909669518470764,\n",
       "   -0.07685013115406036,\n",
       "   -0.20470896363258362,\n",
       "   -0.0997924953699112,\n",
       "   0.28058531880378723,\n",
       "   0.24900396168231964,\n",
       "   -0.1481887698173523,\n",
       "   0.27832841873168945,\n",
       "   -0.15085214376449585,\n",
       "   0.16574503481388092,\n",
       "   0.1017065942287445,\n",
       "   0.2509642243385315,\n",
       "   0.0583503283560276,\n",
       "   -0.24396765232086182,\n",
       "   0.3004996180534363,\n",
       "   0.17076005041599274,\n",
       "   0.1609550565481186,\n",
       "   -0.2560757100582123,\n",
       "   0.46377453207969666,\n",
       "   0.05866356939077377,\n",
       "   0.20186057686805725,\n",
       "   0.09791452437639236,\n",
       "   0.9664276242256165,\n",
       "   0.5513371229171753,\n",
       "   -0.0750994086265564,\n",
       "   0.02983500063419342,\n",
       "   -0.49791523814201355,\n",
       "   0.03459111973643303,\n",
       "   0.4288221597671509,\n",
       "   0.2616991698741913,\n",
       "   -0.04570024088025093,\n",
       "   -0.10850845277309418,\n",
       "   0.24152588844299316,\n",
       "   0.2013421207666397,\n",
       "   -0.3603684902191162,\n",
       "   0.4849589467048645,\n",
       "   -0.1286456286907196,\n",
       "   0.2094230055809021,\n",
       "   -0.22924649715423584,\n",
       "   -0.3565734922885895,\n",
       "   0.14567619562149048,\n",
       "   -0.01447363942861557,\n",
       "   -0.08789686858654022,\n",
       "   -0.14937156438827515,\n",
       "   0.3680241107940674,\n",
       "   -0.3244099020957947,\n",
       "   -0.3256990313529968,\n",
       "   0.41314923763275146,\n",
       "   0.24584878981113434,\n",
       "   -0.1623287945985794,\n",
       "   0.4430100917816162,\n",
       "   0.21592511236667633,\n",
       "   0.2870866656303406],\n",
       "  [0.2704888880252838,\n",
       "   0.15668241679668427,\n",
       "   -0.21538881957530975,\n",
       "   -0.1682991236448288,\n",
       "   0.6598243117332458,\n",
       "   -0.24779342114925385,\n",
       "   0.34117391705513,\n",
       "   0.015212450176477432,\n",
       "   -0.1518067866563797,\n",
       "   0.26221147179603577,\n",
       "   0.005026239436119795,\n",
       "   0.3260757625102997,\n",
       "   -0.1704380363225937,\n",
       "   0.13932710886001587,\n",
       "   -0.8387949466705322,\n",
       "   0.03736183047294617,\n",
       "   0.28406956791877747,\n",
       "   0.005601579323410988,\n",
       "   -0.025563135743141174,\n",
       "   0.1581997275352478,\n",
       "   0.07068724930286407,\n",
       "   -0.15886642038822174,\n",
       "   0.24196265637874603,\n",
       "   0.052666351199150085,\n",
       "   0.1822461038827896,\n",
       "   0.28754258155822754,\n",
       "   0.32671988010406494,\n",
       "   0.8491677045822144,\n",
       "   -0.17913705110549927,\n",
       "   0.5304561853408813,\n",
       "   0.3548371493816376,\n",
       "   0.21336057782173157,\n",
       "   0.31977441906929016,\n",
       "   -0.024873193353414536,\n",
       "   -0.11033976823091507,\n",
       "   0.03818220645189285,\n",
       "   -0.19125661253929138,\n",
       "   0.2971962094306946,\n",
       "   -0.23124346137046814,\n",
       "   0.12048693001270294,\n",
       "   -0.04031917080283165,\n",
       "   -0.11449263989925385,\n",
       "   -0.023995913565158844,\n",
       "   0.0030201077461242676,\n",
       "   -0.01795753836631775,\n",
       "   -0.1335994452238083,\n",
       "   -0.32513418793678284,\n",
       "   -0.17043471336364746,\n",
       "   0.15771648287773132,\n",
       "   0.3267560303211212,\n",
       "   -0.008189157582819462,\n",
       "   -0.5808807611465454,\n",
       "   -0.24822114408016205,\n",
       "   -0.40274742245674133,\n",
       "   -0.06865199655294418,\n",
       "   -0.16874322295188904,\n",
       "   -0.041234128177165985,\n",
       "   -0.17978882789611816,\n",
       "   -0.5146220326423645,\n",
       "   0.371805340051651,\n",
       "   0.12456700950860977,\n",
       "   0.16852359473705292,\n",
       "   0.6325782537460327,\n",
       "   0.11719655990600586,\n",
       "   -0.18686482310295105,\n",
       "   0.25135764479637146,\n",
       "   0.001454656943678856,\n",
       "   -0.1250779926776886,\n",
       "   0.1994074285030365,\n",
       "   -0.1260053813457489,\n",
       "   0.009528376162052155,\n",
       "   0.021739192306995392,\n",
       "   -0.03098297119140625,\n",
       "   -0.3972286581993103,\n",
       "   0.4722896218299866,\n",
       "   0.0005988627672195435,\n",
       "   0.28818070888519287,\n",
       "   -0.34668925404548645,\n",
       "   -0.3532627522945404,\n",
       "   -0.032103344798088074,\n",
       "   0.40629705786705017,\n",
       "   0.2704189717769623,\n",
       "   0.09890839457511902,\n",
       "   0.3141528069972992,\n",
       "   0.25025397539138794,\n",
       "   -0.010308198630809784,\n",
       "   0.16103726625442505,\n",
       "   0.1984650194644928,\n",
       "   0.15697526931762695,\n",
       "   0.3424021303653717,\n",
       "   -0.09883369505405426,\n",
       "   -0.24419929087162018,\n",
       "   0.0639607161283493,\n",
       "   0.16316282749176025,\n",
       "   -0.4174017310142517,\n",
       "   0.24066594243049622,\n",
       "   -0.42625197768211365,\n",
       "   0.22274009883403778,\n",
       "   -0.037530917674303055,\n",
       "   0.04393419250845909,\n",
       "   -0.134674534201622,\n",
       "   -0.19136419892311096,\n",
       "   0.27775436639785767,\n",
       "   -0.13864588737487793,\n",
       "   0.15246286988258362,\n",
       "   -0.2737397849559784,\n",
       "   -0.5061299204826355,\n",
       "   -0.7519993185997009,\n",
       "   -0.5665062069892883,\n",
       "   0.3571265935897827,\n",
       "   0.05391538888216019,\n",
       "   0.6686203479766846,\n",
       "   0.40178167819976807,\n",
       "   -0.036407649517059326,\n",
       "   -0.4991784691810608,\n",
       "   -0.1225125640630722,\n",
       "   0.15181945264339447,\n",
       "   0.3484557867050171,\n",
       "   -0.04023658111691475,\n",
       "   -0.3909125030040741,\n",
       "   0.41839998960494995,\n",
       "   0.3448885381221771,\n",
       "   -0.025989767163991928,\n",
       "   -0.22647342085838318,\n",
       "   0.02788335084915161,\n",
       "   -0.002520168200135231,\n",
       "   -0.18043562769889832,\n",
       "   -0.14192943274974823,\n",
       "   0.23908907175064087,\n",
       "   0.13966485857963562,\n",
       "   -0.2786042094230652,\n",
       "   -0.418093204498291,\n",
       "   -0.005834158509969711,\n",
       "   0.4547346830368042,\n",
       "   0.048731666058301926,\n",
       "   0.1208546981215477,\n",
       "   -0.32972198724746704,\n",
       "   0.034153178334236145,\n",
       "   -0.9670757055282593,\n",
       "   0.07584848999977112,\n",
       "   -0.44971489906311035,\n",
       "   0.10873768478631973,\n",
       "   0.4678407907485962,\n",
       "   -0.16284003853797913,\n",
       "   -0.04060177505016327,\n",
       "   0.08491195738315582,\n",
       "   -0.20084330439567566,\n",
       "   -0.0955774188041687,\n",
       "   0.39163678884506226,\n",
       "   0.2993854880332947,\n",
       "   -0.38535305857658386,\n",
       "   0.18919937312602997,\n",
       "   0.048753757029771805,\n",
       "   0.06813283264636993,\n",
       "   -0.29731595516204834,\n",
       "   -0.11096104979515076,\n",
       "   0.27128058671951294,\n",
       "   -0.703679084777832,\n",
       "   0.8165761232376099,\n",
       "   0.24340447783470154,\n",
       "   -0.2943010628223419,\n",
       "   -0.4595535099506378,\n",
       "   -0.4243869483470917,\n",
       "   0.363910973072052,\n",
       "   0.25375333428382874,\n",
       "   -0.6121049523353577,\n",
       "   -0.18234899640083313,\n",
       "   0.29444190859794617,\n",
       "   -0.11774718761444092,\n",
       "   0.6756157875061035,\n",
       "   0.4874635636806488,\n",
       "   0.2308993935585022,\n",
       "   -0.23773203790187836,\n",
       "   -0.06251442432403564,\n",
       "   0.1624116450548172,\n",
       "   0.3346039354801178,\n",
       "   0.2555679678916931,\n",
       "   -0.24654443562030792,\n",
       "   0.3070126175880432,\n",
       "   0.049567561596632004,\n",
       "   0.05037682503461838,\n",
       "   -0.04486856609582901,\n",
       "   -0.27035555243492126,\n",
       "   -0.21052232384681702,\n",
       "   -0.1313413381576538,\n",
       "   -0.2342882603406906,\n",
       "   0.14279870688915253,\n",
       "   -0.7091217041015625,\n",
       "   0.6887480616569519,\n",
       "   -0.0779348760843277,\n",
       "   0.07293060421943665,\n",
       "   -0.3115408420562744,\n",
       "   -0.717670202255249,\n",
       "   -0.1034478098154068,\n",
       "   -0.840051531791687,\n",
       "   0.17481431365013123,\n",
       "   0.3153996169567108,\n",
       "   0.20919588208198547,\n",
       "   0.07392917573451996,\n",
       "   0.7409327030181885,\n",
       "   -0.16798043251037598,\n",
       "   0.11457137763500214,\n",
       "   -0.06842206418514252,\n",
       "   0.07183706015348434,\n",
       "   0.05610291659832001,\n",
       "   0.02154899761080742,\n",
       "   -0.36041802167892456,\n",
       "   0.5176113843917847,\n",
       "   -0.18345855176448822,\n",
       "   -0.4168426990509033,\n",
       "   -0.17852988839149475,\n",
       "   -0.34320101141929626,\n",
       "   0.10425791889429092,\n",
       "   0.0008233076077885926,\n",
       "   0.3267160654067993,\n",
       "   -0.3826327919960022,\n",
       "   0.1793615221977234,\n",
       "   -0.3918711841106415,\n",
       "   0.17378738522529602,\n",
       "   0.08455318212509155,\n",
       "   -0.383946031332016,\n",
       "   -0.02222112938761711,\n",
       "   0.6806156635284424,\n",
       "   -0.13604944944381714,\n",
       "   -0.03500014543533325,\n",
       "   0.7893765568733215,\n",
       "   -0.4820172190666199,\n",
       "   0.4446515440940857,\n",
       "   0.46896663308143616,\n",
       "   -0.015163619071245193,\n",
       "   0.19094832241535187,\n",
       "   0.2803398668766022,\n",
       "   -0.02647363394498825,\n",
       "   -0.38848555088043213,\n",
       "   -0.28053537011146545,\n",
       "   0.5652727484703064,\n",
       "   -0.19074392318725586,\n",
       "   -0.18350930511951447,\n",
       "   -0.26253101229667664,\n",
       "   -0.5773023366928101,\n",
       "   0.1741522252559662,\n",
       "   -0.005111847072839737,\n",
       "   0.6079854369163513,\n",
       "   -0.05640903115272522,\n",
       "   -0.17008338868618011,\n",
       "   -0.11082771420478821,\n",
       "   -0.11141179502010345,\n",
       "   0.40516868233680725,\n",
       "   -0.11774921417236328,\n",
       "   0.2437855750322342,\n",
       "   -0.3240918815135956,\n",
       "   -0.07127007097005844,\n",
       "   -0.38785919547080994,\n",
       "   -0.05410357192158699,\n",
       "   0.23277133703231812,\n",
       "   -0.218143492937088,\n",
       "   0.5615977644920349,\n",
       "   -0.11108863353729248,\n",
       "   0.05532713979482651,\n",
       "   -0.5107295513153076,\n",
       "   0.5725882649421692,\n",
       "   0.013936683535575867,\n",
       "   0.2869585454463959,\n",
       "   -0.16210481524467468,\n",
       "   -0.07882022112607956,\n",
       "   -0.11265046894550323,\n",
       "   -0.4859989881515503,\n",
       "   -0.049304187297821045,\n",
       "   -0.512675404548645,\n",
       "   0.4313805103302002,\n",
       "   0.0945458859205246,\n",
       "   -0.09012646973133087,\n",
       "   0.38654273748397827,\n",
       "   0.14490284025669098,\n",
       "   -0.4029698669910431,\n",
       "   0.5476506352424622,\n",
       "   0.31057682633399963,\n",
       "   -0.4348992109298706,\n",
       "   0.1393841952085495,\n",
       "   -0.10260447859764099,\n",
       "   0.3583800196647644,\n",
       "   0.6478508114814758,\n",
       "   -0.33212050795555115,\n",
       "   0.5317214131355286,\n",
       "   -0.5623786449432373,\n",
       "   -0.5293625593185425,\n",
       "   0.16154292225837708,\n",
       "   0.7328402400016785,\n",
       "   0.14152483642101288,\n",
       "   -0.11595302820205688,\n",
       "   0.04775000363588333,\n",
       "   -0.3396628797054291,\n",
       "   -0.27763888239860535,\n",
       "   0.5422872304916382,\n",
       "   0.35272228717803955,\n",
       "   0.06423959136009216,\n",
       "   -0.32937920093536377,\n",
       "   0.5074756145477295,\n",
       "   -0.426339715719223,\n",
       "   -0.27202531695365906,\n",
       "   0.2031649500131607,\n",
       "   -0.025758665055036545,\n",
       "   0.33031216263771057,\n",
       "   0.04340123012661934,\n",
       "   -0.6946108937263489,\n",
       "   0.6312726140022278,\n",
       "   0.10478408634662628,\n",
       "   0.22381369769573212,\n",
       "   0.24453721940517426,\n",
       "   0.4900631904602051,\n",
       "   0.46956145763397217,\n",
       "   0.2251928150653839,\n",
       "   -0.10173358768224716,\n",
       "   -0.03345035761594772,\n",
       "   -0.14090277254581451,\n",
       "   -0.38173773884773254,\n",
       "   -0.049033671617507935,\n",
       "   0.012664956040680408,\n",
       "   0.0009602386271581054,\n",
       "   0.030626796185970306,\n",
       "   -0.1769290417432785,\n",
       "   0.4990819990634918,\n",
       "   -0.4158218204975128,\n",
       "   0.22434131801128387,\n",
       "   -0.05617372319102287,\n",
       "   0.13888531923294067,\n",
       "   -0.09369733929634094,\n",
       "   0.4875762462615967,\n",
       "   -0.1011451706290245,\n",
       "   -0.2252218872308731,\n",
       "   -0.1082426980137825,\n",
       "   -0.10733214765787125,\n",
       "   -0.10540477931499481,\n",
       "   0.39563727378845215,\n",
       "   -0.1121392548084259,\n",
       "   0.06164192408323288,\n",
       "   0.024327345192432404,\n",
       "   0.47014331817626953,\n",
       "   -0.5488746762275696,\n",
       "   0.2576960027217865,\n",
       "   0.015597447752952576,\n",
       "   -0.3209717273712158,\n",
       "   0.11687347292900085,\n",
       "   -0.35220232605934143,\n",
       "   -0.11630924046039581,\n",
       "   0.31123846769332886,\n",
       "   0.39252224564552307,\n",
       "   0.03631728142499924,\n",
       "   0.30670303106307983,\n",
       "   -0.23669546842575073,\n",
       "   -0.35490724444389343,\n",
       "   0.08126808702945709,\n",
       "   -0.17264536023139954,\n",
       "   -0.2919338345527649,\n",
       "   0.37779003381729126,\n",
       "   0.4306500554084778,\n",
       "   0.0679822638630867,\n",
       "   0.09067768603563309,\n",
       "   0.18065963685512543,\n",
       "   -0.12497559189796448,\n",
       "   -0.180204838514328,\n",
       "   -0.27074843645095825,\n",
       "   0.051565300673246384,\n",
       "   -0.0996241346001625,\n",
       "   -0.20803359150886536,\n",
       "   -0.05709020048379898,\n",
       "   -0.502321183681488,\n",
       "   -0.29205840826034546,\n",
       "   0.06015939265489578,\n",
       "   0.234599307179451,\n",
       "   0.5431786179542542,\n",
       "   -0.36660677194595337,\n",
       "   0.5095208287239075,\n",
       "   0.32620370388031006,\n",
       "   0.30905354022979736,\n",
       "   -0.03678476810455322,\n",
       "   -0.0242132768034935,\n",
       "   0.26306843757629395,\n",
       "   0.18294470012187958,\n",
       "   -0.8472190499305725,\n",
       "   0.186133474111557,\n",
       "   0.010790127329528332,\n",
       "   -0.19137927889823914,\n",
       "   0.04862590879201889,\n",
       "   0.2203270047903061,\n",
       "   -0.0718340054154396,\n",
       "   0.05826636031270027,\n",
       "   -0.30758601427078247,\n",
       "   0.5217295289039612,\n",
       "   -0.2900809943675995,\n",
       "   0.28204187750816345,\n",
       "   -0.05290256813168526,\n",
       "   0.2970668375492096,\n",
       "   -0.36826804280281067,\n",
       "   0.1739918440580368,\n",
       "   0.03266330063343048,\n",
       "   -0.6018846035003662,\n",
       "   0.5473194718360901,\n",
       "   0.2560291290283203,\n",
       "   -0.3428504765033722,\n",
       "   0.16273345053195953,\n",
       "   0.2502906918525696,\n",
       "   -0.2144850194454193,\n",
       "   0.2665657103061676,\n",
       "   -0.6228355169296265,\n",
       "   -0.32255423069000244,\n",
       "   -0.016143573448061943,\n",
       "   0.4270678758621216,\n",
       "   0.013884387910366058,\n",
       "   -0.9283337593078613,\n",
       "   0.054453037679195404,\n",
       "   -0.44055846333503723,\n",
       "   0.2837667763233185,\n",
       "   0.8677451610565186,\n",
       "   0.1835462749004364,\n",
       "   -0.15333044528961182,\n",
       "   0.1767890453338623,\n",
       "   -0.2131185084581375,\n",
       "   0.02845630794763565,\n",
       "   0.22995421290397644,\n",
       "   -0.16822484135627747,\n",
       "   -0.4777420461177826,\n",
       "   -0.25052088499069214,\n",
       "   -0.24041500687599182,\n",
       "   0.44111716747283936,\n",
       "   0.20260214805603027,\n",
       "   -0.060364339500665665,\n",
       "   -0.24747234582901,\n",
       "   -0.2124616950750351,\n",
       "   -0.003535322844982147,\n",
       "   -0.06224365159869194,\n",
       "   -0.07603639364242554,\n",
       "   0.06815033406019211,\n",
       "   0.361062616109848,\n",
       "   -0.0190255306661129,\n",
       "   0.23208846151828766,\n",
       "   -0.057168520987033844,\n",
       "   0.1887468695640564,\n",
       "   0.05221977084875107,\n",
       "   -0.28047269582748413,\n",
       "   0.1886342614889145,\n",
       "   -0.5492247939109802,\n",
       "   0.05304446071386337,\n",
       "   -0.8842459917068481,\n",
       "   0.22142457962036133,\n",
       "   -0.26619696617126465,\n",
       "   -0.1766323745250702,\n",
       "   0.24325688183307648,\n",
       "   -0.12930519878864288,\n",
       "   -0.11929590255022049,\n",
       "   0.43411168456077576,\n",
       "   0.09164778143167496,\n",
       "   0.34538811445236206,\n",
       "   0.20361275970935822,\n",
       "   0.3434421122074127,\n",
       "   0.08104626834392548,\n",
       "   -0.36214131116867065,\n",
       "   -0.1348235309123993,\n",
       "   -0.6124111413955688,\n",
       "   0.1677848994731903,\n",
       "   -0.24922053515911102,\n",
       "   0.45567265152931213,\n",
       "   -2.0969659090042114e-05,\n",
       "   0.2646811306476593,\n",
       "   0.21099480986595154,\n",
       "   0.3289877772331238,\n",
       "   -0.29379791021347046,\n",
       "   0.7859093546867371,\n",
       "   -0.06815662980079651,\n",
       "   0.2621147036552429,\n",
       "   0.0964185893535614,\n",
       "   0.36799928545951843,\n",
       "   0.13283690810203552,\n",
       "   0.9933958053588867,\n",
       "   -0.037785470485687256,\n",
       "   0.31912457942962646,\n",
       "   0.17931833863258362,\n",
       "   -0.19380739331245422,\n",
       "   -0.34329676628112793,\n",
       "   0.2200632393360138,\n",
       "   -0.3294244110584259,\n",
       "   0.16755305230617523,\n",
       "   -0.3149671256542206,\n",
       "   0.033647894859313965,\n",
       "   0.0957680493593216,\n",
       "   0.27099329233169556,\n",
       "   0.9067674279212952,\n",
       "   0.24268899857997894,\n",
       "   0.11451900005340576,\n",
       "   0.4106093943119049,\n",
       "   0.2744535803794861,\n",
       "   0.10705794394016266,\n",
       "   -0.03656599670648575,\n",
       "   -0.22545558214187622,\n",
       "   0.20897842943668365,\n",
       "   0.49156832695007324,\n",
       "   0.08622272312641144,\n",
       "   0.22100046277046204,\n",
       "   0.42445215582847595,\n",
       "   0.13810791075229645,\n",
       "   -0.2959926724433899,\n",
       "   0.04071451723575592,\n",
       "   -0.13757942616939545,\n",
       "   0.1646026074886322,\n",
       "   -0.6933708786964417,\n",
       "   0.46627792716026306,\n",
       "   -0.028910044580698013,\n",
       "   0.06575130671262741,\n",
       "   -0.03242667764425278,\n",
       "   0.06256352365016937,\n",
       "   -0.1314077228307724,\n",
       "   -0.5594707131385803,\n",
       "   -0.30025607347488403,\n",
       "   -0.19596588611602783,\n",
       "   -0.1347367763519287,\n",
       "   0.18794286251068115,\n",
       "   0.49604418873786926,\n",
       "   0.1064373254776001,\n",
       "   0.12519922852516174,\n",
       "   -0.03960990533232689,\n",
       "   0.3903629183769226,\n",
       "   0.10821371525526047,\n",
       "   -0.17056874930858612,\n",
       "   0.08031852543354034,\n",
       "   0.6037553548812866,\n",
       "   -0.25391653180122375,\n",
       "   0.44372355937957764,\n",
       "   0.26769310235977173,\n",
       "   -0.16261053085327148,\n",
       "   -0.018006520345807076,\n",
       "   -0.3149082064628601,\n",
       "   -0.09894698858261108,\n",
       "   0.13392458856105804,\n",
       "   -0.3745853006839752,\n",
       "   1.1765953302383423,\n",
       "   0.2271745204925537,\n",
       "   0.1342247724533081,\n",
       "   -0.1414567232131958,\n",
       "   -0.05257548391819,\n",
       "   -0.12496046721935272,\n",
       "   0.003510747104883194,\n",
       "   0.755328893661499,\n",
       "   0.10820424556732178,\n",
       "   0.31130826473236084,\n",
       "   0.0074991993606090546,\n",
       "   0.4806958734989166,\n",
       "   0.1018279641866684,\n",
       "   0.531169056892395,\n",
       "   0.06917569041252136,\n",
       "   0.13883469998836517,\n",
       "   0.3566824793815613,\n",
       "   0.4060056209564209,\n",
       "   -0.7701281309127808,\n",
       "   0.43623143434524536,\n",
       "   0.6099736094474792,\n",
       "   0.5601497888565063,\n",
       "   -0.5176159143447876,\n",
       "   -8.882248878479004,\n",
       "   0.08591829240322113,\n",
       "   0.19002845883369446,\n",
       "   0.0005193725228309631,\n",
       "   0.0601414293050766,\n",
       "   0.15132881700992584,\n",
       "   0.335122287273407,\n",
       "   -0.21233199536800385,\n",
       "   0.3100104331970215,\n",
       "   0.16984966397285461,\n",
       "   -0.0912327915430069,\n",
       "   -0.16460049152374268,\n",
       "   0.4975757300853729,\n",
       "   -0.5497260093688965,\n",
       "   0.0397951677441597,\n",
       "   0.0012768208980560303,\n",
       "   -0.14203643798828125,\n",
       "   -0.06944122910499573,\n",
       "   0.1611465960741043,\n",
       "   -0.2232377827167511,\n",
       "   -0.619208574295044,\n",
       "   -0.029662014916539192,\n",
       "   0.1993045061826706,\n",
       "   0.2560461461544037,\n",
       "   0.0004926547408103943,\n",
       "   0.23917824029922485,\n",
       "   -0.2522028684616089,\n",
       "   -0.19692429900169373,\n",
       "   -0.06634711474180222,\n",
       "   -0.37312981486320496,\n",
       "   -0.24087345600128174,\n",
       "   -0.21920889616012573,\n",
       "   -0.1971622109413147,\n",
       "   -0.015186363831162453,\n",
       "   0.048405393958091736,\n",
       "   -0.25037434697151184,\n",
       "   0.1014641672372818,\n",
       "   0.17023946344852448,\n",
       "   -0.14209240674972534,\n",
       "   -0.37562936544418335,\n",
       "   -0.2304261475801468,\n",
       "   0.16234369575977325,\n",
       "   0.4302489161491394,\n",
       "   -0.10344831645488739,\n",
       "   0.19775860011577606,\n",
       "   -0.11231012642383575,\n",
       "   0.42236605286598206,\n",
       "   0.23827402293682098,\n",
       "   0.087786465883255,\n",
       "   0.48368263244628906,\n",
       "   0.5206184387207031,\n",
       "   0.3055422902107239,\n",
       "   -0.06565054506063461,\n",
       "   -0.16468866169452667,\n",
       "   -0.5692195296287537,\n",
       "   -0.11385378241539001,\n",
       "   -0.13268154859542847,\n",
       "   0.3871884047985077,\n",
       "   -0.3004986643791199,\n",
       "   0.0009191809222102165,\n",
       "   -0.18314909934997559,\n",
       "   -0.4069948196411133,\n",
       "   0.40062159299850464,\n",
       "   -0.3280355930328369,\n",
       "   -0.36208659410476685,\n",
       "   0.06971770524978638,\n",
       "   -0.6421961784362793,\n",
       "   0.14855168759822845,\n",
       "   0.06987325847148895,\n",
       "   -0.08107946068048477,\n",
       "   -0.5309672355651855,\n",
       "   -0.5628448724746704,\n",
       "   -0.17616526782512665,\n",
       "   0.29762589931488037,\n",
       "   -0.0405881330370903,\n",
       "   -0.48409077525138855,\n",
       "   -0.6053865551948547,\n",
       "   0.1538715958595276,\n",
       "   -0.3144032657146454,\n",
       "   0.019195768982172012,\n",
       "   -0.8628642559051514,\n",
       "   0.834604799747467,\n",
       "   0.5519158840179443,\n",
       "   0.00583319365978241,\n",
       "   -0.0809682160615921,\n",
       "   -0.32482460141181946,\n",
       "   -0.15598851442337036,\n",
       "   0.15913936495780945,\n",
       "   -0.06319783627986908,\n",
       "   0.3458787500858307,\n",
       "   0.0007771598175168037,\n",
       "   -0.34493687748908997,\n",
       "   -0.19807717204093933,\n",
       "   -0.05000700056552887,\n",
       "   -0.1122739166021347,\n",
       "   0.05673535540699959,\n",
       "   -0.36781349778175354,\n",
       "   -0.33050429821014404,\n",
       "   0.3967897295951843,\n",
       "   -0.3602331876754761,\n",
       "   -0.6372105479240417,\n",
       "   -0.12956294417381287,\n",
       "   -0.2628241181373596,\n",
       "   0.10332471877336502,\n",
       "   -0.20214833319187164,\n",
       "   -0.3992137312889099,\n",
       "   0.3106653690338135,\n",
       "   -0.47039955854415894,\n",
       "   0.0054001957178115845,\n",
       "   -0.16022594273090363,\n",
       "   0.1288694441318512,\n",
       "   0.26433151960372925,\n",
       "   0.4062961935997009,\n",
       "   -0.5570684671401978,\n",
       "   -0.23155708611011505,\n",
       "   0.04284500703215599,\n",
       "   0.9930725693702698,\n",
       "   0.010312560945749283,\n",
       "   0.0579623207449913,\n",
       "   -0.05208490788936615,\n",
       "   -0.07781324535608292,\n",
       "   -0.5032778978347778,\n",
       "   0.2482863962650299,\n",
       "   0.33959972858428955,\n",
       "   0.1620939075946808,\n",
       "   0.05484529212117195,\n",
       "   0.049875836819410324,\n",
       "   0.05022125691175461,\n",
       "   0.09724545478820801,\n",
       "   -0.14259423315525055,\n",
       "   0.551369845867157,\n",
       "   0.01114982832223177,\n",
       "   -0.011863350868225098,\n",
       "   0.35228800773620605,\n",
       "   0.230276957154274,\n",
       "   -0.2735133469104767,\n",
       "   0.049761299043893814,\n",
       "   -0.3913984000682831,\n",
       "   -0.30829310417175293,\n",
       "   -0.10738897323608398,\n",
       "   -0.5475692749023438,\n",
       "   -0.04023740068078041,\n",
       "   0.4157305359840393,\n",
       "   0.06600774079561234,\n",
       "   0.2638891339302063,\n",
       "   -0.47503677010536194,\n",
       "   -0.05310158059000969,\n",
       "   0.13399043679237366,\n",
       "   -0.14963477849960327,\n",
       "   0.12503552436828613,\n",
       "   -0.4453994333744049,\n",
       "   -0.02000802382826805,\n",
       "   0.10009945929050446,\n",
       "   -0.08962804824113846,\n",
       "   0.281576931476593,\n",
       "   -0.14359770715236664,\n",
       "   0.19875960052013397,\n",
       "   0.06333238631486893,\n",
       "   -0.23100902140140533,\n",
       "   0.052107758820056915,\n",
       "   -0.30702123045921326,\n",
       "   0.1284565031528473,\n",
       "   -0.01936493255198002,\n",
       "   0.10840260982513428,\n",
       "   -0.28543171286582947,\n",
       "   0.09791983664035797,\n",
       "   -0.39573749899864197,\n",
       "   0.32370322942733765,\n",
       "   0.40673303604125977,\n",
       "   -0.30319005250930786,\n",
       "   0.26429128646850586,\n",
       "   0.18984998762607574,\n",
       "   -0.1486130654811859,\n",
       "   0.2233276218175888,\n",
       "   0.02932235784828663,\n",
       "   -0.3148609399795532,\n",
       "   0.12900421023368835,\n",
       "   -0.5674055814743042,\n",
       "   -0.347398579120636,\n",
       "   -0.1631479561328888,\n",
       "   0.10660902410745621,\n",
       "   -0.2072911411523819,\n",
       "   -0.45603466033935547,\n",
       "   -0.14815986156463623,\n",
       "   -0.2616688907146454,\n",
       "   0.1407373547554016,\n",
       "   0.02741130068898201,\n",
       "   0.15168039500713348,\n",
       "   0.15766392648220062,\n",
       "   0.17745599150657654,\n",
       "   -0.047319382429122925,\n",
       "   0.22014375030994415,\n",
       "   -0.6324775218963623,\n",
       "   0.07844565808773041,\n",
       "   -0.05542381852865219,\n",
       "   -0.06431809812784195,\n",
       "   -0.1477411538362503,\n",
       "   0.29430723190307617,\n",
       "   -0.28860434889793396,\n",
       "   0.04944398254156113,\n",
       "   -0.008627980947494507,\n",
       "   -0.16417983174324036,\n",
       "   -0.3013004660606384,\n",
       "   -0.4188292920589447,\n",
       "   0.10776594281196594,\n",
       "   -0.41280895471572876,\n",
       "   0.10787101835012436,\n",
       "   -0.05177200213074684,\n",
       "   -0.08718061447143555,\n",
       "   0.31653541326522827,\n",
       "   -0.023058146238327026],\n",
       "  [0.9966166019439697,\n",
       "   0.5051442980766296,\n",
       "   0.0796351432800293,\n",
       "   1.0646259784698486,\n",
       "   -0.08048888295888901,\n",
       "   -0.6290380954742432,\n",
       "   0.4297941327095032,\n",
       "   0.4789951741695404,\n",
       "   0.11691700667142868,\n",
       "   0.04389842599630356,\n",
       "   -0.2538794279098511,\n",
       "   0.21232949197292328,\n",
       "   -1.1743448972702026,\n",
       "   -0.17467422783374786,\n",
       "   -1.2236239910125732,\n",
       "   -0.5252814292907715,\n",
       "   -0.2666276693344116,\n",
       "   0.008942028507590294,\n",
       "   0.4210279583930969,\n",
       "   0.49823594093322754,\n",
       "   -0.08733562380075455,\n",
       "   0.2260645627975464,\n",
       "   -0.4730410873889923,\n",
       "   -0.037924859672784805,\n",
       "   0.22226640582084656,\n",
       "   -0.3419160842895508,\n",
       "   0.6340206861495972,\n",
       "   2.0982704162597656,\n",
       "   0.02851879596710205,\n",
       "   1.1393637657165527,\n",
       "   0.219307079911232,\n",
       "   -0.004972044378519058,\n",
       "   0.4880189895629883,\n",
       "   0.5020006895065308,\n",
       "   -0.5341131091117859,\n",
       "   0.49867457151412964,\n",
       "   -0.3217369616031647,\n",
       "   0.7203011512756348,\n",
       "   -0.7215858101844788,\n",
       "   0.572851300239563,\n",
       "   -0.1572558879852295,\n",
       "   0.4148956835269928,\n",
       "   0.7037241458892822,\n",
       "   -0.8287929892539978,\n",
       "   0.7197139263153076,\n",
       "   -0.547027587890625,\n",
       "   -0.603475034236908,\n",
       "   0.5604388117790222,\n",
       "   -1.0745937824249268,\n",
       "   1.1847885847091675,\n",
       "   -0.10053856670856476,\n",
       "   -0.79759681224823,\n",
       "   0.21009452641010284,\n",
       "   0.422026664018631,\n",
       "   0.7596239447593689,\n",
       "   -0.6827675700187683,\n",
       "   -0.18409477174282074,\n",
       "   0.2806667387485504,\n",
       "   -0.42032498121261597,\n",
       "   0.2819865643978119,\n",
       "   -0.35841211676597595,\n",
       "   0.11121590435504913,\n",
       "   1.059995174407959,\n",
       "   0.262847900390625,\n",
       "   -1.1338154077529907,\n",
       "   0.2574002146720886,\n",
       "   0.7782741785049438,\n",
       "   -0.5635039210319519,\n",
       "   -0.13030028343200684,\n",
       "   -0.15814082324504852,\n",
       "   1.0779776573181152,\n",
       "   0.0829503983259201,\n",
       "   -0.10979298502206802,\n",
       "   -0.774058997631073,\n",
       "   0.2853127717971802,\n",
       "   -0.822665274143219,\n",
       "   0.2338954359292984,\n",
       "   -0.17730703949928284,\n",
       "   -0.6278398036956787,\n",
       "   0.5158247947692871,\n",
       "   0.6460162401199341,\n",
       "   0.5094988942146301,\n",
       "   -0.6028518080711365,\n",
       "   -0.2548489272594452,\n",
       "   0.9487497210502625,\n",
       "   1.5683313608169556,\n",
       "   0.06184415519237518,\n",
       "   -0.8710124492645264,\n",
       "   -0.06904294341802597,\n",
       "   -0.25585493445396423,\n",
       "   0.4358600080013275,\n",
       "   -0.3820740580558777,\n",
       "   0.13903573155403137,\n",
       "   -0.05646504461765289,\n",
       "   -0.1614113599061966,\n",
       "   -0.07046953588724136,\n",
       "   -0.8051484227180481,\n",
       "   -0.06709558516740799,\n",
       "   -0.6068912744522095,\n",
       "   0.43884989619255066,\n",
       "   -0.7309331893920898,\n",
       "   0.36923304200172424,\n",
       "   0.12787172198295593,\n",
       "   0.6440833210945129,\n",
       "   0.35313910245895386,\n",
       "   -0.23661725223064423,\n",
       "   -0.11988383531570435,\n",
       "   -1.7992641925811768,\n",
       "   -0.14372339844703674,\n",
       "   1.2027791738510132,\n",
       "   -0.5309543013572693,\n",
       "   0.3293687105178833,\n",
       "   -0.023165248334407806,\n",
       "   -0.8793084025382996,\n",
       "   -0.2826250493526459,\n",
       "   0.5906854271888733,\n",
       "   0.16178828477859497,\n",
       "   0.2459057867527008,\n",
       "   0.1546250879764557,\n",
       "   0.23513977229595184,\n",
       "   0.3180633783340454,\n",
       "   0.6830695867538452,\n",
       "   -0.44232767820358276,\n",
       "   0.2431076169013977,\n",
       "   0.22383801639080048,\n",
       "   0.6882484555244446,\n",
       "   -0.7403792142868042,\n",
       "   -0.7198989391326904,\n",
       "   0.6861615180969238,\n",
       "   0.5845158696174622,\n",
       "   -0.35918962955474854,\n",
       "   -0.34829285740852356,\n",
       "   0.8548247814178467,\n",
       "   0.6806478500366211,\n",
       "   0.5622525215148926,\n",
       "   -0.32170575857162476,\n",
       "   0.021657153964042664,\n",
       "   0.14444023370742798,\n",
       "   -5.382904529571533,\n",
       "   -0.779613733291626,\n",
       "   -0.8941155672073364,\n",
       "   -0.005599457770586014,\n",
       "   1.3077343702316284,\n",
       "   -0.6629663109779358,\n",
       "   0.08606007695198059,\n",
       "   0.20400267839431763,\n",
       "   -0.38359594345092773,\n",
       "   0.40120285749435425,\n",
       "   -0.4312456250190735,\n",
       "   0.3963766396045685,\n",
       "   -0.638761579990387,\n",
       "   -0.24249833822250366,\n",
       "   0.03979284688830376,\n",
       "   -0.8439095616340637,\n",
       "   -0.4380849599838257,\n",
       "   -0.5735243558883667,\n",
       "   0.2764880359172821,\n",
       "   0.05580158904194832,\n",
       "   0.5104985237121582,\n",
       "   0.05119321495294571,\n",
       "   -0.6289904713630676,\n",
       "   -0.7949182987213135,\n",
       "   -0.4085782468318939,\n",
       "   -0.5195652842521667,\n",
       "   0.5105631351470947,\n",
       "   -0.6848461031913757,\n",
       "   0.36681509017944336,\n",
       "   0.20423392951488495,\n",
       "   -1.0965381860733032,\n",
       "   1.0480029582977295,\n",
       "   0.4561801254749298,\n",
       "   -0.8430496454238892,\n",
       "   -0.8177802562713623,\n",
       "   0.08611973375082016,\n",
       "   0.5965105891227722,\n",
       "   0.88656085729599,\n",
       "   0.34649038314819336,\n",
       "   0.0013875216245651245,\n",
       "   0.1656966656446457,\n",
       "   -0.5799514055252075,\n",
       "   0.21883171796798706,\n",
       "   0.40203657746315,\n",
       "   -0.2286025583744049,\n",
       "   -0.4290458559989929,\n",
       "   -0.36325308680534363,\n",
       "   -0.7682759761810303,\n",
       "   0.152288556098938,\n",
       "   0.3736918568611145,\n",
       "   1.6406636238098145,\n",
       "   0.11247226595878601,\n",
       "   0.242835134267807,\n",
       "   -0.6825171709060669,\n",
       "   -1.2728902101516724,\n",
       "   0.2696462571620941,\n",
       "   -0.5900673270225525,\n",
       "   -0.751613438129425,\n",
       "   -0.9755032658576965,\n",
       "   -0.15209734439849854,\n",
       "   0.382368803024292,\n",
       "   1.0102920532226562,\n",
       "   -0.5913691520690918,\n",
       "   -0.013176679611206055,\n",
       "   -0.44378674030303955,\n",
       "   0.06705723702907562,\n",
       "   -0.0760849267244339,\n",
       "   -0.13592134416103363,\n",
       "   -0.3084085285663605,\n",
       "   0.05930522084236145,\n",
       "   -0.30219951272010803,\n",
       "   -1.956068515777588,\n",
       "   -0.2783445715904236,\n",
       "   0.283611536026001,\n",
       "   0.4610011577606201,\n",
       "   -0.20101258158683777,\n",
       "   0.260158896446228,\n",
       "   -0.7664265036582947,\n",
       "   -0.5993595123291016,\n",
       "   -0.0555080771446228,\n",
       "   0.024367310106754303,\n",
       "   -0.2475009560585022,\n",
       "   0.42929577827453613,\n",
       "   0.02141609787940979,\n",
       "   0.24583429098129272,\n",
       "   -0.8333982825279236,\n",
       "   0.27040061354637146,\n",
       "   0.8741837739944458,\n",
       "   1.235162615776062,\n",
       "   0.4104243516921997,\n",
       "   0.9193061590194702,\n",
       "   0.29443180561065674,\n",
       "   0.9814881086349487,\n",
       "   0.3362222909927368,\n",
       "   -0.3355731666088104,\n",
       "   -1.0878208875656128,\n",
       "   -0.2534087002277374,\n",
       "   0.510587215423584,\n",
       "   0.09396087378263474,\n",
       "   0.4136310815811157,\n",
       "   -0.7450944185256958,\n",
       "   -0.16089177131652832,\n",
       "   -0.3382852375507355,\n",
       "   -0.030272871255874634,\n",
       "   0.7167389392852783,\n",
       "   -0.5249876976013184,\n",
       "   -0.0163958128541708,\n",
       "   0.33758512139320374,\n",
       "   0.03130848705768585,\n",
       "   0.7587050795555115,\n",
       "   0.1140705943107605,\n",
       "   0.37892070412635803,\n",
       "   -0.05915940925478935,\n",
       "   0.009812891483306885,\n",
       "   -0.43507376313209534,\n",
       "   -0.14671240746974945,\n",
       "   0.7179778814315796,\n",
       "   -0.19640116393566132,\n",
       "   0.6415846943855286,\n",
       "   0.3163132071495056,\n",
       "   -0.0681883841753006,\n",
       "   -0.8220934867858887,\n",
       "   0.11204168945550919,\n",
       "   -0.14812332391738892,\n",
       "   0.5145490765571594,\n",
       "   -0.19566601514816284,\n",
       "   -0.16417114436626434,\n",
       "   0.12814494967460632,\n",
       "   -0.2784128487110138,\n",
       "   0.36980336904525757,\n",
       "   -0.42356622219085693,\n",
       "   0.4767945110797882,\n",
       "   0.3625198006629944,\n",
       "   -0.5520101189613342,\n",
       "   -0.04420441761612892,\n",
       "   0.45391616225242615,\n",
       "   -0.07958551496267319,\n",
       "   1.0332529544830322,\n",
       "   0.5768618583679199,\n",
       "   -0.14286527037620544,\n",
       "   -0.33160537481307983,\n",
       "   -1.18876051902771,\n",
       "   -0.27500492334365845,\n",
       "   0.5394747257232666,\n",
       "   -0.3295271098613739,\n",
       "   1.3745579719543457,\n",
       "   -0.910077691078186,\n",
       "   0.6111759543418884,\n",
       "   0.059578023850917816,\n",
       "   0.7276812195777893,\n",
       "   -0.25699806213378906,\n",
       "   -0.09530559182167053,\n",
       "   -0.07372768223285675,\n",
       "   -0.7207304239273071,\n",
       "   -2.1627888679504395,\n",
       "   -0.6804240942001343,\n",
       "   0.33078867197036743,\n",
       "   -0.6422056555747986,\n",
       "   -0.562590479850769,\n",
       "   0.15342602133750916,\n",
       "   -0.7942054271697998,\n",
       "   -0.27071720361709595,\n",
       "   0.4101298153400421,\n",
       "   0.49176961183547974,\n",
       "   0.10126268863677979,\n",
       "   -0.17397305369377136,\n",
       "   -6.409374237060547,\n",
       "   -0.04659060016274452,\n",
       "   0.3637164831161499,\n",
       "   0.4133484363555908,\n",
       "   -0.5569247603416443,\n",
       "   0.10336089879274368,\n",
       "   1.49155592918396,\n",
       "   0.32203352451324463,\n",
       "   -0.3418823182582855,\n",
       "   0.540574312210083,\n",
       "   0.09602489322423935,\n",
       "   -1.0446794033050537,\n",
       "   0.05905478447675705,\n",
       "   -0.2988530695438385,\n",
       "   -0.7720314860343933,\n",
       "   0.26325514912605286,\n",
       "   0.4404214918613434,\n",
       "   1.3648993968963623,\n",
       "   -0.5464110374450684,\n",
       "   0.6322755813598633,\n",
       "   -0.9537042379379272,\n",
       "   -0.06843538582324982,\n",
       "   -0.7914620637893677,\n",
       "   0.9899349808692932,\n",
       "   0.08796031773090363,\n",
       "   -1.2708133459091187,\n",
       "   0.8117462992668152,\n",
       "   -0.5911773443222046,\n",
       "   -0.16479678452014923,\n",
       "   -0.34194010496139526,\n",
       "   -0.835694432258606,\n",
       "   -0.2520715594291687,\n",
       "   -0.2521521747112274,\n",
       "   0.3393953740596771,\n",
       "   -0.8077855110168457,\n",
       "   0.36378124356269836,\n",
       "   0.9284151196479797,\n",
       "   0.24213600158691406,\n",
       "   -0.034691222012043,\n",
       "   -0.7579531669616699,\n",
       "   -0.05295693501830101,\n",
       "   -0.05127831548452377,\n",
       "   -0.12210466712713242,\n",
       "   -0.1475232094526291,\n",
       "   -0.35739922523498535,\n",
       "   0.14026334881782532,\n",
       "   -0.4730013310909271,\n",
       "   -0.14283908903598785,\n",
       "   -0.2983219623565674,\n",
       "   -1.0388343334197998,\n",
       "   0.21773655712604523,\n",
       "   -0.058680832386016846,\n",
       "   0.34192267060279846,\n",
       "   0.24453720450401306,\n",
       "   -0.40159571170806885,\n",
       "   1.124936580657959,\n",
       "   0.2955511212348938,\n",
       "   0.23193158209323883,\n",
       "   0.864278256893158,\n",
       "   -0.2646962106227875,\n",
       "   -1.0354658365249634,\n",
       "   0.07946765422821045,\n",
       "   -0.5151579976081848,\n",
       "   0.009705223143100739,\n",
       "   0.004450403153896332,\n",
       "   0.7084026336669922,\n",
       "   1.5800018310546875,\n",
       "   -0.12735973298549652,\n",
       "   0.12497851252555847,\n",
       "   -0.211493581533432,\n",
       "   0.42237013578414917,\n",
       "   0.443771094083786,\n",
       "   -0.12323904782533646,\n",
       "   -0.17022477090358734,\n",
       "   -0.3763629198074341,\n",
       "   -2.5258712768554688,\n",
       "   0.12171846628189087,\n",
       "   0.20285680890083313,\n",
       "   0.4640073776245117,\n",
       "   -0.3288494646549225,\n",
       "   0.3366895318031311,\n",
       "   0.07762802392244339,\n",
       "   -0.10144306719303131,\n",
       "   -0.07687735557556152,\n",
       "   0.6196879744529724,\n",
       "   0.6219300627708435,\n",
       "   0.7304767966270447,\n",
       "   -0.17463240027427673,\n",
       "   0.41307035088539124,\n",
       "   -0.8491219878196716,\n",
       "   -1.5872986316680908,\n",
       "   -0.2681031823158264,\n",
       "   -0.9491361975669861,\n",
       "   0.4100206196308136,\n",
       "   -0.5752220749855042,\n",
       "   -0.3670581579208374,\n",
       "   -0.7260393500328064,\n",
       "   -0.15508431196212769,\n",
       "   -0.691481351852417,\n",
       "   0.05365854501724243,\n",
       "   -0.6965556740760803,\n",
       "   -0.4468706250190735,\n",
       "   -0.06434545665979385,\n",
       "   0.5237193703651428,\n",
       "   -0.04626382142305374,\n",
       "   0.29587864875793457,\n",
       "   -0.7005380392074585,\n",
       "   -0.7283106446266174,\n",
       "   0.292477548122406,\n",
       "   1.0486488342285156,\n",
       "   0.7302680015563965,\n",
       "   -1.5917119979858398,\n",
       "   0.1862293928861618,\n",
       "   0.12528197467327118,\n",
       "   0.519064724445343,\n",
       "   0.6232802271842957,\n",
       "   0.7696933150291443,\n",
       "   -0.25548869371414185,\n",
       "   0.06644052267074585,\n",
       "   -0.4419975280761719,\n",
       "   1.312817096710205,\n",
       "   0.4945221543312073,\n",
       "   0.5364686250686646,\n",
       "   -0.24784567952156067,\n",
       "   0.029971901327371597,\n",
       "   0.43332844972610474,\n",
       "   -0.009823253378272057,\n",
       "   0.32519569993019104,\n",
       "   -0.6948183178901672,\n",
       "   0.6346620321273804,\n",
       "   -0.7995097637176514,\n",
       "   -0.09101227670907974,\n",
       "   -0.6642323732376099,\n",
       "   0.6526356339454651,\n",
       "   -0.7232746481895447,\n",
       "   -0.5824552178382874,\n",
       "   -0.022725114598870277,\n",
       "   -0.9558699131011963,\n",
       "   -0.3048466145992279,\n",
       "   -1.642821192741394,\n",
       "   -0.16110679507255554,\n",
       "   -0.5148828029632568,\n",
       "   -0.11817087978124619,\n",
       "   0.4577786326408386,\n",
       "   0.8849971294403076,\n",
       "   -1.0239150524139404,\n",
       "   -0.1493048518896103,\n",
       "   0.13701829314231873,\n",
       "   -0.1928696185350418,\n",
       "   -0.4469330310821533,\n",
       "   0.5568049550056458,\n",
       "   0.22600215673446655,\n",
       "   0.9051112532615662,\n",
       "   0.0925818458199501,\n",
       "   -0.10196717083454132,\n",
       "   1.2752865552902222,\n",
       "   -1.374114990234375,\n",
       "   1.2203530073165894,\n",
       "   -1.0049391984939575,\n",
       "   1.222594141960144,\n",
       "   5.112768173217773,\n",
       "   0.4303187131881714,\n",
       "   0.0551995113492012,\n",
       "   1.0985205173492432,\n",
       "   -0.6836620569229126,\n",
       "   -0.05134126543998718,\n",
       "   -0.4795702397823334,\n",
       "   0.29901906847953796,\n",
       "   1.5985430479049683,\n",
       "   1.566860318183899,\n",
       "   -0.40182414650917053,\n",
       "   -0.07427646219730377,\n",
       "   -0.543463945388794,\n",
       "   0.5273316502571106,\n",
       "   -0.1227610856294632,\n",
       "   0.6446515917778015,\n",
       "   0.3823086619377136,\n",
       "   0.24053937196731567,\n",
       "   -0.6423049569129944,\n",
       "   -0.48001426458358765,\n",
       "   1.1436489820480347,\n",
       "   0.1059863418340683,\n",
       "   0.7766117453575134,\n",
       "   -0.029250649735331535,\n",
       "   -0.016469329595565796,\n",
       "   0.5901606678962708,\n",
       "   0.2668987214565277,\n",
       "   0.15016505122184753,\n",
       "   -0.7249329686164856,\n",
       "   0.39419081807136536,\n",
       "   0.4448661804199219,\n",
       "   0.8646469712257385,\n",
       "   -0.05879248306155205,\n",
       "   0.29690271615982056,\n",
       "   -0.1851050853729248,\n",
       "   0.3629438579082489,\n",
       "   0.1307174414396286,\n",
       "   -0.3848940134048462,\n",
       "   -0.13767024874687195,\n",
       "   0.45853739976882935,\n",
       "   0.09381809085607529,\n",
       "   1.0631779432296753,\n",
       "   0.0668179839849472,\n",
       "   -0.5895479917526245,\n",
       "   -0.14019128680229187,\n",
       "   -1.2560522556304932,\n",
       "   -0.4526906907558441,\n",
       "   -1.610230565071106,\n",
       "   -0.157863050699234,\n",
       "   -0.032844457775354385,\n",
       "   -0.1695462465286255,\n",
       "   0.09798042476177216,\n",
       "   0.9523449540138245,\n",
       "   0.1175832748413086,\n",
       "   0.4550609886646271,\n",
       "   0.1905881017446518,\n",
       "   -0.4279687702655792,\n",
       "   -0.4608759582042694,\n",
       "   -1.6068918704986572,\n",
       "   -0.011250179260969162,\n",
       "   0.1077006384730339,\n",
       "   0.08881443738937378,\n",
       "   0.8807702660560608,\n",
       "   0.6028743982315063,\n",
       "   0.471426784992218,\n",
       "   0.11569920182228088,\n",
       "   -1.0927025079727173,\n",
       "   -0.5552641749382019,\n",
       "   -0.18707221746444702,\n",
       "   -0.1894381046295166,\n",
       "   1.2592533826828003,\n",
       "   -0.08302171528339386,\n",
       "   -0.4243992269039154,\n",
       "   0.0046468451619148254,\n",
       "   0.12677539885044098,\n",
       "   0.2844524383544922,\n",
       "   0.17360244691371918,\n",
       "   0.9756999611854553,\n",
       "   0.014936000108718872,\n",
       "   -0.36958155035972595,\n",
       "   -0.10539407283067703,\n",
       "   0.42237499356269836,\n",
       "   -0.22875690460205078,\n",
       "   1.0328476428985596,\n",
       "   0.29992520809173584,\n",
       "   0.49591541290283203,\n",
       "   0.4197915196418762,\n",
       "   0.8442890644073486,\n",
       "   -1.7034056186676025,\n",
       "   0.5264270305633545,\n",
       "   1.2397984266281128,\n",
       "   0.08535778522491455,\n",
       "   -0.9382041692733765,\n",
       "   -1.51941978931427,\n",
       "   0.4530242681503296,\n",
       "   0.1463508903980255,\n",
       "   0.1871826946735382,\n",
       "   -0.20729336142539978,\n",
       "   0.3681420385837555,\n",
       "   0.7737175822257996,\n",
       "   0.04379706084728241,\n",
       "   0.4584098756313324,\n",
       "   0.4226534962654114,\n",
       "   -0.7509169578552246,\n",
       "   0.566909670829773,\n",
       "   0.23469766974449158,\n",
       "   -0.046615611761808395,\n",
       "   -0.33819323778152466,\n",
       "   0.1843036711215973,\n",
       "   -0.032630421221256256,\n",
       "   -0.7767745852470398,\n",
       "   0.829814612865448,\n",
       "   0.7776120901107788,\n",
       "   0.5364073514938354,\n",
       "   0.4809395968914032,\n",
       "   -0.0727328211069107,\n",
       "   0.2522662878036499,\n",
       "   0.5163940787315369,\n",
       "   0.7864487171173096,\n",
       "   -0.3075459599494934,\n",
       "   -0.33328935503959656,\n",
       "   0.6611073017120361,\n",
       "   -0.7583494186401367,\n",
       "   -0.4271765947341919,\n",
       "   0.436065137386322,\n",
       "   -0.5050907135009766,\n",
       "   -0.0008140564896166325,\n",
       "   0.2606760859489441,\n",
       "   -1.3681904077529907,\n",
       "   0.16441187262535095,\n",
       "   -0.04791706055402756,\n",
       "   0.046414297074079514,\n",
       "   -1.6265337467193604,\n",
       "   -0.6035727262496948,\n",
       "   0.16399432718753815,\n",
       "   -0.3901689648628235,\n",
       "   0.9699586629867554,\n",
       "   0.5005285143852234,\n",
       "   0.03405505418777466,\n",
       "   -0.8668604493141174,\n",
       "   0.24439893662929535,\n",
       "   0.18158771097660065,\n",
       "   0.49089688062667847,\n",
       "   5.3771796226501465,\n",
       "   0.2553398609161377,\n",
       "   -0.5807622671127319,\n",
       "   -0.858312726020813,\n",
       "   -0.40705007314682007,\n",
       "   0.4460916221141815,\n",
       "   -0.3021395206451416,\n",
       "   0.7278069853782654,\n",
       "   0.4642867147922516,\n",
       "   -0.6722913980484009,\n",
       "   -0.4841652512550354,\n",
       "   -0.5266545414924622,\n",
       "   1.8777129650115967,\n",
       "   -0.15462037920951843,\n",
       "   -1.0588332414627075,\n",
       "   0.9768966436386108,\n",
       "   -0.9349134564399719,\n",
       "   0.26638293266296387,\n",
       "   0.6076736450195312,\n",
       "   -0.6981250643730164,\n",
       "   -1.0044280290603638,\n",
       "   -0.28439250588417053,\n",
       "   0.36444172263145447,\n",
       "   -0.10854548215866089,\n",
       "   -0.10744321346282959,\n",
       "   -0.1635902374982834,\n",
       "   -1.2864919900894165,\n",
       "   0.21947774291038513,\n",
       "   -1.0823620557785034,\n",
       "   -0.3427114188671112,\n",
       "   -0.6361173987388611,\n",
       "   0.38543015718460083,\n",
       "   1.1239668130874634,\n",
       "   -0.333648145198822,\n",
       "   0.20754048228263855,\n",
       "   -1.0489847660064697,\n",
       "   -0.006864093244075775,\n",
       "   0.5844829082489014,\n",
       "   0.33713066577911377,\n",
       "   0.7886579036712646,\n",
       "   -0.16389791667461395,\n",
       "   -0.4167415499687195,\n",
       "   -0.5695080161094666,\n",
       "   0.22339066863059998,\n",
       "   -1.2867095470428467,\n",
       "   -0.2652665972709656,\n",
       "   -0.20364052057266235,\n",
       "   0.365551233291626,\n",
       "   0.3810232877731323,\n",
       "   0.8225756287574768,\n",
       "   -1.0550483465194702,\n",
       "   -0.3023075461387634,\n",
       "   -0.810200035572052,\n",
       "   -0.34596773982048035,\n",
       "   0.4030512571334839,\n",
       "   -0.5622056126594543,\n",
       "   -0.009597744792699814,\n",
       "   -0.24118566513061523,\n",
       "   -0.4373781085014343,\n",
       "   0.13732817769050598,\n",
       "   -0.08908018469810486,\n",
       "   -0.22050002217292786,\n",
       "   0.8654424548149109,\n",
       "   0.27824580669403076,\n",
       "   -0.05349411070346832,\n",
       "   0.7777312397956848,\n",
       "   0.7855011224746704,\n",
       "   -0.4795169532299042,\n",
       "   0.1498076170682907,\n",
       "   -0.19033125042915344,\n",
       "   0.4283079504966736,\n",
       "   -1.2399767637252808,\n",
       "   0.32876208424568176,\n",
       "   -0.3028458058834076,\n",
       "   0.09259085357189178,\n",
       "   -0.32082289457321167,\n",
       "   0.24108870327472687,\n",
       "   -0.562587320804596,\n",
       "   -0.0696597471833229,\n",
       "   -0.3374221920967102,\n",
       "   1.1417279243469238,\n",
       "   0.24445337057113647,\n",
       "   -0.759178876876831,\n",
       "   -0.016099315136671066,\n",
       "   0.5024927854537964,\n",
       "   -0.4034804105758667,\n",
       "   0.007404877804219723,\n",
       "   -0.8329916596412659,\n",
       "   -0.9079858064651489,\n",
       "   -0.14958178997039795,\n",
       "   -0.8839249610900879,\n",
       "   0.08374489843845367,\n",
       "   0.6902536749839783,\n",
       "   0.269548624753952,\n",
       "   1.0885686874389648,\n",
       "   -0.4727904796600342,\n",
       "   -0.17973598837852478,\n",
       "   -0.4755564332008362,\n",
       "   -0.3599907159805298,\n",
       "   0.7644367218017578,\n",
       "   -0.07562998682260513,\n",
       "   0.23733815550804138,\n",
       "   0.1548159122467041,\n",
       "   -0.8402559757232666,\n",
       "   0.1603464037179947,\n",
       "   -0.5869195461273193,\n",
       "   0.4438447952270508,\n",
       "   0.33765390515327454,\n",
       "   -0.7756619453430176,\n",
       "   -0.3447466790676117,\n",
       "   -1.3999536037445068,\n",
       "   -0.3533532917499542,\n",
       "   0.1645326018333435,\n",
       "   -0.6445151567459106,\n",
       "   -0.39252910017967224,\n",
       "   0.4821476936340332,\n",
       "   -0.6707287430763245,\n",
       "   -0.3503226041793823,\n",
       "   0.5845308303833008,\n",
       "   0.4962252378463745,\n",
       "   -0.05925365164875984,\n",
       "   -0.3247278034687042,\n",
       "   0.3881114721298218,\n",
       "   0.8882660269737244,\n",
       "   -0.019735628738999367,\n",
       "   0.010697459802031517,\n",
       "   0.19899962842464447,\n",
       "   -0.05850193649530411,\n",
       "   0.6227818727493286,\n",
       "   -0.3181810677051544,\n",
       "   0.13128973543643951,\n",
       "   -0.1555284708738327,\n",
       "   0.02386515587568283,\n",
       "   -0.5985144972801208,\n",
       "   0.31483644247055054,\n",
       "   -0.09440940618515015,\n",
       "   0.46019667387008667,\n",
       "   0.6278024315834045,\n",
       "   -0.5441285967826843,\n",
       "   0.31714993715286255,\n",
       "   -0.9902049899101257,\n",
       "   -0.445306658744812,\n",
       "   -0.775679349899292,\n",
       "   1.1484854221343994,\n",
       "   0.517970621585846,\n",
       "   0.3967013955116272,\n",
       "   -0.23297952115535736,\n",
       "   -1.2480069398880005,\n",
       "   -0.3433879613876343,\n",
       "   0.9208343625068665,\n",
       "   0.4773136377334595,\n",
       "   -0.5381408929824829,\n",
       "   0.6531146764755249,\n",
       "   -0.3053354024887085,\n",
       "   -0.27540189027786255,\n",
       "   0.3247188925743103,\n",
       "   0.4298006594181061,\n",
       "   0.3646901249885559,\n",
       "   -0.16158275306224823,\n",
       "   0.6605656743049622,\n",
       "   -0.16736358404159546]]]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The huggingface/transformers repository lists the other pipeline functions, such as ner extraction, sequence classification, and masking. You are encouraged to explore them. \n",
    "https://github.com/huggingface/transformers#quick-tour-of-pipelines\n",
    "\n",
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, use the pipeline functions or the word or sentence vector functions (e.g., similarity) to explore the social game underlying the production and meaning of texts associated with your final project. You have used similar, but often weaker versions in previous weeks. How does BERT help you gain insight regarding your research question that is similar and different from prior methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we are going to see the sentiment of journal articles in financial economics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df = sentence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate a pipeline for sentiment-analysis\n",
    "nlp_sentiment = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_calculation_category(text):\n",
    "    if nlp_sentiment(text)[0]['label'] == 'POSITIVE':\n",
    "        label = 1\n",
    "    else:\n",
    "        label = -1\n",
    "    return(label)\n",
    "\n",
    "def sentiment_calculation_score(text):\n",
    "    return nlp_sentiment(text)[0]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df['sentiment'] = fin_df.apply(lambda row: sentiment_calculation_category(row['sentence']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-235-e4c5d8b16e71>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fin_df['sentiment_score'] = fin_df.apply(lambda row: sentiment_calculation_score(row['sentence']), axis = 1)\n"
     ]
    }
   ],
   "source": [
    "fin_df['sentiment_score'] = fin_df.apply(lambda row: sentiment_calculation_score(row['sentence']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>875.0</td>\n",
       "      <td>875.000000</td>\n",
       "      <td>875.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.129143</td>\n",
       "      <td>0.936265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.992193</td>\n",
       "      <td>0.109280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.500578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.937097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.985331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label   sentiment  sentiment_score\n",
       "count  875.0  875.000000       875.000000\n",
       "mean     1.0   -0.129143         0.936265\n",
       "std      0.0    0.992193         0.109280\n",
       "min      1.0   -1.000000         0.500578\n",
       "25%      1.0   -1.000000         0.937097\n",
       "50%      1.0   -1.000000         0.985331\n",
       "75%      1.0    1.000000         0.995741\n",
       "max      1.0    1.000000         0.999788"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary statistics above, we can see that the overall sentiment of financial research seems to be a little bit negative. However, the result is not significant. This can be expected as research tends to be not sentiment-charged.\n",
    "\n",
    "However, I think BERT sentiment classsification can help me with the sentiment analysis of the stock market. I can analyze the sentiment in news and stock market to see their connections with the stock returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation using BERT\n",
    "\n",
    "The last method which we will explore is text generation. While some may regard it as a parlour trick due to unpredictability, recent dramatic improvements in text generation suggest that these kind of models can find themselves being used in more serious social scientific applications, such as in survey design and construction, idiomatic translation, and the normalization of phrase and sentence meanings.\n",
    "\n",
    "These models can be quite impressive, even uncanny in how human like they sound. Check out this [cool website](https://transformer.huggingface.co), which allows you to write with a transformer. The website is built by the folks who wrote the package we are using. The code underneath the website can be found in their examples: [run_generation.py](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py).\n",
    "\n",
    "We will be using the built in generate function, but the example file has more detailed code which allows you to set the seed differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151577e72cdb4041a904754f16fc05f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=665.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff0c6b658694b8cab3ba67d7dfe61b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1042301.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54431427b539499fbbb8a001ce4c80d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=456318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1805d9e7b83b4135a4e7764ad6e42030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1355256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f923a6cdefae4e589958e88de07f92be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=548118077.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing that we like to do more than analyse data all day long and then try to figure out what's going on.\n",
      "\n",
      "\"We're not going to be able to do that. We're not going to be able to do that. We\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Nothing that we like to do more than analyse data all day long and\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. A little creepy, and as we can see, far from perfect: GPT doesn't alwats work out flawlessly, but it sometimes can, and we will try and see if fine-tuning helps. We are going to tune the model on a complete dataset of Trump tweets, as they have a set of distinctive, highly identifiable qualities.\n",
    "\n",
    "### Creating a domain-specific language model\n",
    "\n",
    "One of the most exciting things about BERT and GPT is being able to retune them the way we want to. We will be training models to perform two tasks - one is to create a BERT with an \"accent\", by traning a model with english news data from the UK, from the US, and from India. We will also train a language generation model with a bunch of Trump tweets. \n",
    "\n",
    "We can train models specifically over a certain domain to make its language generation similar to that domain. \n",
    "[run_language modelling.py](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py), followed by [run_generation.py](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py). I've downloaded these files and added them to this directory so we can run them through the notebook. You are encouraged to look at these files to get a rough idea of what is going on.\n",
    "\n",
    "### Loading Data \n",
    "\n",
    "We want to now get our Trump tweets and our English news datasets ready. The data the scripts expect is just a text file with relevant data. We load the Trump tweets and then write them to disk as train and test files with only data. I leave the original dataframes in case you would like to use it for your own purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"../data/trump_tweets\"):\n",
    "    dfs.append(pd.read_json(\"../data/trump_tweets/\" + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>947824196909961216</td>\n",
       "      <td>Will be leaving Florida for Washington (D.C.) ...</td>\n",
       "      <td>2018-01-01 13:37:52+00:00</td>\n",
       "      <td>8237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51473</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>947810806430826496</td>\n",
       "      <td>Iran is failing at every level despite the ter...</td>\n",
       "      <td>2018-01-01 12:44:40+00:00</td>\n",
       "      <td>14595</td>\n",
       "      <td>25073877.0</td>\n",
       "      <td>53557</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>947802588174577664</td>\n",
       "      <td>The United States has foolishly given Pakistan...</td>\n",
       "      <td>2018-01-01 12:12:00+00:00</td>\n",
       "      <td>49566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>138808</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>947614110082043904</td>\n",
       "      <td>HAPPY NEW YEAR! We are MAKING AMERICA GREAT AG...</td>\n",
       "      <td>2017-12-31 23:43:04+00:00</td>\n",
       "      <td>35164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154769</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>947592785519173632</td>\n",
       "      <td>As our Country rapidly grows stronger and smar...</td>\n",
       "      <td>2017-12-31 22:18:20+00:00</td>\n",
       "      <td>39428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>157655</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               source              id_str  \\\n",
       "0  Twitter for iPhone  947824196909961216   \n",
       "1  Twitter for iPhone  947810806430826496   \n",
       "2  Twitter for iPhone  947802588174577664   \n",
       "3  Twitter for iPhone  947614110082043904   \n",
       "4  Twitter for iPhone  947592785519173632   \n",
       "\n",
       "                                                text  \\\n",
       "0  Will be leaving Florida for Washington (D.C.) ...   \n",
       "1  Iran is failing at every level despite the ter...   \n",
       "2  The United States has foolishly given Pakistan...   \n",
       "3  HAPPY NEW YEAR! We are MAKING AMERICA GREAT AG...   \n",
       "4  As our Country rapidly grows stronger and smar...   \n",
       "\n",
       "                 created_at  retweet_count  in_reply_to_user_id_str  \\\n",
       "0 2018-01-01 13:37:52+00:00           8237                      NaN   \n",
       "1 2018-01-01 12:44:40+00:00          14595               25073877.0   \n",
       "2 2018-01-01 12:12:00+00:00          49566                      NaN   \n",
       "3 2017-12-31 23:43:04+00:00          35164                      NaN   \n",
       "4 2017-12-31 22:18:20+00:00          39428                      NaN   \n",
       "\n",
       "   favorite_count  is_retweet  \n",
       "0           51473       False  \n",
       "1           53557       False  \n",
       "2          138808       False  \n",
       "3          154769       False  \n",
       "4          157655       False  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text, test_text = train_test_split(df['text'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1538    Getting ready to leave for Poland, after which...\n",
       "530     There is a rumor, put out by the Democrats, th...\n",
       "4325    \"@RyanPaolucci: @realDonaldTrump please buy th...\n",
       "749     Massive crowds inside and outside of the @Toyo...\n",
       "793     Tom Brady played great today. He is a total ch...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text.to_frame().to_csv(r'train_text_trump', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text.to_frame().to_csv(r'test_text_trump', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now used the Google Colab GPUs to train the Trump tweet models. We'll be doing the same for our blog posts too.\n",
    "\n",
    "### GloWBe dataset\n",
    "\n",
    "We'll now load up the GloWbe (Corpus of Global Web-Based English) dataset which have different texts from different countries. We'll try and draw out texts from only the US, UK and India. We'll then save these to disk. Note that this is a Davies Corpora dataset: the full download can be done with the Dropbox link I sent in an announcement a few weeks ago. The whole download is about 3.5 GB but we only need two files, which are anout 250 MB each. The other files might be useful for your research purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lucem_illud_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "address = \"/Users/bhargavvader/Downloads/Academics_tech/corpora/GloWbE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these are the exact name of the files\n",
    "us = \"/text_us_blog_jfy.zip\"\n",
    "gb = \"/text_gb_blog_akq.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_us_blog_jfy.zip\n"
     ]
    }
   ],
   "source": [
    "us_texts = lucem_illud_2020.loadDavies(address, corpus_style=\"us_blog\", num_files=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_gb_blog_akq.zip\n"
     ]
    }
   ],
   "source": [
    "gb_texts = lucem_illud_2020.loadDavies(address, corpus_style=\"gb_blog\", num_files=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dictionary with document ids mapping to text. Since we don't need any information but the text, we can just save these to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'< p > Many workers within the UK are required to wear an uniform for work but very few are aware they could really claim back some money from the tax man to help with the price of washing or repairing the uniform HMRC will actually pay money back once again to those individuals who are eligible even dating back four years worth of washing < p > In order to find out about claiming a tax rebate on uniform it is worth having a look online to find out whether or not you will be eligible The conditions are fairly straightforward you just have to wear a recognisable work uniform which might contain a T shirt which displays a logo design or possibly some specialist protective clothing The type of the occupation is unrelated anyone from nurses to cops to electricians are able to claim As long as the uniform is worn at work is washed on your own and you are an UK tax payer then your chances are you will be eligible to claim are eligible how do you actually go about claiming your tax refund for washing uniform You can access the HMRC web site directly and complete the important forms to help you to claim your hard earned money back Or you can use an agent that is registered with the HMRC who are able to deal with the whole tax rebate on your behalf This may save you a great deal of time and effort and they may also have the ability to advise you on any other areas where you could claim a rebate for instance if you are required to supply your own tools within your job < p > The amount of money you can claim as a tax rebate on uniform will depend on your profession and how long you have already been wearing a work uniform for You might be in a position to claim a rebate for the past four years which could add up to a fine sum of money < p > In those times of economic decline most people are seeking to save lots of washing uniform can be an effective way to start saving a couple of pounds every month which over the course of a'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(list(us_texts.values())[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict_to_texts(texts, file_name):\n",
    "    text = []\n",
    "    for doc in list(texts.values()):\n",
    "        text.append(' '.join(doc).replace(\"< h >\", \"\").replace(\"< p >\", \"\"))\n",
    "    train_text, test_text = train_test_split(text, test_size=0.2)\n",
    "    with open(file_name + \"_train\", 'w') as f:\n",
    "        for item in train_text:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n",
    "    with open(file_name + \"_test\", 'w') as f:\n",
    "        for item in test_text:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_to_texts(us_texts, \"us_blog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_to_texts(gb_texts, \"gb_blog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the training and testing files for both US and GB blogs in English. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WARNING - SHIFT TO GOOGLE COLAB OR GPU ENABLED MACHINE\n",
    "\n",
    "The [Google Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) walks you through the process of fine-tuning models, as we did before for the classification task. Move now to the colab file to fine tune your models. Once you downloaded all the models and their information, place those files in the directory of the HW to use them as demonstrated below. \n",
    "\n",
    "\n",
    "\n",
    "### Running Scripts\n",
    "\n",
    "We use the scripts to do language modelling and text generation. The following cells run the code as if you would have run it in a terminal. I trained all of these models using the Googlr Colab file, and then saved the models to disk.\n",
    "\n",
    "#### Trump GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !python run_language_modelling.py --output_dir=output_gpt_trump --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=train_text_trump --do_eval --eval_data_file=test_text_trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoBERTa US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !python run_language_modeling.py --output_dir=output_roberta_US --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=$TRAIN_FILE --do_eval --eval_data_file=$TEST_FILE --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoBERTa UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !python run_language_modeling.py --output_dir=output_roberta_UK --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=$TRAIN_FILE --do_eval --eval_data_file=$TEST_FILE --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COME BACK TO THIS NOTEBOOK to load and work with your trained model\n",
    "\n",
    "### Loading and using models\n",
    "\n",
    "Let us now load the four models we have and see how we can use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now - let us see what our Trump Tweet Bot looks like!\n",
    "You can generate text via command line using the command below. You can also load a model once it is saved - I trained my model using Google Colab, downloaded the model, and am loading it again via the command below. Note that you have to download all the files in your folder of the fine-tuned model to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !python run_generation.py --model_type=gpt2 --model_name_or_path=output_trump_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_trump = AutoTokenizer.from_pretrained(\"output_trump_gpt\")\n",
    "model_trump = AutoModelWithLMHead.from_pretrained(\"output_trump_gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama is going to be a disaster for the United States. He is a total loser. He is a total loser!\"\n",
      "\"I will be interviewed on @foxandfriends at 7:00 A.M. Enjoy!\"\n",
      "\"\"\"@jim\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Obama is going to\"\n",
    "\n",
    "input = tokenizer_trump.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_trump.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_trump.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - our Trump bot is nasty, so we know our model trained well. What happens if we try the same sentence for our non-fine tuned model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama is going to be a very good president,\" said Sen. John McCain (R-Ariz.). \"He's going to be a very good president. He's going to be a very good president. He's going to be a very good\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Obama is going to\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite the contrast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that generate a BERT-powered chatbot tuned on text related to your final project. What is interesting about this model, and how to does it compare to an untrained model? What does it reveal about the social game involved with your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get better results, I consider all the economics articles published in the top 5 econ journal over the past 20 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "JEL_GJ = JEL\n",
    "\n",
    "def sentence_split(text):\n",
    "    sentence_list = text.split('.')\n",
    "    return(sentence_list)\n",
    "\n",
    "# AER = JEL_GJ[JEL_GJ['source']=='AMERICAN ECONOMIC REVIEW']\n",
    "\n",
    "sample_df = JEL_GJ\n",
    "\n",
    "sample_df['label'] = np.where(sample_df['one_JEL']==\"G\", 1, 0)\n",
    "\n",
    "sample_df['sentence_list'] = sample_df.apply(lambda row: sentence_split(row['abstract']), axis = 1)\n",
    "\n",
    "sample_df = sample_df[['title', 'abstract', 'label', 'sentence_list', 'source']]\n",
    "\n",
    "sentence_df = sample_df[sample_df['source']!= 'AMERICAN ECONOMIC REVIEW'].explode('sentence_list')\n",
    "\n",
    "sentence_df = sentence_df[sentence_df['sentence_list']!='']\n",
    "\n",
    "sentence_df.columns = ['title', 'abstract', 'label', 'sentence', 'source']\n",
    "\n",
    "sentence_df.to_csv('hw8_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = sentence_df[sentence_df['sentence']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text, test_text = train_test_split(sentence_df['sentence'], test_size=0.2)\n",
    "\n",
    "train_text.to_frame().to_csv(r'train_text_JEL', header=None, index=None, sep=' ', mode='a')\n",
    "test_text.to_frame().to_csv(r'test_text_JEL', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Econ Top 5 Journal Chatbot test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_JEL = AutoTokenizer.from_pretrained(\"output_gpt_JEL\")\n",
    "model_JEL = AutoModelWithLMHead.from_pretrained(\"output_gpt_JEL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we feed the model with relatively complete sentences. From the first two examples, the text looks good at first glance. However, if we read it closely, we will find that part of the logic does not make perfect sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We examine the effect of sentiment on cross section of stock returns\"\n",
      "\" We show that the effect of the presence of a positive external factor on the stock returns of the stock market is small, and that the effect of the presence of a negative external factor on the stock returns of the stock market is large\"\n",
      "\" We show that the effect of the presence of a positive external factor on the stock returns of the stock market is small, and that the effect of the presence of a negative external factor\n"
     ]
    }
   ],
   "source": [
    "sequence = \"We examine the effect of sentiment on cross section of stock returns\"\n",
    "\n",
    "input = tokenizer_JEL.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_JEL.generate(input, max_length=100, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_JEL.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We test the efficient market hypothesis under the assumption that the price of a commodity is determined by the number of agents in the market\"\n",
      "\" We show that the optimal equilibrium is a simple one with a finite number of agents\"\n",
      "\" We show that\n"
     ]
    }
   ],
   "source": [
    "sequence = \"We test the efficient market hypothesis under the assumption that\"\n",
    "\n",
    "input = tokenizer_JEL.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_JEL.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_JEL.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compare the trained model with untrained one. The first result showed that academic articles tend to use long sentences with clause. This contrast the shorter sentences in untrained model. This generation is also pretty philosophical. This also illustrates how economists come up with their arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We combine the thinking in behavioral finance and rational expectations with the empirical evidence to show that the optimal allocation of resources is not a simple choice between maximizing the welfare of the individual and maximizing the welfare of the society\"\n",
      "\" We show that the optimal allocation\n"
     ]
    }
   ],
   "source": [
    "sequence = \"We combine the thinking in behavioral finance and rational expectations\"\n",
    "\n",
    "input = tokenizer_JEL.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_JEL.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_JEL.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We combine the thinking in behavioral finance and rational expectations with the insights of the behavioral sciences to create a new paradigm for financial markets.\n",
      "\n",
      "The new paradigm is based on the idea that the market is a system of incentives, and that the incentives are\n"
     ]
    }
   ],
   "source": [
    "# Finally, we try to see how untrained data works.\n",
    "\n",
    "sequence = \"We combine the thinking in behavioral finance and rational expectations\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to make sure whether the model simply 'remembers' the text from training data. We try some sentence in our training set. As can be seen from below, the model generates different results from the original ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We estimate an equilibrium on-the-job search model with endogenous search intensity. Workers differ by skill, firms by productivity. Workers respond to mismatch by intensive search, and sorting may result from complementarities in the match-level production function. The model is estimated on Danish-matched employeremployee data. Firms are ranked through revealed preference by the fraction of hires that is poached from other firms: the poaching rank. Identification is obtained by firm rank conditional mobility and wage patterns. Wage variation is decomposed into four sources: sorting (40%), worker heterogeneity (32%), firm heterogeneity (18%), and frictional competition (10%). A social planner can improve output net of search cost by 1.5% relative to the decentralized solution.'"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the text from the corpus\n",
    "JEL.iloc[-2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We estimate an equilibrium on-the-job search model with endogenous endogenous demand and endogenous demand-neutrality\"\n",
      "\" We show that the model is robust to the assumption that the economy is dynamic and that the equilibrium is stable\"\n",
      "\" We show\n"
     ]
    }
   ],
   "source": [
    "sequence = \"We estimate an equilibrium on-the-job search model with endogenous\"\n",
    "\n",
    "input = tokenizer_JEL.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_JEL.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_JEL.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we want to see whether the model can generate sentences that make sense from short beginning. The results look pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent critiques on the role of the media in shaping public opinion are based on the assumption that the media is a powerful tool for influencing public opinion\"\n",
      "\" The paper examines the role of the media in shaping public opinion and the role of the media in\n"
     ]
    }
   ],
   "source": [
    "sequence = 'Recent critiques on'\n",
    "input = tokenizer_JEL.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_JEL.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_JEL.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning in finance can be used to develop a model of the optimal allocation of capital\"\n",
      "\" We show that the model is robust to the assumption that the agent\\'s preferences are sufficiently large to be consistent with the observed preferences\"\n",
      "\" We show that the model is robust to the assumption that the agent\\'s preferences are sufficiently large to be consistent with the observed preferences\"\n",
      "\" We show that the model is robust to the assumption that the agent\\'s preferences are sufficiently large to be consistent\n"
     ]
    }
   ],
   "source": [
    "sequence = 'Machine learning in finance can'\n",
    "input = tokenizer_JEL.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_JEL.generate(input, max_length=100, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_JEL.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combine all the evidence from recent development in econometrics, we demonstrate the semiparametric efficiency bound of the model and show that the model is robust to the presence of a large number of unobserved heterogeneity effects\"\n",
      "\" We show that the model is robust to the existence of a large number of unobserved heterogeneity effects\"\n",
      "\" We show that the model is robust to the existence of a large number of unobserved heterogeneity effects\"\n",
      "\" We show that the model is robust to\n"
     ]
    }
   ],
   "source": [
    "sequence = 'Combine all the evidence from recent development in econometrics, we demonstrate the semiparametric efficiency bound'\n",
    "input = tokenizer_JEL.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_JEL.generate(input, max_length=100, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_JEL.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning\"\n",
      "\"This is a simple example of a simple model of the relationship between the number of agents and the number of agents\\'s preferences\"\n",
      "\"This paper develops a model of the relationship between the number of agents and the number of preferences\"\n",
      "\" We show that the optimal allocation of resources is determined by the number of agents and the number of preferences\"\n",
      "\" We show that the optimal allocation of resources is determined by the number of agents and the number of preferences\"\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "sequence = 'Good morning'\n",
    "input = tokenizer_JEL.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_JEL.generate(input, max_length=100, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_JEL.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check out our UK and GB embeddings - how do you think the two models will differ? Maybe in the way different words relate to each other in the same sentence? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta_us and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "roberta_us_model_embedding = RobertaModel.from_pretrained('roberta_us')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_us_tokenizer = RobertaTokenizer.from_pretrained('roberta_us')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to visualise how words in a sentence or different or similar to each other. We will try to construct sentences where words might mean different things in different countries - in the US, people might eat chips with salsa, but in the UK, chips are what Americans call french fries, and might eat it fried fish instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Do you have your chips with fish or with salsa?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"He went out in just his undershirt and pants.\" #pants are underwear in Britain; maybe closer to an undershirt\n",
    "text2 = \"His braces completed the outfit.\" #braces are suspenders (in Britain); maybe closer to an outfit\n",
    "text3 = \"Does your pencil have a rubber on it?\" #rubber is an eraser in Britain); maybe closer to a pencil\n",
    "text4 = \"Was the bog closer to the forest or the house?\" #bog is a toilen in Britain); maybe closer to a house\n",
    "text5 = \"Are you taking the trolley or the train to the grocery market\" #trolley is a food carriage; possibly closer to a market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_diffs(text, model, tokenizer):\n",
    "    word_vecs = []\n",
    "    for i in range(0, len(text.split())):\n",
    "        word_vecs.append(word_vector(text, i, model, tokenizer))\n",
    "    L = []\n",
    "    for p in word_vecs:\n",
    "        l = []\n",
    "        for q in word_vecs:\n",
    "            l.append(1 - cosine(p, q))\n",
    "        L.append(l)\n",
    "    M = np.array(L)\n",
    "    fig = plt.figure()\n",
    "    div = pd.DataFrame(M, columns = list(text.split()), index = list(text.split()))\n",
    "    ax = sns.heatmap(div)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, roberta_us_model_embedding, roberta_us_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta_gb and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "roberta_gb_model_embedding = RobertaModel.from_pretrained('roberta_gb')\n",
    "roberta_gb_tokenizer = RobertaTokenizer.from_pretrained('roberta_gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, roberta_gb_model_embedding, roberta_gb_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see regarding the relations with chips and sala/fish? What about the other sentences? How about comparing sentence embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that tune BERT to at least two different textual samples. These could be from different corpora, distinct time periods, separate authors, alternative publishing outlets, etc. Then compare the meaning of words, phrases and sentences to each other across the separate models. What do they reveal about the social worlds inscribed by the distinctive samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we compare corpus pre 2008 with corpus post 2008 to see the effect of financial crisis on economics research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hesongrun/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (7,8,13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/hesongrun/Dropbox/TTIC31220/wos_econ_text_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['title', 'pubyear', 'pubmonth', 'pubday']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-292-bd9f8badc4ea>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['title'] = df['title'].apply(lambda x: x.upper())\n"
     ]
    }
   ],
   "source": [
    "df['title'] = df['title'].apply(lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "JEL_df = pd.merge(JEL, df, on = 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "JEL_pre2008 = JEL_df[JEL_df['pubyear']<2008]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "JEL_post2008 = JEL_df[JEL_df['pubyear']>=2008]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-305-3eb403d7478e>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_df['label'] = np.where(sample_df['one_JEL']==\"G\", 1, 0)\n",
      "<ipython-input-305-3eb403d7478e>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_df['sentence_list'] = sample_df.apply(lambda row: sentence_split(row['abstract']), axis = 1)\n"
     ]
    }
   ],
   "source": [
    "def sentence_split(text):\n",
    "    sentence_list = text.split('.')\n",
    "    return(sentence_list)\n",
    "\n",
    "# AER = JEL_GJ[JEL_GJ['source']=='AMERICAN ECONOMIC REVIEW']\n",
    "\n",
    "sample_df = JEL_pre2008\n",
    "\n",
    "sample_df['label'] = np.where(sample_df['one_JEL']==\"G\", 1, 0)\n",
    "\n",
    "sample_df['sentence_list'] = sample_df.apply(lambda row: sentence_split(row['abstract']), axis = 1)\n",
    "\n",
    "sample_df = sample_df[['title', 'abstract', 'label', 'sentence_list', 'source']]\n",
    "\n",
    "sentence_df = sample_df.explode('sentence_list')\n",
    "\n",
    "sentence_df = sentence_df[sentence_df['sentence_list']!='']\n",
    "\n",
    "sentence_df.columns = ['title', 'abstract', 'label', 'sentence', 'source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text, test_text = train_test_split(sentence_df['sentence'], test_size=0.2)\n",
    "\n",
    "train_text.to_frame().to_csv(r'train_text_JEL_pre2008', header=None, index=None, sep=' ', mode='a')\n",
    "test_text.to_frame().to_csv(r'test_text_JEL_pre2008', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-308-6d25caee445b>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_df['label'] = np.where(sample_df['one_JEL']==\"G\", 1, 0)\n",
      "<ipython-input-308-6d25caee445b>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_df['sentence_list'] = sample_df.apply(lambda row: sentence_split(row['abstract']), axis = 1)\n"
     ]
    }
   ],
   "source": [
    "def sentence_split(text):\n",
    "    sentence_list = text.split('.')\n",
    "    return(sentence_list)\n",
    "\n",
    "# AER = JEL_GJ[JEL_GJ['source']=='AMERICAN ECONOMIC REVIEW']\n",
    "\n",
    "sample_df = JEL_post2008\n",
    "\n",
    "sample_df['label'] = np.where(sample_df['one_JEL']==\"G\", 1, 0)\n",
    "\n",
    "sample_df['sentence_list'] = sample_df.apply(lambda row: sentence_split(row['abstract']), axis = 1)\n",
    "\n",
    "sample_df = sample_df[['title', 'abstract', 'label', 'sentence_list', 'source']]\n",
    "\n",
    "sentence_df = sample_df.explode('sentence_list')\n",
    "\n",
    "sentence_df = sentence_df[sentence_df['sentence_list']!='']\n",
    "\n",
    "sentence_df.columns = ['title', 'abstract', 'label', 'sentence', 'source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text, test_text = train_test_split(sentence_df['sentence'], test_size=0.2)\n",
    "\n",
    "train_text.to_frame().to_csv(r'train_text_JEL_post2008', header=None, index=None, sep=' ', mode='a')\n",
    "test_text.to_frame().to_csv(r'test_text_JEL_post2008', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre- and Post- 2008 comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at output_JEL_pre2008 were not used when initializing RobertaModel: ['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at output_JEL_pre2008 and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "pre2008_model_embedding = RobertaModel.from_pretrained('output_JEL_pre2008')\n",
    "pre2008_tokenizer = RobertaTokenizer.from_pretrained('output_JEL_pre2008')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Financial crisis reminds us to reexamine the rational expectation vesrus behavior economics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_diffs(text, model, tokenizer):\n",
    "    word_vecs = []\n",
    "    for i in range(0, len(text.split())):\n",
    "        word_vecs.append(word_vector(text, i, model, tokenizer))\n",
    "    L = []\n",
    "    for p in word_vecs:\n",
    "        l = []\n",
    "        for q in word_vecs:\n",
    "            l.append(1 - cosine(p, q))\n",
    "        L.append(l)\n",
    "    M = np.array(L)\n",
    "    fig = plt.figure()\n",
    "    div = pd.DataFrame(M, columns = list(text.split()), index = list(text.split()))\n",
    "    ax = sns.heatmap(div)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, pre2008_model_embedding, pre2008_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure fits my expectation. Before 2008 crisis is less related to behavioral finance. People tend to analyze the crisis from a rational expectation perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at output_JEL_post2008 were not used when initializing RobertaModel: ['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at output_JEL_post2008 and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "post2008_model_embedding = RobertaModel.from_pretrained('output_JEL_post2008')\n",
    "post2008_tokenizer = RobertaTokenizer.from_pretrained('output_JEL_post2008')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, post2008_model_embedding, post2008_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 2008, we can see financial crisis is more closely related to behavioral economics than rational expectations. The model from rational expectation cannot fully account for the abnormal scenario of financial crisis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also explored the relationship between financial crisis with morgage backed securities as well as the interaction between asset pricing and corporate finance pre- and post- 2008. The figures below show their differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Financial crisis can arise from morgage backed securities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, pre2008_model_embedding, pre2008_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, post2008_model_embedding, post2008_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Asset Pricing and Corporate Finance are two main streams of modern finance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, pre2008_model_embedding, pre2008_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, post2008_model_embedding, post2008_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, this is only a small dataset. We expect the model to perform much better if trained on a larger corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
